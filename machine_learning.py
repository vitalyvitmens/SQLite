# TODO: Machine Learning (Машинное обучение)
#  Узнайте, как создавать интеллектуальные системы рекомендаций,
#  которые помогают нам в повседневной жизни.
#  Добро пожаловать в машинное обучение.

# TODO: Welcome to Machine Learning (Добро пожаловать в машинное обучение)
#  Поздравляем! Вы сделали большой шаг к тому, чтобы стать практиком машинного обучения!
#  В дополнение к прохождению этого курса не забудьте воспользоваться всей поддержкой обучения,
#  доступной вам на SoloLearn, включая ежедневные советы, практики «Попробуйте сами», задачи тренера по коду,
#  игровую площадку для кода и участие в нашем замечательном сообществе учащихся. Мы рады услышать от вас,
#  поэтому, пожалуйста, оставляйте комментарии и отзывы, когда вы учитесь с нами.
#  Python — это язык программирования, который мы будем использовать на протяжении всего курса.
#  Давайте начнем!

# TODO: Machine Learning Overview (Обзор машинного обучения)
#  Добро пожаловать на курс машинного обучения! Машинное обучение — это способ сбора данных и превращения их в идеи.
#  Мы используем мощность компьютера для анализа примеров из прошлого, чтобы построить модель,
#  которая может предсказать результат для новых примеров. Мы сталкиваемся с моделями машинного обучения каждый день.
#  Например, когда Netflix рекомендует вам шоу, они использовали модель, основанную на том,
#  что вы и другие пользователи смотрели, чтобы предсказать, что вы хотели бы.
#  Когда Amazon выбирает цену для товара, они используют модель,
#  основанную на том, как подобные товары продавались в прошлом.
#  Когда компания, выпустившая вашу кредитную карту, звонит вам из-за подозрительной активности,
#  она использует модель, основанную на вашей прошлой активности, для распознавания аномального поведения.
#  В этом курсе мы изучим несколько методов решения задач машинного обучения.
#  Машинное обучение можно использовать для создания чат-бота, обнаружения спама или распознавания изображений.

# TODO: Course Basics (Основы курса)
#  Одним из наиболее распространенных языков, используемых профессионалами в области машинного обучения, является Python
#  Это очень доступно и очень мощно, поэтому мы будем использовать его в этом курсе.
#  Мы предполагаем рабочее знание Python. В этом курсе мы будем использовать несколько пакетов Python,
#  которые помогут решить задачи машинного обучения. Мы будем использовать pandas, numpy, matplotlib и scikit-learn.
#  Pandas используется для чтения данных и обработки данных, numpy используется для вычисления числовых данных,
#  matplotlib используется для построения графиков данных, а scikit-learn используется для моделей машинного обучения.
#  Каждый из этих пакетов довольно обширен, но мы рассмотрим функции, которые будем использовать.
#  Мы также рассмотрим некоторые основные статистические данные, поскольку они являются основой машинного обучения.
#  Курс будет охватывать как теорию, так и практику методов машинного обучения, но сосредоточится на том,
#  как их использовать на реальных примерах.

# TODO: What's in this Course? (Что в этом курсе?)
#  В машинном обучении мы говорим о контролируемом и неконтролируемом обучении.
#  Обучение с учителем — это когда у нас есть известная цель на основе прошлых данных
#  (например, прогнозирование цены, по которой будет продаваться дом),
#  а обучение без учителя — это когда нет известного прошлого ответа
#  (например, определение тем, обсуждаемых в обзорах ресторанов).
#  В этом курсе мы сосредоточимся на контролируемом обучении.
#  В контролируемом обучении существуют проблемы классификации и регрессии.
#  Регрессия прогнозирует числовое значение (например, прогнозирует, по какой цене будет продаваться дом)
#  и классифицирует предсказывает, к какому классу что-либо принадлежит
#  (например, предсказывает, не выполнит ли заемщик дефолт по своему кредиту).
#  Мы сосредоточимся на проблемах классификации.
#  Это задачи, в которых мы предсказываем, к какому классу что-то принадлежит.
#  Наши примеры будут включать:
#  • Прогнозирование того, кто выживет при крушении «Титаника»
#  • Определение рукописной цифры по изображению
#  • Использование данных биопсии для определения злокачественности новообразования
#  Мы будем использовать ряд популярных методов для решения этих проблем.
#  Мы рассмотрим каждый из них более подробно в следующих модулях:
#  • Логистическая регрессия
#  • Деревья решений
#  • Случайные леса
#  • Нейронные сети
#  В конце этого курса вы сможете взять классификационный набор данных
#  и использовать Python для создания нескольких различных моделей,
#  чтобы определить лучшую модель для данной проблемы.
#  Машинное обучение можно использовать для решения широкого круга задач.
#  Этот курс будет посвящен контролируемому обучению и классификации.

# TODO: Averages (Средние значения)
#  При работе с данными нам часто нужно вычислить некоторые простые статистические данные.
#  Допустим, у нас есть список возрастов людей в классе.
#  Мы располагаем их в порядке возрастания, так будет проще производить расчеты.
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: Среднее значение является наиболее известным средним значением.
#  Сложите все значения и разделите на количество значений:
#      (15 + 16 + 18 + 19 + 22 + 24 + 29 + 30 + 34) / 9 =  207/9 = 23
# TODO: Медиана — это значение посередине упорядоченных чисел.
#  В этом случае, поскольку имеется 9 значений, среднее значение 5-тое, то есть 22.
#  В статистике как среднее, так и медиана называются средними.
#  Среднее значение для непрофессионала является средним.

# TODO: Percentiles (процентили)
#  Медиану также можно рассматривать как 50-й процентиль.
#  Это означает, что 50% данных меньше медианы, а 50% данных больше медианы.
#  Это говорит нам, где находится середина данных, но нам часто требуется больше понимания распределения данных.
#  Мы часто будем рассматривать 25 -й процентиль и 75 -й процентиль.
#  25 - й процентиль — это значение, которое составляет одну четверть пути через данные.
#  Это значение, при котором 25% данных меньше его (а 75% данных больше его).
#  Точно так же 75 -й процентиль составляет три четверти пути через данные.
#  Это значение, при котором 75% данных меньше его (а 25% данных больше его).
#  Если мы снова посмотрим на наш возраст:
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: У нас есть 9 значений, поэтому 25% данных будут составлять примерно 2 точки данных.
#  Таким образом, третья точка данных превышает 25% данных. Таким образом, 25-й процентиль равен 18 (3-я точка данных).
#  Точно так же 75% данных составляют примерно 6 точек данных. Таким образом, 7-я точка данных превышает 75% данных.
#  Таким образом, 75-й процентиль равен 29 (7-я точка данных).
#  Полный диапазон наших данных находится в диапазоне от 15 до 34.
#  25-й и 75-й процентили говорят нам, что половина наших данных находится в диапазоне от 18 до 29.
#  Это помогает нам понять, как распределяются данные.
#  Если имеется четное количество точек данных, чтобы найти медиану (или 50-й процентиль),
#  вы берете среднее значение двух значений в середине.

# TODO: Standard Deviation & Variance (Стандартное отклонение и дисперсия)
#  Мы можем получить более глубокое понимание распределения наших данных со стандартным отклонением и дисперсией.
#  Стандартное отклонение и дисперсия — это меры того, насколько разбросаны или рассредоточены данные.
#  Мы измеряем, насколько далеко каждая точка данных от среднего.
#  Давайте еще раз посмотрим на нашу возрастную группу:
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: Напомним, что среднее равно 23. Давайте подсчитаем, насколько далеко каждое значение от среднего.
#  15 на 8 отличается от среднего (поскольку 23-15=8).
#  Вот список всех этих расстояний:
#      8, 7, 5, 4, 1, 1, 6, 7, 11
# TODO: Мы возводим эти значения в квадрат и складываем их вместе.
#  См. Рис: StandardDeviationPicture1.png
#  Мы делим это значение на общее количество значений, и это дает нам дисперсию.
#      362 / 9 = 40.22
# TODO: Чтобы получить стандартное отклонение, мы просто возьмем квадратный корень из этого числа и получим: 6.34
#  Если наши данные нормально распределены, как показано на графике ниже (См. Рис: StandardDeviationPicture2.png),
#  68% населения находится в пределах одного стандартного отклонения от среднего.
#  На графике мы выделили область в пределах одного стандартного отклонения от среднего значения.
#  Вы можете видеть, что заштрихованная область составляет около двух третей (точнее 68%) от общей площади под кривой.
#  Если мы предположим, что наши данные нормально распределены, мы можем сказать,
#  что 68% данных находятся в пределах 1 стандартного отклонения от среднего.
# TODO: В нашем примере с возрастом, хотя возрасты, вероятно, не совсем нормально распределены,
#  мы предполагаем, что это так, и говорим, что примерно 68% населения имеет возраст
#  в пределах одного стандартного отклонения от среднего. Поскольку среднее значение равно 23 годам,
#  а стандартное отклонение равно 6,34, мы можем сказать, что приблизительно 68% возрастов нашей популяции находятся
#  в диапазоне от 16,66 (23 минус 6,34) до 29,34 (23 плюс 6,34).
#  Несмотря на то, что данные никогда не бывают идеальным нормальным распределением,
#  мы все же можем использовать стандартное отклонение, чтобы получить представление о том, как распределяются данные.

# TODO: Statistics with Python (Статистика с Python)
#  Мы можем рассчитать все эти операции с помощью Python. Мы будем использовать пакет Python numpy.
#  Мы будем использовать numpy позже для работы с массивами, а сейчас мы просто будем использовать несколько функций
#  для статистических вычислений: mean, median, centile, std, var.
#  Сначала мы импортируем пакет. Стандартной практикой является псевдоним numpy как np.
#      import numpy as np
# TODO: Давайте инициализируем переменную data, чтобы иметь список возрастов.
#      data = [15, 16, 18, 19, 22, 24, 29, 30, 34]
# TODO: Теперь мы можем использовать функции numpy.
#  Для функций среднего, медианы, стандартного отклонения и дисперсии мы просто передаем список данных.
#  Для функции процентиля мы передаем список данных и процентиль (в виде числа от 0 до 100).
# import numpy as np
#
# data = [15, 16, 18, 19, 22, 24, 29, 30, 34]
#
# print("mean:", np.mean(data))
# print("median:", np.median(data))
# print("50th percentile (median):", np.percentile(data, 50))
# print("25th percentile:", np.percentile(data, 25))
# print("75th percentile:", np.percentile(data, 75))
# print("standard deviation:", np.std(data).round(2))
# print("variance:", np.var(data).round(2))
# TODO: Numpy — это библиотека Python, которая позволяет быстро и легко выполнять математические операции с массивами.

# TODO: What is Pandas? (Что такое Панды?)
#  Этот курс написан на Python, одном из наиболее часто используемых языков для машинного обучения.
#  Одна из причин, по которой он так популярен, заключается в том,
#  что существует множество полезных модулей Python для работы с данными.
#  Первый, который мы представим, называется Pandas.
#  Pandas — это модуль Python, который помогает нам читать данные и управлять ими.
#  Что хорошо в pandas, так это то, что вы можете брать данные и просматривать их в виде таблицы,
#  удобочитаемой для человека, но их также можно интерпретировать в числовом виде,
#  чтобы вы могли выполнять с ними множество вычислений. Мы называем таблицу данных DataFrame .
#  Python удовлетворит все наши потребности в машинном обучении.
#  Мы будем использовать модуль Pandas для обработки данных.

# TODO: Read in Your Data (Читать в ваших данных)
#  Нам нужно начать с импорта Pandas. Стандартной практикой является прозвище pd, чтобы потом быстрее печатать.
#      import pandas as pd
# TODO: Мы будем работать с набором данных пассажиров Титаника.
#  Для каждого пассажира у нас будут данные о нем, а также о том, выжили ли они в кораблекрушении.
#  Наши данные хранятся в виде файла CSV (значения, разделенные запятыми). Файл titanic.csv находится ниже.
#  Первая строка — это заголовок, а затем каждая последующая строка — это данные для одного пассажира.
#      Survived, Pclass, Sex, Age, Siblings/Spouses, Parents/Children, Fare
#      Выжившие, ПКласс, Пол, Возраст, Братья и сестры/супруги, Родители/Дети, Плата за проезд
#      0, 3, male, 22.0, 1, 0, 7.25
#      1, 1, female, 38.0, 1, 0, 71.2833
#      1, 3, female, 26.0, 0, 0, 7.925
#      1, 1, female, 35.0, 1, 0, 53.1
# TODO: Мы собираемся загружать данные в pandas, чтобы мы могли просматривать их как DataFrame.
#  Функция read_csv берет файл в формате csv и преобразует его в Pandas DataFrame.
#      df = pd.read_csv('titanic.csv')
# TODO: Объект df теперь является нашим фреймом данных pandas с набором данных Titanic.
#  Теперь мы можем использовать метод head для просмотра данных.
#  Метод head возвращает первые 5 строк DataFrame.
#      print(df.head())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head())
# TODO: Обычно данные хранятся в файлах CSV (значения, разделенные запятыми),
#  которые мы можем легко прочитать с помощью функции panda read_csv.
#  Метод head возвращает первые 5 строк.

# TODO: Summarize the Data (Суммарные данные)
#  Обычно наши данные слишком велики, чтобы мы могли отобразить их все.
#  Рассмотрение первых нескольких строк — это первый шаг к пониманию наших данных,
#  но затем мы хотим взглянуть на некоторую сводную статистику.
#  В пандах мы можем использовать метод описания describe().
#  Он возвращает таблицу статистики о столбцах.
#      print(df.describe())
# TODO: Мы добавляем строку в приведенный ниже код, чтобы заставить Python отображать все 6 столбцов.
#  Без линии это будет сокращать результаты.
# import pandas as pd
#
# pd.options.display.max_columns = 6
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.describe())
# TODO: Для каждого столбца мы видим несколько статистических данных.
#  Обратите внимание, что он дает статистику только для числовых столбцов.
#  Давайте рассмотрим, что означает каждая из этих статистических данных:
#  Count - это количество строк, которые имеют значение.
#  В нашем случае у каждого пассажира есть значение для каждого из столбцов,
#  поэтому значение равно 887 (общее количество пассажиров).
#  Среднее значение: Напомним, что среднее значение является стандартным средним значением.
#  Std : это сокращение от стандартного отклонения. Это мера того, насколько разбросаны данные.
#  Min : наименьшее значение
#  25% : 25-й процентиль
#  50% : 50-й процентиль, также известный как медиана.
#  75% : 75-й процентиль
#  Макс : наибольшее значение
#  Мы используем метод описания Pandas, чтобы начать интуитивно понимать наши данные.

# TODO: Selecting a Single Column (Выбор одного столбца)
#  Часто нам нужно иметь дело только с некоторыми столбцами, которые есть в нашем наборе данных.
#  Чтобы выбрать один столбец, мы используем квадратные скобки и имя столбца.
#  В этом примере мы выбираем только столбец с пассажирскими тарифами.
# col = df['Fare']
# print(col)
# TODO: Результатом является то, что мы называем Pandas Series.
#  Серия похожа на DataFrame, но это всего лишь один столбец.
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# col = df['Fare']
# print(col)
# TODO: Серия Pandas — это один столбец из Pandas DataFrame.

# TODO: Selecting Multiple Columns (Выбор нескольких столбцов)
#  Мы также можем выбрать несколько столбцов из нашего исходного DataFrame, создав меньший DataFrame.
#  Мы собираемся выбрать только столбцы Age, Sex и Survived из нашего исходного DataFrame.
#  Мы помещаем эти значения в список следующим образом:
#      ['Age', 'Sex', 'Survived']
# TODO: Теперь мы используем этот список внутри скобочной нотации df[...]
#  При печати большого DataFrame, который слишком велик для отображения,
#  вы можете использовать метод head для печати только первых 5 строк.
#     small_df = df[['Age', 'Sex', 'Survived']]
#     print(small_df.head())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# small_df = df[['Age', 'Sex', 'Survived']]
# print(small_df.head())
# TODO: При выборе одного столбца из Pandas DataFrame мы используем одинарные квадратные скобки.
#  При выборе нескольких столбцов мы используем двойные квадратные скобки.

# TODO: Creating a Column (Создание столбца)
#  Мы часто хотим, чтобы наши данные были в несколько ином формате, чем они были изначально.
#  Например, наши данные имеют пол пассажира в виде строки («мужской» или «женский»).
#  Это легко прочитать человеку, но когда мы позже будем выполнять вычисления с нашими данными,
#  нам понадобятся логические значения (истина и ложь).
#  Мы можем легко создать новый столбец в нашем DataFrame, который имеет значение True,
#  если пассажир — мужчина, и False, если он — женщина.
#  Вспомните синтаксис выбора столбца «Пол»:
#      df['Sex']
# TODO: Мы создаем серию панд, которая будет серией истин и ложностей
#  (истина, если пассажир — мужчина, и ложь, если пассажир — женщина ).
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df['Sex'] == 'male')
# TODO: Теперь мы хотим создать столбец с этим результатом.
#  Чтобы создать новый столбец, мы используем тот же синтаксис квадратных скобок (df['male']),
#  а затем присваиваем ему это новое значение.
#      df['male'] = df['Sex'] == 'male'
# import pandas as pd
#
# pd.options.display.max_columns = 8
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
#
# df['male'] = df['Sex'] == 'male'
# print(df.head())
# # print(df[['Age', 'male', 'Survived']].tail())
# TODO: Наш фрейм данных теперь выглядит следующим образом.
#  Обратите внимание на новый столбец в конце. Часто наши данные не в идеальном формате.
#  К счастью, Pandas позволяет нам легко создавать новые столбцы на основе наших данных,
#  чтобы мы могли их соответствующим образом отформатировать.

# TODO: What is Numpy? (Что такое Нампи?)
#  Numpy — это пакет Python для управления списками и таблицами числовых данных.
#  Мы можем использовать его для выполнения большого количества статистических вычислений.
#  Мы называем список или таблицу данных массивом numpy.
#  Мы часто берем данные из нашего панда DataFrame и помещаем их в массивы numpy.
#  Pandas DataFrames великолепны, потому что у нас есть имена столбцов
#  и другие текстовые данные, которые делают их удобочитаемыми.
#  DataFrame, хотя и легко читается человеком, не является идеальным форматом для выполнения вычислений.
#  Массивы numpy, как правило, менее удобочитаемы для человека,
#  но имеют формат, позволяющий выполнять необходимые вычисления.
#  Numpy — это модуль Python для выполнения вычислений в таблицах данных.
#  На самом деле Pandas был построен с использованием Numpy в качестве основы.

# TODO: Converting from a Pandas Series to a Numpy Array (Преобразование из серии Pandas в массив Numpy)
#  Мы часто начинаем с наших данных в Pandas DataFrame, но затем хотим преобразовать их в массив numpy.
#  Атрибут values делает это за нас. Давайте преобразуем столбец Fare в пустой массив.
#  Сначала мы вспомним, что мы можем использовать нотацию с одной скобкой,
#  чтобы получить серию pandas в столбце Fare следующим образом.
#      df['Fare']
# TODO: Затем мы используем атрибут values, чтобы получить значения в виде массива numpy.
#      df['Fare'].values
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df['Fare'].values)
# print(df['Fare'].head().values)
# TODO: Вот как выглядит приведенный выше массив:
#      array([ 7.25 , 71.2833,  7.925, 53.1, 8.05, 8.4583, …
# TODO: В результате получается одномерный массив.
#  Вы можете сказать это, так как есть только один набор скобок,
#  и он расширяется только по странице (а не вниз).
#  Атрибут значений серии Pandas предоставляет данные в виде массива numpy.

# TODO: Converting from a Pandas DataFrame to a Numpy Array (Преобразование из Pandas DataFrame в массив Numpy)
#  Если у нас есть DataFrame pandas (вместо Series, как в прошлой части),
#  мы все еще можем использовать атрибут values, но он возвращает двумерный массив numpy.
#  Напомним, что мы можем создать меньший DataFrame pandas со следующим синтаксисом.
#      df[['Pclass', 'Fare', 'Age']]
# TODO: Опять же, мы применяем атрибут values, чтобы получить пустой массив.
#      df[['Pclass', 'Fare', 'Age']].values
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df[['Pclass', 'Fare', 'Age']].values)
# TODO: Вот как выглядит приведенный выше массив:
#      array([[ 3.    ,  7.25  , 22.    ],
#             [ 1.    , 71.2833, 38.    ],
#             [ 3.    ,  7.925 , 26.    ],
#                          ...           ,
#             [ 3.    , 23.45  ,  7.    ],
#             [ 1.    , 30.    , 26.    ],
#             [ 3.    ,  7.75  , 32.    ]])
# TODO: Это двумерный массив numpy. Вы можете сказать, потому что есть два набора скобок,
#  и они расширяются как по странице, так и вниз.
#  Атрибут значений Pandas DataFrame предоставляет данные в виде массива 2d numpy.

# TODO: Numpy Shape Attribute (Атрибут Numpy Форма)
#  Мы используем атрибут numpy форма, чтобы определить размер нашего массива numpy.
#  Размер говорит нам, сколько строк и столбцов в наших данных.
#  Во-первых, давайте создадим массив numpy с Pclass, Fare и Age.
#      arr = df[['Pclass', 'Fare', 'Age']].values
# TODO: Если мы посмотрим на форму, мы получим количество строк и количество столбцов:
#      print(arr.shape) #(887, 3)
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr.shape)
# print(df.shape)
# TODO: Этот результат означает, что у нас есть 887 строк и 3 столбца.
#  Используйте атрибут shape, чтобы найти количество строк и количество столбцов для массива Numpy.
#  Вы также можете использовать атрибут формы в DataFrame pandas (df.shape).

# TODO: Select from a Numpy Array (Выберите из массива Numpy)
#  Предположим, мы создали следующий массив numpy:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# # arr = df[['Pclass', 'Fare', 'Age']].tail(10).values
# print(arr)
# TODO: Мы можем выбрать один элемент из массива numpy следующим образом:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0, 1])
# TODO: Это будет 2-й столбец 1-й строки (помните, что мы начинаем считать с 0 ).
#  Таким образом, это будет тариф 1-го пассажира или 7,25.
#  Мы также можем выбрать одну строку, например, весь ряд первого пассажира:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0])
# # print(arr[0, :])
# TODO: Чтобы выбрать один столбец (в данном случае столбец «Возраст»), мы должны использовать специальный синтаксис:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[:, 2])
# TODO:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0, 1])
# print(arr[0])
# print(arr[:, 2])
# TODO: Синтаксис можно интерпретировать так, что мы берем все строки, но только столбец с индексом 2.
#  Используя другой синтаксис в скобках, мы можем выбрать отдельные значения, всю строку или весь столбец.

# TODO: Masking (Маскировка)
#  Часто требуется выбрать все строки, соответствующие определенному критерию.
#  В этом примере мы выберем все строки детей (пассажиры младше 18 лет).
#  Напоминание о нашем определении массива:
#      arr = df[['Pclass', 'Fare', 'Age']].values
# TODO: Напомним, что мы можем получить столбец Age с помощью следующего синтаксиса:
#      arr[:, 2]
# TODO: Сначала мы создаем то, что мы называем маской.
#  Это массив логических значений (True/False), указывающих, является ли пассажир ребенком или нет.
#      mask = arr[:, 2] < 18
# TODO: Давайте посмотрим на массив маски, чтобы убедиться, что мы его понимаем.
#      array([False, False, False, False, False, False, False, True, False, …
# TODO: Значения False означают взрослого, а значения True — ребенка,
#  поэтому первые 7 пассажиров — взрослые, затем 8-й — ребенок, а 9-й — взрослый.
#  Теперь мы используем нашу маску, чтобы выбрать только нужные нам строки:
#      arr[mask]
# TODO: Давайте посмотрим на этот новый массив.
#      array([[3., 21.075, 2.],
#             [2., 30.0708, 14.],
#             [3., 16.7, 4.],
#             [3., 7.8542, 14.],
# TODO: Если мы вспомним, что третий столбец — это возраст пассажиров,
#  мы увидим, что все строки здесь — для пассажиров-детей.
#  Как правило, нам не нужно определять переменную маски, и мы можем сделать это всего в одной строке:
#      arr[arr[:, 2] < 18]
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# # take first 10 values for simplicity
# arr = df[['Pclass', 'Fare', 'Age']].values
#
# mask = arr[:, 2] < 18
# print(arr[mask])
# print(arr[arr[:, 2] < 18])
# TODO: Маска — это логический массив (значения True/False),
#  который сообщает нам, какие значения из массива нас интересуют.

# TODO: Summing and Counting (Суммирование и подсчет)
#  Допустим, мы хотим знать, сколько среди наших пассажиров детей.
#  У нас все еще есть то же определение массива,
#  и мы можем взять нашу маску или логические значения из предыдущей части.
#      arr = df[['Pclass', 'Fare', 'Age']].values
#      mask = arr[:, 2] < 18
# TODO: Напомним, что значения True интерпретируются как 1, а значения False интерпретируются как 0.
#  Таким образом, мы можем просто суммировать массив, и это эквивалентно подсчету количества истинных значений.
#      print(mask.sum())
# TODO: Опять же, мы можем не определять переменную маски.
#      print((arr[:, 2] < 18).sum())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age', 'Survived', 'Sex']].values
# mask = arr[:, 2] < 18
#
# print(mask.sum())
# print((arr[:, 2] < 18).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1)).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] == 'male')).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] == 'female')).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] != 'male')).sum())
# TODO: Суммирование массива логических значений дает количество значений True.

# TODO: Scatter Plot (Точечная диаграмма)
#  Мы можем использовать библиотеку matplotlib для построения графика наших данных.
#  Нанесение данных на график часто может помочь нам создать интуицию в отношении наших данных.
#  Сначала нам нужно импортировать matplotlib. Это стандартная практика называть его plt.
#      import matplotlib.pyplot as plt
# TODO: Мы используем функцию разброса для построения наших данных.
#  Первым аргументом функции рассеяния является ось x (горизонтальное направление),
#  а вторым аргументом — ось y (вертикальное направление).
#      plt.scatter(df['Age'], df['Fare'])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'])
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Это отображает возраст по оси x и стоимость проезда по оси y.
#  Чтобы упростить интерпретацию, мы можем добавить метки x и y.
#      plt.xlabel('Age')
#      plt.ylabel('Fare')
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Мы также можем использовать наши данные для цветового кодирования нашей диаграммы рассеивания.
#  Это даст каждому из 3 классов другой цвет. Мы добавляем параметр c и присваиваем ему серию Pandas.
#  В этом случае наш ряд Pandas имеет 3 возможных значения (1-й, 2-й и 3-й класс),
#  поэтому мы увидим, что наши точки данных получают один из трех цветов.
#      plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# # plt.scatter(df['Age'], df['Fare'], c=[df['Sex'] == 'male'])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Фиолетовые точки относятся к 1-первому классу,
#  зеленые точки ко 2-второму классу,
#  желтые точки к 3-третьему классу.
#  Точечная диаграмма используется для отображения всех значений ваших данных на графике.
#  Чтобы получить визуальное представление наших данных, мы должны ограничить наши данные двумя функциями.

# TODO: Line (Линия)
#  Теперь, когда мы можем поместить отдельные точки данных на график, давайте посмотрим, как нарисовать линию.
#  Функция plot делает именно это. Далее проводится линия, примерно отделяющая 1-й класс от 2-го и 3-го классов.
#  На глазок проведем линию от (0, 85) до (80, 5). Наш синтаксис ниже имеет список значений x и список значений y:
#      plt.plot([0, 80], [85, 5])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# plt.plot([0, 80], [85, 5])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Вы можете видеть, что желтые (3-й класс) и зеленые (2-й класс) точки в основном находятся ниже линии,
#  а фиолетовые (1-й класс) — в основном выше.
#  Мы сделали это вручную, но в следующем модуле мы научимся делать это алгоритмически.
#  В matplotlib мы используем функцию scatter (рассеяния) для создания графика scatter
#  и функцию plot (график) для построения линии.

# TODO: ЗАДАЧА: Machine Learning - What's in a Column? (Машинное обучение — что в столбце?)
#  Получение столбца из массива numpy.
#  Задача Учитывая CSV-файл и имя столбца, распечатать элементы в данном столбце.
#  Формат ввода:
#  Первая строка: имя файла csv
#  Вторая строка: имя столбца
#  Формат вывода Массив Numpy
#  Sample Input:
#  https://sololearn.com/uploads/files/one.csv
#  a
#  File one.csv contents:
#  a,b
#  1,3
#  2,4
#  Sample Output:
#  [1 2]
#  Пояснение: a — это заголовок для первого столбца со значениями [1 2].
# import pandas as pd
#
# filename = input()
# column_name = input()
#
# df = pd.read_csv(filename)
# print(df[column_name].values)

# TODO: Where does Classification Fit in the World of Machine Learning?
#  (Какое место классификация Fit (занимает) в мире машинного обучения?)
#  Машинное обучение на высоком уровне состоит из контролируемого и неконтролируемого обучения.
#  Контролируемое обучение означает, что у нас будут помеченные исторические данные,
#  которые мы будем использовать для информирования нашей модели.
#  Мы называем ярлык или вещь, которую пытаемся предсказать - target (целью).
#  Таким образом, в контролируемом обучении есть известная цель для исторических данных,
#  а в неконтролируемом обучении нет известной цели.
#  В контролируемом обучении есть классификация и регрессия.
#  Проблемы классификации возникают, когда целью является категориальное значение
#  (часто True или False, но может быть несколько категорий).
#  Проблемы регрессии — это когда целью является числовое значение.
#  Например, прогнозирование цен на жилье — это проблема регрессии.
#  Это контролируется, так как у нас есть исторические данные о продажах домов в прошлом.
#  Это регрессия, потому что цена жилья является числовым значением.
#  Предсказание того, не выполнит ли кто-то дефолт по своему кредиту, является проблемой классификации.
#  Опять же, он находится под наблюдением, поскольку у нас есть исторические данные о дефолте прошлых кредиторов,
#  и это проблема классификации, потому что мы пытаемся предсказать,
#  относится ли кредит к одной из двух категорий (дефолт или нет).
#  Логистическая регрессия, хотя в ее названии есть регрессия,
#  представляет собой алгоритм решения задач классификации, а не проблем регрессии.

# TODO: Classification Terminology (Терминология классификации)
#  Давайте вернемся к нашему набору данных Титаника.
#  Вот снова Pandas DataFrame данные:
# import pandas as pd
#
# pd.options.display.max_columns = 7
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head(5))
# TODO:
#  столбец Survived — это то, что мы пытаемся предсказать. Мы называем это целью.
#  Вы можете видеть, что это список из 1 и 0.
#  1 означает, что пассажир выжил, а 0 означает, что пассажир не выжил.
#  Остальные столбцы — это информация о пассажире, которую мы можем использовать для прогнозирования цели.
#  Мы называем каждый из этих столбцов feature (характеристика).
#  Характеристики — это данные, которые мы используем для прогнозирования.
#  Хотя мы знаем, выжил ли каждый пассажир в наборе данных,
#  мы хотели бы иметь возможность делать прогнозы о дополнительных пассажирах,
#  для которых мы не смогли собрать эти данные.
#  Мы построим модель машинного обучения чтобы помочь нам сделать это.
#  Иногда вы будете слышать характеристики, называемые предикторами.

# TODO: Classification Graphically (Классификация графически)
#  В конечном итоге мы захотим использовать все характеристики, но для простоты давайте начнем только
#  с двух характеристик Fare and Age (плата за проезд и возраст).
#  Использование двух характеристик позволяет нам визуализировать данные на графике.
#  По оси абсцисс (X) отложена стоимость проезда пассажира, а по оси ординат (Y) — его возраст.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([32, 100], [0, 80])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Желтые точки — пассажиры, которые выжили, а фиолетовые точки — пассажиры, которые не выжили.
#  Вы можете видеть, что желтых точек внизу графика больше, чем вверху.
#  Это потому, что у детей было больше шансов выжить, чем у взрослых, что соответствует нашей интуиции.
#  Точно так же справа на графике больше желтых точек, что означает,
#  что люди, которые платили больше, имели больше шансов выжить.
# TODO: Задача линейной модели состоит в том, чтобы найти линию, которая наилучшим образом разделяет два класса так,
#  чтобы желтые точки находились с одной стороны, а фиолетовые — с другой. Вот пример хорошей линии:
#      plt.plot([32, 100], [0, 80])
#  Линия используется для прогнозирования появления новых пассажиров.
#  Если точка данных пассажира находится на правой стороне линии, мы прогнозируем, что он выживет.
#  Если на левой стороне, мы бы предсказали, что они не выжили.
#  Задача построения модели будет заключаться в том, чтобы определить наилучшую возможную линию.

# TODO: Equation for the Line (Уравнение для линии)
#  Линия определяется уравнением в следующем виде:
#      0 = ax + by + c
# TODO: Значения a, b и c являются коэффициентами. Любые три значения будут определять уникальную строку.
#  Давайте рассмотрим конкретный пример строки, где коэффициенты равны a=1, b=-1 и c=-30.
#      0 = (1)x + (-1)y + (-30)
# TODO: Вот три коэффициента: 1, -1, -30.
#  Вспомним, что мы наносили наши данные по оси x на тариф и по оси y на возраст пассажира.
#  Чтобы нарисовать прямую из уравнения, нам нужны две точки, лежащие на прямой.
#  Мы можем видеть, например, что точка (30, 0) лежит прямо на линии (Fare 30, Age 0).
#  Если мы подставим это в уравнение, оно сработает.
#      30 - 0 - 30 = 0
# TODO: Мы также можем видеть, что точка (50, 20) находится на линии (Fare 50, Age 20).
#      50 - 20 - 30 = 0
# TODO: Вот как наша линия выглядит на графике.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([30, 50], [0, 20])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Коэффициенты линии определяют, где находится линия.

# TODO: Making a Prediction Based on the Line (Делаем предсказание на основе линии)
#  Давайте снова посмотрим на ту же строку.
#      0 = (1)x + (-1)y - 30
# TODO: Если мы возьмем данные о пассажирах, мы можем использовать это уравнение,
#  чтобы определить, на какую сторону линии они попадают.
#  Например, предположим, что у нас есть пассажир, тариф которого равен 100, а возраст 20.
#  Подставим эти значения в наше уравнение:
#      (1)100 + (-1)20 - 30 = 100 - 20 - 30 = 50
# TODO: Поскольку это значение положительное, точка находится на правой стороне линии,
#  и мы предполагаем, что пассажир выжил.
#  Теперь предположим, что тариф для пассажира равен 10, а его возраст равен 50 годам.
#  Подставим эти значения в уравнение.
#      (1)10 + (-1)50 - 30 = -70
# TODO: Поскольку это значение отрицательное, точка находится на левой стороне линии,
#  и мы предполагаем, что пассажир не выжил. Мы можем видеть эти две зеленые точки на графике ниже.
#  С какой стороны линии находится точка, зависит, по нашему мнению, выживет этот пассажир или нет.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([32, 100], [0, 80])
# plt.scatter([100, 10], [20, 50], c='g')
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()

# TODO: What Makes a Good Line? (Что делает линию хорошей?)
#  Давайте посмотрим на две разные линии. Сначала у нас есть линия, с которой мы работали до сих пор.
#  Назовем эту линию 1.
#      0 = (1)x + (-1)y - 30
# TODO: Далее у нас есть еще одно уравнение для линии.
#  Назовем эту линию 2.
#      0 = (4)x + (5)y - 400
# TODO: Если мы посмотрим на две линии, то увидим, что в строке 1 справа больше желтых точек, а слева больше фиолетовых.
#  У линии 2 не так много точек справа от нее; большинство фиолетовых и желтых точек слева.
#  Это делает линию 1 предпочтительной, поскольку она лучше разделяет желтые и фиолетовые точки.
#  Нам нужно математически определить эту идею, чтобы мы могли алгоритмически найти лучшую линию.
#  Логистическая регрессия — это способ математического поиска наилучшей линии.

# TODO: Probability of Surviving (Вероятность выживания)
#  Чтобы определить наилучшую возможную линию для разделения наших данных, нам нужен способ оценки линии.
#  Во-первых, давайте посмотрим на одну точку данных. В идеале, если точка данных — это выживший пассажир,
#  он должен находиться с правой стороны очереди и далеко от нее. Если это точка данных для пассажира,
#  который не выжил, она будет далеко от линии слева. Чем дальше он от линии, тем больше мы уверены,
#  что он находится на правильной стороне линии.
#  Для каждой точки данных у нас будет оценка со значением от 0 до 1.
#  Мы можем думать об этом как о вероятности чтобы пассажир выжил.
#  Если значение близко к 0, эта точка будет далеко слева от линии,
#  и это означает, что мы уверены, что пассажир не выжил.
#  Если значение близко к 1, эта точка будет далеко справа от линии, и это означает, что мы уверены, что пассажир выжил.
#  Значение 0,5 означает, что точка падает прямо на линию, и мы не уверены, выживет ли пассажир.
#  Уравнение для расчета этой оценки приведено ниже, хотя интуиция для него гораздо важнее фактического уравнения.
#  Напомним, что уравнение для линии имеет вид:
#      0 = ax+by+c
#  x — это Тариф,
#  y — Возраст,
#  a, b, c — коэффициенты, которыми мы управляем.
#  См. Рис: ProbabilitySurvivingPicture.png
#  Число e — математическая константа, приблизительно равная 2.71828
#  Эта функция называется сигмоидой.
# TODO: Логистическая регрессия дает не просто прогноз (выжил или нет),
#  а вероятность (вероятность 80%, что этот человек выжил).

# TODO: Likelihood (Вероятность)
#  Чтобы рассчитать, насколько хороша наша линия, нам нужно оценить, верны ли наши прогнозы.
#  В идеале, если мы прогнозируем с высокой вероятностью, что пассажир выживет
#  (это означает, что точка данных находится далеко справа от линии), то этот пассажир действительно выживает.
#  Таким образом, мы будем вознаграждены - когда предскажем что-то правильно,
#  и наказаны - если предскажем что-то неправильно.
#  Вот уравнение вероятности. Хотя опять же, интуиция важнее уравнения.
#  См. Рис: LikelihoodPicture_1.png
#  Здесь p — прогнозируемая вероятность выживания из предыдущей части.
#  Вероятность будет иметь значение от 0 до 1.
#  Чем выше значение, тем лучше наша линия.
#  Давайте рассмотрим пару возможностей:
#  • Если прогнозируемая вероятность p равна 0.25, а пассажир не выжил, мы получаем 0.75 балла (хорошо).
#  • Если прогнозируемая вероятность p равна 0.25 и пассажир выжил, мы получаем оценку 0.25 (плохо).
#   Мы умножаем все отдельные оценки для каждой точки данных вместе, чтобы получить оценку для нашей линии.
#   Таким образом, мы можем сравнивать разные линии, чтобы определить лучшую.
#   Скажем для простоты вычислений, что у нас есть 4 точки данных.
#   Мы получаем общий балл, умножая четыре балла вместе:
#  См. Рис: LikelihoodPicture_2.png
#      0.25 * 0.75 * 0.6 * 0.8 = 0.09
#  Значение всегда будет очень маленьким, поскольку это вероятность того, что наша модель все предсказывает идеально.
#  Идеальная модель будет иметь прогнозируемую вероятность 1 для всех положительных случаев
#  и 0 для всех отрицательных случаев.
#  Вероятность — это то, как мы оцениваем и сравниваем возможные варианты наиболее подходящей линии.

# TODO: What is Scikit-learn? (Что такое Scikit-learn?)
#  Теперь, когда мы заложили основы работы логистической регрессии, давайте углубимся в код для построения модели.
#  Для этого мы представим новый модуль Python под названием scikit-learn. Scikit-learn, часто сокращаемый до sklearn,
#  — это наш научный инструментарий. Все основные алгоритмы машинного обучения реализованы в sklearn.
#  Мы увидим, что с помощью всего нескольких строк кода мы можем построить несколько различных мощных моделей.
#  Обратите внимание, что scikit-learn постоянно обновляется.
#  Если у вас на компьютере установлена немного другая версия модуля, все будет работать корректно,
#  но вы можете увидеть немного другие значения, чем на игровой площадке.
#  Scikit-learn — один из лучших документированных модулей Python.
#  Вы можете найти множество примеров кода на scikit-learn.org.
#  https://scikit-learn.org/stable/

# TODO: Prep Data with Pandas (Подготовьте данные с помощью Pandas)
#  Прежде чем мы сможем использовать sklearn для построения модели, нам нужно подготовить данные с помощью Pandas.
#  Вернемся к нашему полному набору данных и рассмотрим команды Pandas. Вот кадр данных Pandas со всеми столбцами:
# import pandas as pd
#
# pd.options.display.max_columns = 7
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head(5))
# TODO: во- первых, нам нужно сделать все наши столбцы числовыми. Вспомните, как создать булев столбец для пола.
#      df['male'] = df['Sex'] == 'male'
# TODO: Теперь давайте возьмем все характеристики и создадим пустой массив с именем X.
#  Сначала мы выберем все интересующие нас столбцы,
#  а затем используем метод значений, чтобы преобразовать его в пустой массив.
#      X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# TODO: Теперь возьмем цель (столбец Survived) и сохраним ее в переменной y.
#      y = df['Survived'].values
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# print(X)
# print(y)
# TODO: Стандартной практикой является вызов нашего двумерного массива характеристик X
#  и одномерного массива целевых значений y.

# TODO: Build a Logistic Regression Model with Sklearn (Создайте модель логистической регрессии с помощью Sklearn)
#  Начнем с импорта модели логистической регрессии:
#      from sklearn.linear_model import LogisticRegression
# TODO: Все модели sklearn построены как классы Python. Сначала мы создаем экземпляр класса.
#      model = LogisticRegression()
# TODO: Теперь мы можем использовать наши данные, которые мы ранее подготовили, для обучения модели.
#  Для построения модели используется метод подгонки.
#  Он принимает два аргумента: X (характеристики в виде массива 2d numpy) и y (цель в виде массива 1d numpy).
#  Для простоты давайте сначала предположим, что мы строим модель логистической регрессии,
#  используя только столбцы «Тариф» и «Возраст».
#  Сначала мы определяем X как матрицу признаков, а y как целевой массив.
#      X = df[['Fare', 'Age']].values
#      y = df['Survived'].values
# TODO: Теперь мы используем метод подгонки для построения модели.
#      model.fit(X, y)
# TODO: Подгонка модели означает использование данных для выбора линии наилучшего соответствия.
#  Мы можем видеть коэффициенты с атрибутами coef_ и intercept_.
#      print(model.coef_, model.intercept_)
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# X = df[['Fare', 'Age']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# print(model.coef_, model.intercept_)
# # [[ 0.01615949 -0.01549065]] [-0.51037152]
#
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# # plt.plot([32, 100], [0, 80])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Эти значения означают, что уравнение выглядит следующим образом:
#      0 = 0.0161594x + -0.01549065y + -0.51037152
# TODO: Вот линия, нарисованная на графике.
#  Вы можете видеть, что он делает достойную (но не отличную) работу по разделению желтых и фиолетовых точек.
#  Мы немного поставили себя в тупик, используя только 2 из наших функций,
#  поэтому в следующих частях мы будем использовать все функции.
#  Может быть трудно запомнить операторы импорта для разных моделей sklearn.
#  Если не можете вспомнить, просто посмотрите документацию scikit-learn.
#  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

# TODO: Make Predictions with the Model (Делайте прогнозы с помощью модели)
#  Мы действительно усложнили нашу модель тем, что использовали только две характеристики из предыдущих частей,
#  поэтому давайте перестроим модель, используя их все.
#      X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
#      y = df['Survived'].values
#      model = LogisticRegression()
#      model.fit(X, y)
# TODO: Теперь мы можем использовать метод predict() для прогнозирования.
#      model.predict(X)
# TODO: Первый пассажир в наборе данных:
#      [3, True, 22.0, 1, 0, 7.25]
# TODO: Это означает, что пассажир относится к классу P 3, является мужчиной, ему 22 года,
#  у него на борту 1 брат/сестра/супруга, 0 родителей/ребенка на борту и он заплатил 7.25 долларов США.
#  Посмотрим, что предсказывает модель для этого пассажира.
#  Обратите внимание, что даже с одной точкой данных метод прогнозирования принимает
#  двумерный массив numpy и возвращает одномерный массив numpy.
#      print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
#      # [0]
# TODO: Результат равен 0, что означает, что модель предсказывает, что этот пассажир не выжил.
#  Давайте посмотрим, что предсказывает модель для первых 5 строк данных, и сравним их с нашим целевым массивом.
#  Мы получаем первые 5 строк данных с помощью X[:5] и первые 5 значений цели с помощью y[:5].
#      print(model.predict(X[:5]))
#      # [0 1 1 1 0]
#      print(y[:5])
#      # [0 1 1 1 0]
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
# print(model.predict(X[:5]))
# print(y[:5])
# TODO: Мы видим, что он получил все 5 правильных!
#  Метод предсказания возвращает массив из 1 и 0, где 1 означает, что модель предсказывает,
#  что пассажир выжил, а 0 означает, что модель предсказывает, что пассажир не выжил.

# TODO: Score the Model (Оценка модели)
#  Мы можем понять, насколько хороша наша модель, подсчитав количество точек данных, которые она правильно предсказывает
#  Это называется показателем точности. Давайте создадим массив с предсказанными значениями y.
#      y_pred = model.predict(X)
# TODO: Теперь мы создаем массив логических значений того, правильно ли наша модель предсказала каждого пассажира.
#      y == y_pred
# TODO: Чтобы получить число из них, которые верны, мы можем использовать метод numpy sum.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print(y.size)
# print(y.shape[0])
# print((y == y_pred).sum())
# TODO: Это означает, что из 887 точек данных модель делает правильный прогноз для 714 из них.
#  Чтобы получить правильный процент, мы делим его на общее количество пассажиров.
#  Мы получаем общее количество пассажиров, используя атрибут shape.
#      y.shape[0]
# TODO: Таким образом, наша оценка точности вычисляется следующим образом.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print((y == y_pred).sum() / y.shape[0])
# TODO: Таким образом, точность модели составляет 80%.
#  Другими словами, модель делает правильный прогноз для 80% точек данных.
#  Это достаточно распространенный расчет, который sklearn уже реализовал за нас.
#  Таким образом, мы можем получить тот же результат, используя метод score.
#  Метод оценки использует модель для предсказания X и подсчитывает, какой процент из них соответствует y.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print(model.score(X, y))
# TODO: При таком альтернативном методе расчета точности мы получаем то же значение, 80%.
#  Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print((y == y_pred).sum())
# print((y == y_pred).sum() / y.shape[0])
# print(model.score(X, y))
# TODO: В следующем модуле мы увидим, что оценка модели — это гораздо больше.

# TODO: Introducing the Breast Cancer Dataset (Представляем набор данных по раку молочной железы)
#  Теперь, когда мы создали инструменты для построения модели логистической регрессии для набора данных классификации,
#  мы представим новый набор данных. В наборе данных о раке молочной железы каждая точка данных имеет
#  измерения на основе изображения массы молочной железы и того, является ли она злокачественной.
#  Цель будет заключаться в том, чтобы использовать эти измерения, чтобы предсказать, является ли образование раковым.
#  Этот набор данных встроен прямо в scikit-learn, поэтому нам не нужно будет читать CSV.
#  Давайте начнем с загрузки набора данных и просмотра данных и их форматирования.
#      from sklearn.datasets import load_breast_cancer
#      cancer_data = load_breast_cancer()
# TODO: Возвращаемый объект (который мы сохранили в переменной Cancer_data) представляет собой объект,
#  похожий на словарь Python. Мы можем увидеть доступные ключи с помощью метода keys.
#      print(cancer_data.keys())
# TODO: Мы начнем с просмотра DESCR, который дает подробное описание набора данных.
#      print(cancer_data['DESCR'])
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
# print(cancer_data.keys())
# print(cancer_data['DESCR'])
# TODO: Мы видим, что есть 30 характеристик, 569 точек данных, и цель является либо злокачественной (раковой),
#  либо доброкачественной (не раковой). Для каждой из точек данных у нас есть измерения массы груди
#  (радиус, текстура, периметр и т. д.). Для каждого из 10 измерений было вычислено несколько значений,
#  поэтому у нас есть среднее значение, стандартная ошибка и наихудшее значение.
#  Это приводит к 10 * 3 или 30 общим характеристикам.
#  В наборе данных о раке молочной железы есть несколько характеристик, которые рассчитываются на основе других столбцов
#  Процесс выяснения того, какие дополнительные характеристики следует рассчитать,
#  называется проектированием характеристик.

# TODO: Loading the Data into Pandas (Загрузка данных в Pandas)
#  Давайте вытащим характеристики и целевые данные из объекта Cancer_data.
#  Во-первых, данные объекта сохраняются с помощью ключа «данные».
#  Когда мы смотрим на него, мы видим, что это пустой массив с 569 строками и 30 столбцами.
#  Это потому, что у нас есть 569 точек данных и 30 характеристик. Ниже приведен массив данных.
#      cancer_data['data']
# TODO: Мы используем shape (форму), чтобы увидеть, что это массив с 569 строками и 30 столбцами.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['data'].shape)
# TODO: Чтобы поместить это в Pandas DataFrame и сделать его более удобочитаемым, нам нужны имена столбцов.
#  Они сохраняются с ключом «feature_names».
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['feature_names'])
# TODO: Теперь мы можем создать Pandas DataFrame со всеми данными наших характеристик.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# # pd.options.display.max_columns = 30
# cancer_data = load_breast_cancer()
#
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# print(df.head())
# TODO: мы видим, что у нас есть 30 столбцов в DataFrame, так как у нас есть 30 характеристик.
#  Вывод усекается, чтобы поместиться на экране.
#  Мы использовали метод head(), поэтому наш результат имеет только 5 точек данных.
#  Нам все еще нужно поместить целевые данные в наш DataFrame, который можно найти с помощью ключа target.
#  Мы видим, что цель представляет собой одномерный массив numpy из 1 и 0.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target'])
# TODO: Если мы посмотрим на форму массива, то увидим, что это одномерный массив с 569 значениями
#  (именно столько у нас было точек данных).
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target'].shape)
# TODO: Чтобы интерпретировать эти 1 и 0, нам нужно знать, являются ли 1 или 0 доброкачественными или злокачественными.
#  Это дается target_names
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target_names'])
# TODO: Это дает массив ['злокачественный' 'доброкачественный'], который говорит нам, что 0 означает злокачественный,
#  а 1 означает доброкачественный. Давайте добавим эти данные в Pandas DataFrame.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# df['target'] = cancer_data['target']
# print(df.head())
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# # pd.options.display.max_columns = 30
# cancer_data = load_breast_cancer()
#
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
# print(df.head())
# TODO: Важно дважды проверить, правильно ли вы интерпретируете логические столбцы.
#  В нашем случае цель 0 означает злокачественную опухоль, а 1 означает доброкачественную.

# TODO: Build a Logistic Regression Model (Создайте модель логистической регрессии)
#  Теперь, когда мы просмотрели наши данные и привели их в удобный формат,
#  мы можем построить нашу матрицу характеристик X и целевой массив y,
#  чтобы мы могли построить модель логистической регрессии.
#      X = df[cancer_data.feature_names].values
#      y = df['target'].values
# TODO: Теперь мы создаем объект логистической регрессии и используем метод подгонки для построения модели.
#      model = LogisticRegression()
#      model.fit(X, y)
# TODO: Когда мы запускаем этот код, мы получаем предупреждение о конвергенции.
#  Это означает, что модели требуется больше времени, чтобы найти оптимальное решение.
#  Один из вариантов — увеличить количество итераций. Вы также можете переключиться на другой решатель,
#  что мы и сделаем. Решатель — это алгоритм, который модель использует для нахождения уравнения линии.
#  Вы можете увидеть возможные решатели в документации по логистической регрессии.
#  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
#                      decision_function(X) - Прогнозировать показатели достоверности для образцов.
#                      densify() - Преобразование матрицы коэффициентов в формат плотного массива.
#                      fit(X, y[, sample_weight]) - Соответствуйте модели в соответствии с данными обучения.
#                      get_params([deep]) - Получить параметры для этого оценщика.
#                      predict(X) - Предсказать метки класса для образцов в X.
#                      predict_log_proba(X) - Предсказать логарифм оценок вероятности.
#                      predict_proba(X) - Оценки вероятности.
#                      score(X, y[, sample_weight]) - Возвращает среднюю точность для заданных тестовых данных и меток.
#                      set_params(**params) - Установите параметры этой оценки.
#                      sparsify() - Преобразование матрицы коэффициентов в разреженный формат.
#      model = LogisticRegression(solver='liblinear')
#      model.fit(X, y)
# TODO: Давайте посмотрим, что предсказывает модель для первой точки данных в нашем наборе данных.
#  Напомним, что метод предсказания принимает двумерный массив, поэтому мы должны поместить точку данных в список.
#      model.predict([X[0]])
# TODO: Таким образом, модель предсказывает, что первая точка данных является злокачественной.
#  Чтобы увидеть, насколько хорошо модель работает со всем набором данных,
#  мы используем метод оценки, чтобы увидеть точность модели.
#      model.score(X, y)
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.linear_model import LogisticRegression
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# model = LogisticRegression(solver='liblinear')
# model.fit(X, y)
# print("prediction for datapoint 0:", model.predict([X[0]]))
# print(model.score(X, y))
# TODO: Мы видим, что модель правильно получает 96% точек данных.
#  С помощью разработанных нами инструментов мы можем построить модель для любого набора классификационных данных.

# TODO: ЗАДАЧА: Machine Learning - Bob the Builder (Машинное обучение — Боб Строитель)
#  Построение модели логистической регрессии.
#  Задача Вам дается матрица признаков и одна точка данных для прогнозирования.
#  Ваша задача будет состоять в том, чтобы построить модель логистической регрессии с матрицей признаков
#  и сделать прогноз (1 или 0) для одной точки данных.
#  Формат ввода:
#  Первая строка: количество точек данных в матрице признаков (n)
#  Следующие n строк: значения строки в матрице признаков, разделенные пробелами
#  Следующая строка: целевые значения, разделенные пробелами
#  Последняя строка: значения (разделенные пробелами) одна точка данных без целевого значения
#  Формат вывода 1 или 0
#  Sample Input:
#  6
#  1 3
#  3 5
#  5 7
#  3 1
#  5 3
#  7 5
#  1 1 1 0 0 0
#  2 4
#  Sample Output:
#  1
#  Пояснение:
#  Мы видим точки, нанесенные на график выше, и линию, разделяющую данные.
#  Точка (2, 4) отмечена на графике, и вы можете видеть,
#  что она находится на положительной стороне линии, поэтому результат равен 1.
# from sklearn.linear_model import LogisticRegression
#
# n = int(input())
# X = []
# for i in range(n):
#     X.append([float(x) for x in input().split()])
# y = [int(x) for x in input().split()]
# datapoint = [float(x) for x in input().split()]
#
# model = LogisticRegression()
# model.fit(X, y)
# print(*model.predict([datapoint]))

# TODO: Accuracy (Точность)
#  В предыдущем модуле мы рассчитали, насколько хорошо наша модель работает с точностью.
#  Точность — это процент верных прогнозов.
#  Если у вас есть 100 точек данных и вы предсказываете 70 из них правильно и 30 неправильно, точность составляет 70%.
#  Точность — очень простая и понятная метрика, однако она не всегда самая лучшая.
#  Например, предположим, что у меня есть модель для прогнозирования мошеннических списаний с кредитной карты.
#  Из 10000 кредитных карт у нас есть 9900 законных списаний и 100 мошеннических списаний.
#  Я мог бы построить модель, которая просто предсказывает, что каждое отдельное обвинение является законным,
#  и она оправдала бы 9900/10000 (99%) прогнозов!
#  Точность является хорошей мерой, если наши классы разделены поровну, но она вводит в заблуждение,
#  если у нас несбалансированные классы.
#  Всегда проявляйте осторожность с точностью.
#  Вам нужно знать распределение классов, чтобы знать, как интерпретировать значение.

# TODO: Confusion Matrix (Матрица путаницы)
#  Как мы заметили в предыдущей части, нас заботит не только то, для скольких точек данных мы предсказываем
#  правильный класс, нас заботит, сколько положительных точек данных мы предсказываем правильно,
#  а также сколько отрицательных точек данных мы предсказываем правильно.
#  Мы можем увидеть все важные значения в так называемой матрице путаницы (или матрице ошибок, или таблице путаницы).
#  Матрица путаницы представляет собой таблицу, показывающую четыре значения:
#  • Точки данных, которые мы предсказывали как положительные, но на самом деле положительные
#  • Точки данных, которые мы предсказывали как положительные, но на самом деле отрицательные
#  • Точки данных, которые мы прогнозировали как отрицательные, но на самом деле положительные
#  • Точки данных, которые мы прогнозировали как отрицательные, но на самом деле отрицательные
#  Первая и четвертая — это точки данных, которые мы предсказали правильно, а вторая и третья — точки данных,
#  которые мы предсказали неправильно.
#  В нашем наборе данных Титаника у нас есть 887 пассажиров, 342 выжили (положительно) и 545 не выжили (отрицательно).
#  Модель, которую мы построили в предыдущем модуле, имеет следующую матрицу путаницы.
#      См. Рис: ConfusionMatrixPicture_1.png
#  Заштрихованные синим квадраты — это количество верных прогнозов.
#  Таким образом, из 342 выживших пассажиров мы предсказали 233 или их правильно (и 109 из них неверно).
#  Из 545 пассажиров, которые не выжили, мы правильно предсказали 480 (и 65 неверно).
#  Мы можем использовать матрицу путаницы для вычисления точности.
#  Напоминаем, что точность — это количество правильно предсказанных точек данных,
#  деленное на общее количество точек данных.
#      (233+480)/(233+65+109+480) = 713/887 = 80.38%
# TODO: Это действительно то же самое значение, которое мы получили в предыдущем модуле.
#  Матрица путаницы полностью описывает, как модель работает с набором данных,
#  хотя ее сложно использовать для сравнения моделей.

# TODO: True Positives, True Negatives, False Positives, False Negatives (TP Истинные позитивы, TN Истинные негативы,
#  FP Ложноположительные, FN Ложноотрицательные)
#  У нас есть имена для каждого квадрата матрицы путаницы.
#  Истинный положительный результат (TP) — это точка данных,
#  которую мы предсказали положительно и в отношении которой мы были правы.
#  Истинный отрицательный результат (TN) — это точка данных, которую мы предсказали отрицательно
#  и в отношении которой мы были правы.
#  Ложноположительный результат (FP) — это точка данных, которую мы предсказали положительно,
#  в отношении которой мы ошиблись.
#  Ложноотрицательный результат (FN) — это точка данных, которую мы предсказали отрицательно,
#  но в отношении которой мы ошиблись.
#  Условия могут быть немного сложными для отслеживания.
#  Способ запомнить, что второе слово — это наш прогноз (положительный или отрицательный),
#  а первое слово — то, был ли этот прогноз правильным (истинным или ложным).
#  Вы часто будете видеть матрицу путаницы, описанную следующим образом:
#      См. Рис: ConfusionMatrixPicture_2.png
#  TODO: Четыре значения матрицы путаницы (TP, TN, FP, FN) используются для вычисления нескольких различных показателей,
#   которые мы будем использовать позже.
