# TODO: Machine Learning (Машинное обучение)
#  Узнайте, как создавать интеллектуальные системы рекомендаций,
#  которые помогают нам в повседневной жизни.
#  Добро пожаловать в машинное обучение.

# TODO: Welcome to Machine Learning (Добро пожаловать в машинное обучение)
#  Поздравляем! Вы сделали большой шаг к тому, чтобы стать практиком машинного обучения!
#  В дополнение к прохождению этого курса не забудьте воспользоваться всей поддержкой обучения,
#  доступной вам на SoloLearn, включая ежедневные советы, практики «Попробуйте сами», задачи тренера по коду,
#  игровую площадку для кода и участие в нашем замечательном сообществе учащихся. Мы рады услышать от вас,
#  поэтому, пожалуйста, оставляйте комментарии и отзывы, когда вы учитесь с нами.
#  Python — это язык программирования, который мы будем использовать на протяжении всего курса.
#  Давайте начнем!

# TODO: Machine Learning Overview (Обзор машинного обучения)
#  Добро пожаловать на курс машинного обучения! Машинное обучение — это способ сбора данных и превращения их в идеи.
#  Мы используем мощность компьютера для анализа примеров из прошлого, чтобы построить модель,
#  которая может предсказать результат для новых примеров. Мы сталкиваемся с моделями машинного обучения каждый день.
#  Например, когда Netflix рекомендует вам шоу, они использовали модель, основанную на том,
#  что вы и другие пользователи смотрели, чтобы предсказать, что вы хотели бы.
#  Когда Amazon выбирает цену для товара, они используют модель,
#  основанную на том, как подобные товары продавались в прошлом.
#  Когда компания, выпустившая вашу кредитную карту, звонит вам из-за подозрительной активности,
#  она использует модель, основанную на вашей прошлой активности, для распознавания аномального поведения.
#  В этом курсе мы изучим несколько методов решения задач машинного обучения.
#  Машинное обучение можно использовать для создания чат-бота, обнаружения спама или распознавания изображений.

# TODO: Course Basics (Основы курса)
#  Одним из наиболее распространенных языков, используемых профессионалами в области машинного обучения, является Python
#  Это очень доступно и очень мощно, поэтому мы будем использовать его в этом курсе.
#  Мы предполагаем рабочее знание Python. В этом курсе мы будем использовать несколько пакетов Python,
#  которые помогут решить задачи машинного обучения. Мы будем использовать pandas, numpy, matplotlib и scikit-learn.
#  Pandas используется для чтения данных и обработки данных, numpy используется для вычисления числовых данных,
#  matplotlib используется для построения графиков данных, а scikit-learn используется для моделей машинного обучения.
#  Каждый из этих пакетов довольно обширен, но мы рассмотрим функции, которые будем использовать.
#  Мы также рассмотрим некоторые основные статистические данные, поскольку они являются основой машинного обучения.
#  Курс будет охватывать как теорию, так и практику методов машинного обучения, но сосредоточится на том,
#  как их использовать на реальных примерах.

# TODO: What's in this Course? (Что в этом курсе?)
#  В машинном обучении мы говорим о контролируемом и неконтролируемом обучении.
#  Обучение с учителем — это когда у нас есть известная цель на основе прошлых данных
#  (например, прогнозирование цены, по которой будет продаваться дом),
#  а обучение без учителя — это когда нет известного прошлого ответа
#  (например, определение тем, обсуждаемых в обзорах ресторанов).
#  В этом курсе мы сосредоточимся на контролируемом обучении.
#  В контролируемом обучении существуют проблемы классификации и регрессии.
#  Регрессия прогнозирует числовое значение (например, прогнозирует, по какой цене будет продаваться дом)
#  и классифицирует предсказывает, к какому классу что-либо принадлежит
#  (например, предсказывает, не выполнит ли заемщик дефолт по своему кредиту).
#  Мы сосредоточимся на проблемах классификации.
#  Это задачи, в которых мы предсказываем, к какому классу что-то принадлежит.
#  Наши примеры будут включать:
#  • Прогнозирование того, кто выживет при крушении «Титаника»
#  • Определение рукописной цифры по изображению
#  • Использование данных биопсии для определения злокачественности новообразования
#  Мы будем использовать ряд популярных методов для решения этих проблем.
#  Мы рассмотрим каждый из них более подробно в следующих модулях:
#  • Логистическая регрессия
#  • Деревья решений
#  • Случайные леса
#  • Нейронные сети
#  В конце этого курса вы сможете взять классификационный набор данных
#  и использовать Python для создания нескольких различных моделей,
#  чтобы определить лучшую модель для данной проблемы.
#  Машинное обучение можно использовать для решения широкого круга задач.
#  Этот курс будет посвящен контролируемому обучению и классификации.

# TODO: Averages (Средние значения)
#  При работе с данными нам часто нужно вычислить некоторые простые статистические данные.
#  Допустим, у нас есть список возрастов людей в классе.
#  Мы располагаем их в порядке возрастания, так будет проще производить расчеты.
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: Среднее значение является наиболее известным средним значением.
#  Сложите все значения и разделите на количество значений:
#      (15 + 16 + 18 + 19 + 22 + 24 + 29 + 30 + 34) / 9 =  207/9 = 23
# TODO: Медиана — это значение посередине упорядоченных чисел.
#  В этом случае, поскольку имеется 9 значений, среднее значение 5-тое, то есть 22.
#  В статистике как среднее, так и медиана называются средними.
#  Среднее значение для непрофессионала является средним.

# TODO: Percentiles (процентили)
#  Медиану также можно рассматривать как 50-й процентиль.
#  Это означает, что 50% данных меньше медианы, а 50% данных больше медианы.
#  Это говорит нам, где находится середина данных, но нам часто требуется больше понимания распределения данных.
#  Мы часто будем рассматривать 25 -й процентиль и 75 -й процентиль.
#  25 - й процентиль — это значение, которое составляет одну четверть пути через данные.
#  Это значение, при котором 25% данных меньше его (а 75% данных больше его).
#  Точно так же 75 -й процентиль составляет три четверти пути через данные.
#  Это значение, при котором 75% данных меньше его (а 25% данных больше его).
#  Если мы снова посмотрим на наш возраст:
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: У нас есть 9 значений, поэтому 25% данных будут составлять примерно 2 точки данных.
#  Таким образом, третья точка данных превышает 25% данных. Таким образом, 25-й процентиль равен 18 (3-я точка данных).
#  Точно так же 75% данных составляют примерно 6 точек данных. Таким образом, 7-я точка данных превышает 75% данных.
#  Таким образом, 75-й процентиль равен 29 (7-я точка данных).
#  Полный диапазон наших данных находится в диапазоне от 15 до 34.
#  25-й и 75-й процентили говорят нам, что половина наших данных находится в диапазоне от 18 до 29.
#  Это помогает нам понять, как распределяются данные.
#  Если имеется четное количество точек данных, чтобы найти медиану (или 50-й процентиль),
#  вы берете среднее значение двух значений в середине.

# TODO: Standard Deviation & Variance (Стандартное отклонение и дисперсия)
#  Мы можем получить более глубокое понимание распределения наших данных со стандартным отклонением и дисперсией.
#  Стандартное отклонение и дисперсия — это меры того, насколько разбросаны или рассредоточены данные.
#  Мы измеряем, насколько далеко каждая точка данных от среднего.
#  Давайте еще раз посмотрим на нашу возрастную группу:
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: Напомним, что среднее равно 23. Давайте подсчитаем, насколько далеко каждое значение от среднего.
#  15 на 8 отличается от среднего (поскольку 23-15=8).
#  Вот список всех этих расстояний:
#      8, 7, 5, 4, 1, 1, 6, 7, 11
# TODO: Мы возводим эти значения в квадрат и складываем их вместе.
#  См. Рис: StandardDeviationPicture1.png
#  Мы делим это значение на общее количество значений, и это дает нам дисперсию.
#      362 / 9 = 40.22
# TODO: Чтобы получить стандартное отклонение, мы просто возьмем квадратный корень из этого числа и получим: 6.34
#  Если наши данные нормально распределены, как показано на графике ниже (См. Рис: StandardDeviationPicture2.png),
#  68% населения находится в пределах одного стандартного отклонения от среднего.
#  На графике мы выделили область в пределах одного стандартного отклонения от среднего значения.
#  Вы можете видеть, что заштрихованная область составляет около двух третей (точнее 68%) от общей площади под кривой.
#  Если мы предположим, что наши данные нормально распределены, мы можем сказать,
#  что 68% данных находятся в пределах 1 стандартного отклонения от среднего.
# TODO: В нашем примере с возрастом, хотя возрасты, вероятно, не совсем нормально распределены,
#  мы предполагаем, что это так, и говорим, что примерно 68% населения имеет возраст
#  в пределах одного стандартного отклонения от среднего. Поскольку среднее значение равно 23 годам,
#  а стандартное отклонение равно 6,34, мы можем сказать, что приблизительно 68% возрастов нашей популяции находятся
#  в диапазоне от 16,66 (23 минус 6,34) до 29,34 (23 плюс 6,34).
#  Несмотря на то, что данные никогда не бывают идеальным нормальным распределением,
#  мы все же можем использовать стандартное отклонение, чтобы получить представление о том, как распределяются данные.

# TODO: Statistics with Python (Статистика с Python)
#  Мы можем рассчитать все эти операции с помощью Python. Мы будем использовать пакет Python numpy.
#  Мы будем использовать numpy позже для работы с массивами, а сейчас мы просто будем использовать несколько функций
#  для статистических вычислений: mean, median, centile, std, var.
#  Сначала мы импортируем пакет. Стандартной практикой является псевдоним numpy как np.
#      import numpy as np
# TODO: Давайте инициализируем переменную data, чтобы иметь список возрастов.
#      data = [15, 16, 18, 19, 22, 24, 29, 30, 34]
# TODO: Теперь мы можем использовать функции numpy.
#  Для функций среднего, медианы, стандартного отклонения и дисперсии мы просто передаем список данных.
#  Для функции процентиля мы передаем список данных и процентиль (в виде числа от 0 до 100).
# import numpy as np
#
# data = [15, 16, 18, 19, 22, 24, 29, 30, 34]
#
# print("mean:", np.mean(data))
# print("median:", np.median(data))
# print("50th percentile (median):", np.percentile(data, 50))
# print("25th percentile:", np.percentile(data, 25))
# print("75th percentile:", np.percentile(data, 75))
# print("standard deviation:", np.std(data).round(2))
# print("variance:", np.var(data).round(2))
# TODO: Numpy — это библиотека Python, которая позволяет быстро и легко выполнять математические операции с массивами.

# TODO: What is Pandas? (Что такое Панды?)
#  Этот курс написан на Python, одном из наиболее часто используемых языков для машинного обучения.
#  Одна из причин, по которой он так популярен, заключается в том,
#  что существует множество полезных модулей Python для работы с данными.
#  Первый, который мы представим, называется Pandas.
#  Pandas — это модуль Python, который помогает нам читать данные и управлять ими.
#  Что хорошо в pandas, так это то, что вы можете брать данные и просматривать их в виде таблицы,
#  удобочитаемой для человека, но их также можно интерпретировать в числовом виде,
#  чтобы вы могли выполнять с ними множество вычислений. Мы называем таблицу данных DataFrame .
#  Python удовлетворит все наши потребности в машинном обучении.
#  Мы будем использовать модуль Pandas для обработки данных.

# TODO: Read in Your Data (Читать в ваших данных)
#  Нам нужно начать с импорта Pandas. Стандартной практикой является прозвище pd, чтобы потом быстрее печатать.
#      import pandas as pd
# TODO: Мы будем работать с набором данных пассажиров Титаника.
#  Для каждого пассажира у нас будут данные о нем, а также о том, выжили ли они в кораблекрушении.
#  Наши данные хранятся в виде файла CSV (значения, разделенные запятыми). Файл titanic.csv находится ниже.
#  Первая строка — это заголовок, а затем каждая последующая строка — это данные для одного пассажира.
#      Survived, Pclass, Sex, Age, Siblings/Spouses, Parents/Children, Fare
#      Выжившие, ПКласс, Пол, Возраст, Братья и сестры/супруги, Родители/Дети, Плата за проезд
#      0, 3, male, 22.0, 1, 0, 7.25
#      1, 1, female, 38.0, 1, 0, 71.2833
#      1, 3, female, 26.0, 0, 0, 7.925
#      1, 1, female, 35.0, 1, 0, 53.1
# TODO: Мы собираемся загружать данные в pandas, чтобы мы могли просматривать их как DataFrame.
#  Функция read_csv берет файл в формате csv и преобразует его в Pandas DataFrame.
#      df = pd.read_csv('titanic.csv')
# TODO: Объект df теперь является нашим фреймом данных pandas с набором данных Titanic.
#  Теперь мы можем использовать метод head для просмотра данных.
#  Метод head возвращает первые 5 строк DataFrame.
#      print(df.head())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head())
# TODO: Обычно данные хранятся в файлах CSV (значения, разделенные запятыми),
#  которые мы можем легко прочитать с помощью функции panda read_csv.
#  Метод head возвращает первые 5 строк.

# TODO: Summarize the Data (Суммарные данные)
#  Обычно наши данные слишком велики, чтобы мы могли отобразить их все.
#  Рассмотрение первых нескольких строк — это первый шаг к пониманию наших данных,
#  но затем мы хотим взглянуть на некоторую сводную статистику.
#  В пандах мы можем использовать метод описания describe().
#  Он возвращает таблицу статистики о столбцах.
#      print(df.describe())
# TODO: Мы добавляем строку в приведенный ниже код, чтобы заставить Python отображать все 6 столбцов.
#  Без линии это будет сокращать результаты.
# import pandas as pd
#
# pd.options.display.max_columns = 6
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.describe())
# TODO: Для каждого столбца мы видим несколько статистических данных.
#  Обратите внимание, что он дает статистику только для числовых столбцов.
#  Давайте рассмотрим, что означает каждая из этих статистических данных:
#  Count - это количество строк, которые имеют значение.
#  В нашем случае у каждого пассажира есть значение для каждого из столбцов,
#  поэтому значение равно 887 (общее количество пассажиров).
#  Среднее значение: Напомним, что среднее значение является стандартным средним значением.
#  Std : это сокращение от стандартного отклонения. Это мера того, насколько разбросаны данные.
#  Min : наименьшее значение
#  25% : 25-й процентиль
#  50% : 50-й процентиль, также известный как медиана.
#  75% : 75-й процентиль
#  Макс : наибольшее значение
#  Мы используем метод описания Pandas, чтобы начать интуитивно понимать наши данные.

# TODO: Selecting a Single Column (Выбор одного столбца)
#  Часто нам нужно иметь дело только с некоторыми столбцами, которые есть в нашем наборе данных.
#  Чтобы выбрать один столбец, мы используем квадратные скобки и имя столбца.
#  В этом примере мы выбираем только столбец с пассажирскими тарифами.
# col = df['Fare']
# print(col)
# TODO: Результатом является то, что мы называем Pandas Series.
#  Серия похожа на DataFrame, но это всего лишь один столбец.
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# col = df['Fare']
# print(col)
# TODO: Серия Pandas — это один столбец из Pandas DataFrame.

# TODO: Selecting Multiple Columns (Выбор нескольких столбцов)
#  Мы также можем выбрать несколько столбцов из нашего исходного DataFrame, создав меньший DataFrame.
#  Мы собираемся выбрать только столбцы Age, Sex и Survived из нашего исходного DataFrame.
#  Мы помещаем эти значения в список следующим образом:
#      ['Age', 'Sex', 'Survived']
# TODO: Теперь мы используем этот список внутри скобочной нотации df[...]
#  При печати большого DataFrame, который слишком велик для отображения,
#  вы можете использовать метод head для печати только первых 5 строк.
#     small_df = df[['Age', 'Sex', 'Survived']]
#     print(small_df.head())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# small_df = df[['Age', 'Sex', 'Survived']]
# print(small_df.head())
# TODO: При выборе одного столбца из Pandas DataFrame мы используем одинарные квадратные скобки.
#  При выборе нескольких столбцов мы используем двойные квадратные скобки.

# TODO: Creating a Column (Создание столбца)
#  Мы часто хотим, чтобы наши данные были в несколько ином формате, чем они были изначально.
#  Например, наши данные имеют пол пассажира в виде строки («мужской» или «женский»).
#  Это легко прочитать человеку, но когда мы позже будем выполнять вычисления с нашими данными,
#  нам понадобятся логические значения (истина и ложь).
#  Мы можем легко создать новый столбец в нашем DataFrame, который имеет значение True,
#  если пассажир — мужчина, и False, если он — женщина.
#  Вспомните синтаксис выбора столбца «Пол»:
#      df['Sex']
# TODO: Мы создаем серию панд, которая будет серией истин и ложностей
#  (истина, если пассажир — мужчина, и ложь, если пассажир — женщина ).
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df['Sex'] == 'male')
# TODO: Теперь мы хотим создать столбец с этим результатом.
#  Чтобы создать новый столбец, мы используем тот же синтаксис квадратных скобок (df['male']),
#  а затем присваиваем ему это новое значение.
#      df['male'] = df['Sex'] == 'male'
# import pandas as pd
#
# pd.options.display.max_columns = 8
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
#
# df['male'] = df['Sex'] == 'male'
# print(df.head())
# # print(df[['Age', 'male', 'Survived']].tail())
# TODO: Наш фрейм данных теперь выглядит следующим образом.
#  Обратите внимание на новый столбец в конце. Часто наши данные не в идеальном формате.
#  К счастью, Pandas позволяет нам легко создавать новые столбцы на основе наших данных,
#  чтобы мы могли их соответствующим образом отформатировать.

# TODO: What is Numpy? (Что такое Нампи?)
#  Numpy — это пакет Python для управления списками и таблицами числовых данных.
#  Мы можем использовать его для выполнения большого количества статистических вычислений.
#  Мы называем список или таблицу данных массивом numpy.
#  Мы часто берем данные из нашего панда DataFrame и помещаем их в массивы numpy.
#  Pandas DataFrames великолепны, потому что у нас есть имена столбцов
#  и другие текстовые данные, которые делают их удобочитаемыми.
#  DataFrame, хотя и легко читается человеком, не является идеальным форматом для выполнения вычислений.
#  Массивы numpy, как правило, менее удобочитаемы для человека,
#  но имеют формат, позволяющий выполнять необходимые вычисления.
#  Numpy — это модуль Python для выполнения вычислений в таблицах данных.
#  На самом деле Pandas был построен с использованием Numpy в качестве основы.

# TODO: Converting from a Pandas Series to a Numpy Array (Преобразование из серии Pandas в массив Numpy)
#  Мы часто начинаем с наших данных в Pandas DataFrame, но затем хотим преобразовать их в массив numpy.
#  Атрибут values делает это за нас. Давайте преобразуем столбец Fare в пустой массив.
#  Сначала мы вспомним, что мы можем использовать нотацию с одной скобкой,
#  чтобы получить серию pandas в столбце Fare следующим образом.
#      df['Fare']
# TODO: Затем мы используем атрибут values, чтобы получить значения в виде массива numpy.
#      df['Fare'].values
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df['Fare'].values)
# print(df['Fare'].head().values)
# TODO: Вот как выглядит приведенный выше массив:
#      array([ 7.25 , 71.2833,  7.925, 53.1, 8.05, 8.4583, …
# TODO: В результате получается одномерный массив.
#  Вы можете сказать это, так как есть только один набор скобок,
#  и он расширяется только по странице (а не вниз).
#  Атрибут значений серии Pandas предоставляет данные в виде массива numpy.

# TODO: Converting from a Pandas DataFrame to a Numpy Array (Преобразование из Pandas DataFrame в массив Numpy)
#  Если у нас есть DataFrame pandas (вместо Series, как в прошлой части),
#  мы все еще можем использовать атрибут values, но он возвращает двумерный массив numpy.
#  Напомним, что мы можем создать меньший DataFrame pandas со следующим синтаксисом.
#      df[['Pclass', 'Fare', 'Age']]
# TODO: Опять же, мы применяем атрибут values, чтобы получить пустой массив.
#      df[['Pclass', 'Fare', 'Age']].values
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df[['Pclass', 'Fare', 'Age']].values)
# TODO: Вот как выглядит приведенный выше массив:
#      array([[ 3.    ,  7.25  , 22.    ],
#             [ 1.    , 71.2833, 38.    ],
#             [ 3.    ,  7.925 , 26.    ],
#                          ...           ,
#             [ 3.    , 23.45  ,  7.    ],
#             [ 1.    , 30.    , 26.    ],
#             [ 3.    ,  7.75  , 32.    ]])
# TODO: Это двумерный массив numpy. Вы можете сказать, потому что есть два набора скобок,
#  и они расширяются как по странице, так и вниз.
#  Атрибут значений Pandas DataFrame предоставляет данные в виде массива 2d numpy.

# TODO: Numpy Shape Attribute (Атрибут Numpy Форма)
#  Мы используем атрибут numpy форма, чтобы определить размер нашего массива numpy.
#  Размер говорит нам, сколько строк и столбцов в наших данных.
#  Во-первых, давайте создадим массив numpy с Pclass, Fare и Age.
#      arr = df[['Pclass', 'Fare', 'Age']].values
# TODO: Если мы посмотрим на форму, мы получим количество строк и количество столбцов:
#      print(arr.shape) #(887, 3)
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr.shape)
# print(df.shape)
# TODO: Этот результат означает, что у нас есть 887 строк и 3 столбца.
#  Используйте атрибут shape, чтобы найти количество строк и количество столбцов для массива Numpy.
#  Вы также можете использовать атрибут формы в DataFrame pandas (df.shape).

# TODO: Select from a Numpy Array (Выберите из массива Numpy)
#  Предположим, мы создали следующий массив numpy:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# # arr = df[['Pclass', 'Fare', 'Age']].tail(10).values
# print(arr)
# TODO: Мы можем выбрать один элемент из массива numpy следующим образом:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0, 1])
# TODO: Это будет 2-й столбец 1-й строки (помните, что мы начинаем считать с 0 ).
#  Таким образом, это будет тариф 1-го пассажира или 7,25.
#  Мы также можем выбрать одну строку, например, весь ряд первого пассажира:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0])
# # print(arr[0, :])
# TODO: Чтобы выбрать один столбец (в данном случае столбец «Возраст»), мы должны использовать специальный синтаксис:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[:, 2])
# TODO:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0, 1])
# print(arr[0])
# print(arr[:, 2])
# TODO: Синтаксис можно интерпретировать так, что мы берем все строки, но только столбец с индексом 2.
#  Используя другой синтаксис в скобках, мы можем выбрать отдельные значения, всю строку или весь столбец.

# TODO: Masking (Маскировка)
#  Часто требуется выбрать все строки, соответствующие определенному критерию.
#  В этом примере мы выберем все строки детей (пассажиры младше 18 лет).
#  Напоминание о нашем определении массива:
#      arr = df[['Pclass', 'Fare', 'Age']].values
# TODO: Напомним, что мы можем получить столбец Age с помощью следующего синтаксиса:
#      arr[:, 2]
# TODO: Сначала мы создаем то, что мы называем маской.
#  Это массив логических значений (True/False), указывающих, является ли пассажир ребенком или нет.
#      mask = arr[:, 2] < 18
# TODO: Давайте посмотрим на массив маски, чтобы убедиться, что мы его понимаем.
#      array([False, False, False, False, False, False, False, True, False, …
# TODO: Значения False означают взрослого, а значения True — ребенка,
#  поэтому первые 7 пассажиров — взрослые, затем 8-й — ребенок, а 9-й — взрослый.
#  Теперь мы используем нашу маску, чтобы выбрать только нужные нам строки:
#      arr[mask]
# TODO: Давайте посмотрим на этот новый массив.
#      array([[3., 21.075, 2.],
#             [2., 30.0708, 14.],
#             [3., 16.7, 4.],
#             [3., 7.8542, 14.],
# TODO: Если мы вспомним, что третий столбец — это возраст пассажиров,
#  мы увидим, что все строки здесь — для пассажиров-детей.
#  Как правило, нам не нужно определять переменную маски, и мы можем сделать это всего в одной строке:
#      arr[arr[:, 2] < 18]
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# # take first 10 values for simplicity
# arr = df[['Pclass', 'Fare', 'Age']].values
#
# mask = arr[:, 2] < 18
# print(arr[mask])
# print(arr[arr[:, 2] < 18])
# TODO: Маска — это логический массив (значения True/False),
#  который сообщает нам, какие значения из массива нас интересуют.

# TODO: Summing and Counting (Суммирование и подсчет)
#  Допустим, мы хотим знать, сколько среди наших пассажиров детей.
#  У нас все еще есть то же определение массива,
#  и мы можем взять нашу маску или логические значения из предыдущей части.
#      arr = df[['Pclass', 'Fare', 'Age']].values
#      mask = arr[:, 2] < 18
# TODO: Напомним, что значения True интерпретируются как 1, а значения False интерпретируются как 0.
#  Таким образом, мы можем просто суммировать массив, и это эквивалентно подсчету количества истинных значений.
#      print(mask.sum())
# TODO: Опять же, мы можем не определять переменную маски.
#      print((arr[:, 2] < 18).sum())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age', 'Survived', 'Sex']].values
# mask = arr[:, 2] < 18
#
# print(mask.sum())
# print((arr[:, 2] < 18).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1)).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] == 'male')).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] == 'female')).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] != 'male')).sum())
# TODO: Суммирование массива логических значений дает количество значений True.

# TODO: Scatter Plot (Точечная диаграмма)
#  Мы можем использовать библиотеку matplotlib для построения графика наших данных.
#  Нанесение данных на график часто может помочь нам создать интуицию в отношении наших данных.
#  Сначала нам нужно импортировать matplotlib. Это стандартная практика называть его plt.
#      import matplotlib.pyplot as plt
# TODO: Мы используем функцию разброса для построения наших данных.
#  Первым аргументом функции рассеяния является ось x (горизонтальное направление),
#  а вторым аргументом — ось y (вертикальное направление).
#      plt.scatter(df['Age'], df['Fare'])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'])
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Это отображает возраст по оси x и стоимость проезда по оси y.
#  Чтобы упростить интерпретацию, мы можем добавить метки x и y.
#      plt.xlabel('Age')
#      plt.ylabel('Fare')
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Мы также можем использовать наши данные для цветового кодирования нашей диаграммы рассеивания.
#  Это даст каждому из 3 классов другой цвет. Мы добавляем параметр c и присваиваем ему серию Pandas.
#  В этом случае наш ряд Pandas имеет 3 возможных значения (1-й, 2-й и 3-й класс),
#  поэтому мы увидим, что наши точки данных получают один из трех цветов.
#      plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# # plt.scatter(df['Age'], df['Fare'], c=[df['Sex'] == 'male'])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Фиолетовые точки относятся к 1-первому классу,
#  зеленые точки ко 2-второму классу,
#  желтые точки к 3-третьему классу.
#  Точечная диаграмма используется для отображения всех значений ваших данных на графике.
#  Чтобы получить визуальное представление наших данных, мы должны ограничить наши данные двумя функциями.

# TODO: Line (Линия)
#  Теперь, когда мы можем поместить отдельные точки данных на график, давайте посмотрим, как нарисовать линию.
#  Функция plot делает именно это. Далее проводится линия, примерно отделяющая 1-й класс от 2-го и 3-го классов.
#  На глазок проведем линию от (0, 85) до (80, 5). Наш синтаксис ниже имеет список значений x и список значений y:
#      plt.plot([0, 80], [85, 5])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# plt.plot([0, 80], [85, 5])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Вы можете видеть, что желтые (3-й класс) и зеленые (2-й класс) точки в основном находятся ниже линии,
#  а фиолетовые (1-й класс) — в основном выше.
#  Мы сделали это вручную, но в следующем модуле мы научимся делать это алгоритмически.
#  В matplotlib мы используем функцию scatter (рассеяния) для создания графика scatter
#  и функцию plot (график) для построения линии.

# TODO: ЗАДАЧА: Machine Learning - What's in a Column? (Машинное обучение — что в столбце?)
#  Получение столбца из массива numpy.
#  Задача Учитывая CSV-файл и имя столбца, распечатать элементы в данном столбце.
#  Формат ввода:
#  Первая строка: имя файла csv
#  Вторая строка: имя столбца
#  Формат вывода Массив Numpy
#  Sample Input:
#  https://sololearn.com/uploads/files/one.csv
#  a
#  File one.csv contents:
#  a,b
#  1,3
#  2,4
#  Sample Output:
#  [1 2]
#  Пояснение: a — это заголовок для первого столбца со значениями [1 2].
# import pandas as pd
#
# filename = input()
# column_name = input()
#
# df = pd.read_csv(filename)
# print(df[column_name].values)

# TODO: Where does Classification Fit in the World of Machine Learning?
#  (Какое место классификация Fit (занимает) в мире машинного обучения?)
#  Машинное обучение на высоком уровне состоит из контролируемого и неконтролируемого обучения.
#  Контролируемое обучение означает, что у нас будут помеченные исторические данные,
#  которые мы будем использовать для информирования нашей модели.
#  Мы называем ярлык или вещь, которую пытаемся предсказать - target (целью).
#  Таким образом, в контролируемом обучении есть известная цель для исторических данных,
#  а в неконтролируемом обучении нет известной цели.
#  В контролируемом обучении есть классификация и регрессия.
#  Проблемы классификации возникают, когда целью является категориальное значение
#  (часто True или False, но может быть несколько категорий).
#  Проблемы регрессии — это когда целью является числовое значение.
#  Например, прогнозирование цен на жилье — это проблема регрессии.
#  Это контролируется, так как у нас есть исторические данные о продажах домов в прошлом.
#  Это регрессия, потому что цена жилья является числовым значением.
#  Предсказание того, не выполнит ли кто-то дефолт по своему кредиту, является проблемой классификации.
#  Опять же, он находится под наблюдением, поскольку у нас есть исторические данные о дефолте прошлых кредиторов,
#  и это проблема классификации, потому что мы пытаемся предсказать,
#  относится ли кредит к одной из двух категорий (дефолт или нет).
#  Логистическая регрессия, хотя в ее названии есть регрессия,
#  представляет собой алгоритм решения задач классификации, а не проблем регрессии.

# TODO: Classification Terminology (Терминология классификации)
#  Давайте вернемся к нашему набору данных Титаника.
#  Вот снова Pandas DataFrame данные:
# import pandas as pd
#
# pd.options.display.max_columns = 7
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head(5))
# TODO:
#  столбец Survived — это то, что мы пытаемся предсказать. Мы называем это целью.
#  Вы можете видеть, что это список из 1 и 0.
#  1 означает, что пассажир выжил, а 0 означает, что пассажир не выжил.
#  Остальные столбцы — это информация о пассажире, которую мы можем использовать для прогнозирования цели.
#  Мы называем каждый из этих столбцов feature (характеристика).
#  Характеристики — это данные, которые мы используем для прогнозирования.
#  Хотя мы знаем, выжил ли каждый пассажир в наборе данных,
#  мы хотели бы иметь возможность делать прогнозы о дополнительных пассажирах,
#  для которых мы не смогли собрать эти данные.
#  Мы построим модель машинного обучения чтобы помочь нам сделать это.
#  Иногда вы будете слышать характеристики, называемые предикторами.

# TODO: Classification Graphically (Классификация графически)
#  В конечном итоге мы захотим использовать все характеристики, но для простоты давайте начнем только
#  с двух характеристик Fare and Age (плата за проезд и возраст).
#  Использование двух характеристик позволяет нам визуализировать данные на графике.
#  По оси абсцисс (X) отложена стоимость проезда пассажира, а по оси ординат (Y) — его возраст.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([32, 100], [0, 80])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Желтые точки — пассажиры, которые выжили, а фиолетовые точки — пассажиры, которые не выжили.
#  Вы можете видеть, что желтых точек внизу графика больше, чем вверху.
#  Это потому, что у детей было больше шансов выжить, чем у взрослых, что соответствует нашей интуиции.
#  Точно так же справа на графике больше желтых точек, что означает,
#  что люди, которые платили больше, имели больше шансов выжить.
# TODO: Задача линейной модели состоит в том, чтобы найти линию, которая наилучшим образом разделяет два класса так,
#  чтобы желтые точки находились с одной стороны, а фиолетовые — с другой. Вот пример хорошей линии:
#      plt.plot([32, 100], [0, 80])
#  Линия используется для прогнозирования появления новых пассажиров.
#  Если точка данных пассажира находится на правой стороне линии, мы прогнозируем, что он выживет.
#  Если на левой стороне, мы бы предсказали, что они не выжили.
#  Задача построения модели будет заключаться в том, чтобы определить наилучшую возможную линию.

# TODO: Equation for the Line (Уравнение для линии)
#  Линия определяется уравнением в следующем виде:
#      0 = ax + by + c
# TODO: Значения a, b и c являются коэффициентами. Любые три значения будут определять уникальную строку.
#  Давайте рассмотрим конкретный пример строки, где коэффициенты равны a=1, b=-1 и c=-30.
#      0 = (1)x + (-1)y + (-30)
# TODO: Вот три коэффициента: 1, -1, -30.
#  Вспомним, что мы наносили наши данные по оси x на тариф и по оси y на возраст пассажира.
#  Чтобы нарисовать прямую из уравнения, нам нужны две точки, лежащие на прямой.
#  Мы можем видеть, например, что точка (30, 0) лежит прямо на линии (Fare 30, Age 0).
#  Если мы подставим это в уравнение, оно сработает.
#      30 - 0 - 30 = 0
# TODO: Мы также можем видеть, что точка (50, 20) находится на линии (Fare 50, Age 20).
#      50 - 20 - 30 = 0
# TODO: Вот как наша линия выглядит на графике.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([30, 50], [0, 20])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Коэффициенты линии определяют, где находится линия.

# TODO: Making a Prediction Based on the Line (Делаем предсказание на основе линии)
#  Давайте снова посмотрим на ту же строку.
#      0 = (1)x + (-1)y - 30
# TODO: Если мы возьмем данные о пассажирах, мы можем использовать это уравнение,
#  чтобы определить, на какую сторону линии они попадают.
#  Например, предположим, что у нас есть пассажир, тариф которого равен 100, а возраст 20.
#  Подставим эти значения в наше уравнение:
#      (1)100 + (-1)20 - 30 = 100 - 20 - 30 = 50
# TODO: Поскольку это значение положительное, точка находится на правой стороне линии,
#  и мы предполагаем, что пассажир выжил.
#  Теперь предположим, что тариф для пассажира равен 10, а его возраст равен 50 годам.
#  Подставим эти значения в уравнение.
#      (1)10 + (-1)50 - 30 = -70
# TODO: Поскольку это значение отрицательное, точка находится на левой стороне линии,
#  и мы предполагаем, что пассажир не выжил. Мы можем видеть эти две зеленые точки на графике ниже.
#  С какой стороны линии находится точка, зависит, по нашему мнению, выживет этот пассажир или нет.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([32, 100], [0, 80])
# plt.scatter([100, 10], [20, 50], c='g')
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()

# TODO: What Makes a Good Line? (Что делает линию хорошей?)
#  Давайте посмотрим на две разные линии. Сначала у нас есть линия, с которой мы работали до сих пор.
#  Назовем эту линию 1.
#      0 = (1)x + (-1)y - 30
# TODO: Далее у нас есть еще одно уравнение для линии.
#  Назовем эту линию 2.
#      0 = (4)x + (5)y - 400
# TODO: Если мы посмотрим на две линии, то увидим, что в строке 1 справа больше желтых точек, а слева больше фиолетовых.
#  У линии 2 не так много точек справа от нее; большинство фиолетовых и желтых точек слева.
#  Это делает линию 1 предпочтительной, поскольку она лучше разделяет желтые и фиолетовые точки.
#  Нам нужно математически определить эту идею, чтобы мы могли алгоритмически найти лучшую линию.
#  Логистическая регрессия — это способ математического поиска наилучшей линии.

# TODO: Probability of Surviving (Вероятность выживания)
#  Чтобы определить наилучшую возможную линию для разделения наших данных, нам нужен способ оценки линии.
#  Во-первых, давайте посмотрим на одну точку данных. В идеале, если точка данных — это выживший пассажир,
#  он должен находиться с правой стороны очереди и далеко от нее. Если это точка данных для пассажира,
#  который не выжил, она будет далеко от линии слева. Чем дальше он от линии, тем больше мы уверены,
#  что он находится на правильной стороне линии.
#  Для каждой точки данных у нас будет оценка со значением от 0 до 1.
#  Мы можем думать об этом как о вероятности чтобы пассажир выжил.
#  Если значение близко к 0, эта точка будет далеко слева от линии,
#  и это означает, что мы уверены, что пассажир не выжил.
#  Если значение близко к 1, эта точка будет далеко справа от линии, и это означает, что мы уверены, что пассажир выжил.
#  Значение 0,5 означает, что точка падает прямо на линию, и мы не уверены, выживет ли пассажир.
#  Уравнение для расчета этой оценки приведено ниже, хотя интуиция для него гораздо важнее фактического уравнения.
#  Напомним, что уравнение для линии имеет вид:
#      0 = ax+by+c
#  x — это Тариф,
#  y — Возраст,
#  a, b, c — коэффициенты, которыми мы управляем.
#  См. Рис: ProbabilitySurvivingPicture.png
#  Число e — математическая константа, приблизительно равная 2.71828
#  Эта функция называется сигмоидой.
# TODO: Логистическая регрессия дает не просто прогноз (выжил или нет),
#  а вероятность (вероятность 80%, что этот человек выжил).

# TODO: Likelihood (Вероятность)
#  Чтобы рассчитать, насколько хороша наша линия, нам нужно оценить, верны ли наши прогнозы.
#  В идеале, если мы прогнозируем с высокой вероятностью, что пассажир выживет
#  (это означает, что точка данных находится далеко справа от линии), то этот пассажир действительно выживает.
#  Таким образом, мы будем вознаграждены - когда предскажем что-то правильно,
#  и наказаны - если предскажем что-то неправильно.
#  Вот уравнение вероятности. Хотя опять же, интуиция важнее уравнения.
#  См. Рис: LikelihoodPicture_1.png
#  Здесь p — прогнозируемая вероятность выживания из предыдущей части.
#  Вероятность будет иметь значение от 0 до 1.
#  Чем выше значение, тем лучше наша линия.
#  Давайте рассмотрим пару возможностей:
#  • Если прогнозируемая вероятность p равна 0.25, а пассажир не выжил, мы получаем 0.75 балла (хорошо).
#  • Если прогнозируемая вероятность p равна 0.25 и пассажир выжил, мы получаем оценку 0.25 (плохо).
#   Мы умножаем все отдельные оценки для каждой точки данных вместе, чтобы получить оценку для нашей линии.
#   Таким образом, мы можем сравнивать разные линии, чтобы определить лучшую.
#   Скажем для простоты вычислений, что у нас есть 4 точки данных.
#   Мы получаем общий балл, умножая четыре балла вместе:
#  См. Рис: LikelihoodPicture_2.png
#      0.25 * 0.75 * 0.6 * 0.8 = 0.09
#  Значение всегда будет очень маленьким, поскольку это вероятность того, что наша модель все предсказывает идеально.
#  Идеальная модель будет иметь прогнозируемую вероятность 1 для всех положительных случаев
#  и 0 для всех отрицательных случаев.
#  Вероятность — это то, как мы оцениваем и сравниваем возможные варианты наиболее подходящей линии.

# TODO: What is Scikit-learn? (Что такое Scikit-learn?)
#  Теперь, когда мы заложили основы работы логистической регрессии, давайте углубимся в код для построения модели.
#  Для этого мы представим новый модуль Python под названием scikit-learn. Scikit-learn, часто сокращаемый до sklearn,
#  — это наш научный инструментарий. Все основные алгоритмы машинного обучения реализованы в sklearn.
#  Мы увидим, что с помощью всего нескольких строк кода мы можем построить несколько различных мощных моделей.
#  Обратите внимание, что scikit-learn постоянно обновляется.
#  Если у вас на компьютере установлена немного другая версия модуля, все будет работать корректно,
#  но вы можете увидеть немного другие значения, чем на игровой площадке.
#  Scikit-learn — один из лучших документированных модулей Python.
#  Вы можете найти множество примеров кода на scikit-learn.org.
#  https://scikit-learn.org/stable/

# TODO: Prep Data with Pandas (Подготовьте данные с помощью Pandas)
#  Прежде чем мы сможем использовать sklearn для построения модели, нам нужно подготовить данные с помощью Pandas.
#  Вернемся к нашему полному набору данных и рассмотрим команды Pandas. Вот кадр данных Pandas со всеми столбцами:
# import pandas as pd
#
# pd.options.display.max_columns = 7
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head(5))
# TODO: во- первых, нам нужно сделать все наши столбцы числовыми. Вспомните, как создать булев столбец для пола.
#      df['male'] = df['Sex'] == 'male'
# TODO: Теперь давайте возьмем все характеристики и создадим пустой массив с именем X.
#  Сначала мы выберем все интересующие нас столбцы,
#  а затем используем метод значений, чтобы преобразовать его в пустой массив.
#      X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# TODO: Теперь возьмем цель (столбец Survived) и сохраним ее в переменной y.
#      y = df['Survived'].values
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# print(X)
# print(y)
# TODO: Стандартной практикой является вызов нашего двумерного массива характеристик X
#  и одномерного массива целевых значений y.

# TODO: Build a Logistic Regression Model with Sklearn (Создайте модель логистической регрессии с помощью Sklearn)
#  Начнем с импорта модели логистической регрессии:
#      from sklearn.linear_model import LogisticRegression
# TODO: Все модели sklearn построены как классы Python. Сначала мы создаем экземпляр класса.
#      model = LogisticRegression()
# TODO: Теперь мы можем использовать наши данные, которые мы ранее подготовили, для обучения модели.
#  Для построения модели используется метод подгонки.
#  Он принимает два аргумента: X (характеристики в виде массива 2d numpy) и y (цель в виде массива 1d numpy).
#  Для простоты давайте сначала предположим, что мы строим модель логистической регрессии,
#  используя только столбцы «Тариф» и «Возраст».
#  Сначала мы определяем X как матрицу признаков, а y как целевой массив.
#      X = df[['Fare', 'Age']].values
#      y = df['Survived'].values
# TODO: Теперь мы используем метод подгонки для построения модели.
#      model.fit(X, y)
# TODO: Подгонка модели означает использование данных для выбора линии наилучшего соответствия.
#  Мы можем видеть коэффициенты с атрибутами coef_ и intercept_.
#      print(model.coef_, model.intercept_)
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# X = df[['Fare', 'Age']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# print(model.coef_, model.intercept_)
# # [[ 0.01615949 -0.01549065]] [-0.51037152]
#
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# # plt.plot([32, 100], [0, 80])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Эти значения означают, что уравнение выглядит следующим образом:
#      0 = 0.0161594x + -0.01549065y + -0.51037152
# TODO: Вот линия, нарисованная на графике.
#  Вы можете видеть, что он делает достойную (но не отличную) работу по разделению желтых и фиолетовых точек.
#  Мы немного поставили себя в тупик, используя только 2 из наших функций,
#  поэтому в следующих частях мы будем использовать все функции.
#  Может быть трудно запомнить операторы импорта для разных моделей sklearn.
#  Если не можете вспомнить, просто посмотрите документацию scikit-learn.
#  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

# TODO: Make Predictions with the Model (Делайте прогнозы с помощью модели)
#  Мы действительно усложнили нашу модель тем, что использовали только две характеристики из предыдущих частей,
#  поэтому давайте перестроим модель, используя их все.
#      X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
#      y = df['Survived'].values
#      model = LogisticRegression()
#      model.fit(X, y)
# TODO: Теперь мы можем использовать метод predict() для прогнозирования.
#      model.predict(X)
# TODO: Первый пассажир в наборе данных:
#      [3, True, 22.0, 1, 0, 7.25]
# TODO: Это означает, что пассажир относится к классу P 3, является мужчиной, ему 22 года,
#  у него на борту 1 брат/сестра/супруга, 0 родителей/ребенка на борту и он заплатил 7.25 долларов США.
#  Посмотрим, что предсказывает модель для этого пассажира.
#  Обратите внимание, что даже с одной точкой данных метод прогнозирования принимает
#  двумерный массив numpy и возвращает одномерный массив numpy.
#      print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
#      # [0]
# TODO: Результат равен 0, что означает, что модель предсказывает, что этот пассажир не выжил.
#  Давайте посмотрим, что предсказывает модель для первых 5 строк данных, и сравним их с нашим целевым массивом.
#  Мы получаем первые 5 строк данных с помощью X[:5] и первые 5 значений цели с помощью y[:5].
#      print(model.predict(X[:5]))
#      # [0 1 1 1 0]
#      print(y[:5])
#      # [0 1 1 1 0]
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
# print(model.predict(X[:5]))
# print(y[:5])
# TODO: Мы видим, что он получил все 5 правильных!
#  Метод предсказания возвращает массив из 1 и 0, где 1 означает, что модель предсказывает,
#  что пассажир выжил, а 0 означает, что модель предсказывает, что пассажир не выжил.

# TODO: Score the Model (Оценка модели)
#  Мы можем понять, насколько хороша наша модель, подсчитав количество точек данных, которые она правильно предсказывает
#  Это называется показателем точности. Давайте создадим массив с предсказанными значениями y.
#      y_pred = model.predict(X)
# TODO: Теперь мы создаем массив логических значений того, правильно ли наша модель предсказала каждого пассажира.
#      y == y_pred
# TODO: Чтобы получить число из них, которые верны, мы можем использовать метод numpy sum.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print(y.size)
# print(y.shape[0])
# print((y == y_pred).sum())
# TODO: Это означает, что из 887 точек данных модель делает правильный прогноз для 714 из них.
#  Чтобы получить правильный процент, мы делим его на общее количество пассажиров.
#  Мы получаем общее количество пассажиров, используя атрибут shape.
#      y.shape[0]
# TODO: Таким образом, наша оценка точности вычисляется следующим образом.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print((y == y_pred).sum() / y.shape[0])
# TODO: Таким образом, точность модели составляет 80%.
#  Другими словами, модель делает правильный прогноз для 80% точек данных.
#  Это достаточно распространенный расчет, который sklearn уже реализовал за нас.
#  Таким образом, мы можем получить тот же результат, используя метод score.
#  Метод оценки использует модель для предсказания X и подсчитывает, какой процент из них соответствует y.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print(model.score(X, y))
# TODO: При таком альтернативном методе расчета точности мы получаем то же значение, 80%.
#  Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print((y == y_pred).sum())
# print((y == y_pred).sum() / y.shape[0])
# print(model.score(X, y))
# TODO: В следующем модуле мы увидим, что оценка модели — это гораздо больше.

# TODO: Introducing the Breast Cancer Dataset (Представляем набор данных по раку молочной железы)
#  Теперь, когда мы создали инструменты для построения модели логистической регрессии для набора данных классификации,
#  мы представим новый набор данных. В наборе данных о раке молочной железы каждая точка данных имеет
#  измерения на основе изображения массы молочной железы и того, является ли она злокачественной.
#  Цель будет заключаться в том, чтобы использовать эти измерения, чтобы предсказать, является ли образование раковым.
#  Этот набор данных встроен прямо в scikit-learn, поэтому нам не нужно будет читать CSV.
#  Давайте начнем с загрузки набора данных и просмотра данных и их форматирования.
#      from sklearn.datasets import load_breast_cancer
#      cancer_data = load_breast_cancer()
# TODO: Возвращаемый объект (который мы сохранили в переменной Cancer_data) представляет собой объект,
#  похожий на словарь Python. Мы можем увидеть доступные ключи с помощью метода keys.
#      print(cancer_data.keys())
# TODO: Мы начнем с просмотра DESCR, который дает подробное описание набора данных.
#      print(cancer_data['DESCR'])
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
# print(cancer_data.keys())
# print(cancer_data['DESCR'])
# TODO: Мы видим, что есть 30 характеристик, 569 точек данных, и цель является либо злокачественной (раковой),
#  либо доброкачественной (не раковой). Для каждой из точек данных у нас есть измерения массы груди
#  (радиус, текстура, периметр и т. д.). Для каждого из 10 измерений было вычислено несколько значений,
#  поэтому у нас есть среднее значение, стандартная ошибка и наихудшее значение.
#  Это приводит к 10 * 3 или 30 общим характеристикам.
#  В наборе данных о раке молочной железы есть несколько характеристик, которые рассчитываются на основе других столбцов
#  Процесс выяснения того, какие дополнительные характеристики следует рассчитать,
#  называется проектированием характеристик.

# TODO: Loading the Data into Pandas (Загрузка данных в Pandas)
#  Давайте вытащим характеристики и целевые данные из объекта Cancer_data.
#  Во-первых, данные объекта сохраняются с помощью ключа «данные».
#  Когда мы смотрим на него, мы видим, что это пустой массив с 569 строками и 30 столбцами.
#  Это потому, что у нас есть 569 точек данных и 30 характеристик. Ниже приведен массив данных.
#      cancer_data['data']
# TODO: Мы используем shape (форму), чтобы увидеть, что это массив с 569 строками и 30 столбцами.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['data'].shape)
# TODO: Чтобы поместить это в Pandas DataFrame и сделать его более удобочитаемым, нам нужны имена столбцов.
#  Они сохраняются с ключом «feature_names».
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['feature_names'])
# TODO: Теперь мы можем создать Pandas DataFrame со всеми данными наших характеристик.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# # pd.options.display.max_columns = 30
# cancer_data = load_breast_cancer()
#
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# print(df.head())
# TODO: мы видим, что у нас есть 30 столбцов в DataFrame, так как у нас есть 30 характеристик.
#  Вывод усекается, чтобы поместиться на экране.
#  Мы использовали метод head(), поэтому наш результат имеет только 5 точек данных.
#  Нам все еще нужно поместить целевые данные в наш DataFrame, который можно найти с помощью ключа target.
#  Мы видим, что цель представляет собой одномерный массив numpy из 1 и 0.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target'])
# TODO: Если мы посмотрим на форму массива, то увидим, что это одномерный массив с 569 значениями
#  (именно столько у нас было точек данных).
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target'].shape)
# TODO: Чтобы интерпретировать эти 1 и 0, нам нужно знать, являются ли 1 или 0 доброкачественными или злокачественными.
#  Это дается target_names
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target_names'])
# TODO: Это дает массив ['злокачественный' 'доброкачественный'], который говорит нам, что 0 означает злокачественный,
#  а 1 означает доброкачественный. Давайте добавим эти данные в Pandas DataFrame.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# df['target'] = cancer_data['target']
# print(df.head())
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# # pd.options.display.max_columns = 30
# cancer_data = load_breast_cancer()
#
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
# print(df.head())
# TODO: Важно дважды проверить, правильно ли вы интерпретируете логические столбцы.
#  В нашем случае цель 0 означает злокачественную опухоль, а 1 означает доброкачественную.

# TODO: Build a Logistic Regression Model (Создайте модель логистической регрессии)
#  Теперь, когда мы просмотрели наши данные и привели их в удобный формат,
#  мы можем построить нашу матрицу характеристик X и целевой массив y,
#  чтобы мы могли построить модель логистической регрессии.
#      X = df[cancer_data.feature_names].values
#      y = df['target'].values
# TODO: Теперь мы создаем объект логистической регрессии и используем метод подгонки для построения модели.
#      model = LogisticRegression()
#      model.fit(X, y)
# TODO: Когда мы запускаем этот код, мы получаем предупреждение о конвергенции.
#  Это означает, что модели требуется больше времени, чтобы найти оптимальное решение.
#  Один из вариантов — увеличить количество итераций. Вы также можете переключиться на другой решатель,
#  что мы и сделаем. Решатель — это алгоритм, который модель использует для нахождения уравнения линии.
#  Вы можете увидеть возможные решатели в документации по логистической регрессии.
#  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
#                      decision_function(X) - Прогнозировать показатели достоверности для образцов.
#                      densify() - Преобразование матрицы коэффициентов в формат плотного массива.
#                      fit(X, y[, sample_weight]) - Соответствуйте модели в соответствии с данными обучения.
#                      get_params([deep]) - Получить параметры для этого оценщика.
#                      predict(X) - Предсказать метки класса для образцов в X.
#                      predict_log_proba(X) - Предсказать логарифм оценок вероятности.
#                      predict_proba(X) - Оценки вероятности.
#                      score(X, y[, sample_weight]) - Возвращает среднюю точность для заданных тестовых данных и меток.
#                      set_params(**params) - Установите параметры этой оценки.
#                      sparsify() - Преобразование матрицы коэффициентов в разреженный формат.
#      model = LogisticRegression(solver='liblinear')
#      model.fit(X, y)
# TODO: Давайте посмотрим, что предсказывает модель для первой точки данных в нашем наборе данных.
#  Напомним, что метод предсказания принимает двумерный массив, поэтому мы должны поместить точку данных в список.
#      model.predict([X[0]])
# TODO: Таким образом, модель предсказывает, что первая точка данных является злокачественной.
#  Чтобы увидеть, насколько хорошо модель работает со всем набором данных,
#  мы используем метод оценки, чтобы увидеть точность модели.
#      model.score(X, y)
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.linear_model import LogisticRegression
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# model = LogisticRegression(solver='liblinear')
# model.fit(X, y)
# print("prediction for datapoint 0:", model.predict([X[0]]))
# print(model.score(X, y))
# TODO: Мы видим, что модель правильно получает 96% точек данных.
#  С помощью разработанных нами инструментов мы можем построить модель для любого набора классификационных данных.

# TODO: ЗАДАЧА: Machine Learning - Bob the Builder (Машинное обучение — Боб Строитель)
#  Построение модели логистической регрессии.
#  Задача Вам дается матрица признаков и одна точка данных для прогнозирования.
#  Ваша задача будет состоять в том, чтобы построить модель логистической регрессии с матрицей признаков
#  и сделать прогноз (1 или 0) для одной точки данных.
#  Формат ввода:
#  Первая строка: количество точек данных в матрице признаков (n)
#  Следующие n строк: значения строки в матрице признаков, разделенные пробелами
#  Следующая строка: целевые значения, разделенные пробелами
#  Последняя строка: значения (разделенные пробелами) одна точка данных без целевого значения
#  Формат вывода 1 или 0
#  Sample Input:
#  6
#  1 3
#  3 5
#  5 7
#  3 1
#  5 3
#  7 5
#  1 1 1 0 0 0
#  2 4
#  Sample Output:
#  1
#  Пояснение:
#  Мы видим точки, нанесенные на график выше, и линию, разделяющую данные.
#  Точка (2, 4) отмечена на графике, и вы можете видеть,
#  что она находится на положительной стороне линии, поэтому результат равен 1.
# from sklearn.linear_model import LogisticRegression
#
# n = int(input())
# X = []
# for i in range(n):
#     X.append([float(x) for x in input().split()])
# y = [int(x) for x in input().split()]
# datapoint = [float(x) for x in input().split()]
#
# model = LogisticRegression()
# model.fit(X, y)
# print(*model.predict([datapoint]))

# TODO: Accuracy (Точность)
#  В предыдущем модуле мы рассчитали, насколько хорошо наша модель работает с точностью.
#  Точность — это процент верных прогнозов.
#  Если у вас есть 100 точек данных и вы предсказываете 70 из них правильно и 30 неправильно, точность составляет 70%.
#  Точность — очень простая и понятная метрика, однако она не всегда самая лучшая.
#  Например, предположим, что у меня есть модель для прогнозирования мошеннических списаний с кредитной карты.
#  Из 10000 кредитных карт у нас есть 9900 законных списаний и 100 мошеннических списаний.
#  Я мог бы построить модель, которая просто предсказывает, что каждое отдельное обвинение является законным,
#  и она оправдала бы 9900/10000 (99%) прогнозов!
#  Точность является хорошей мерой, если наши классы разделены поровну, но она вводит в заблуждение,
#  если у нас несбалансированные классы.
#  Всегда проявляйте осторожность с точностью.
#  Вам нужно знать распределение классов, чтобы знать, как интерпретировать значение.

# TODO: Confusion Matrix (Матрица путаницы)
#  Как мы заметили в предыдущей части, нас заботит не только то, для скольких точек данных мы предсказываем
#  правильный класс, нас заботит, сколько положительных точек данных мы предсказываем правильно,
#  а также сколько отрицательных точек данных мы предсказываем правильно.
#  Мы можем увидеть все важные значения в так называемой матрице путаницы (или матрице ошибок, или таблице путаницы).
#  Матрица путаницы представляет собой таблицу, показывающую четыре значения:
#  • Точки данных, которые мы предсказывали как положительные, но на самом деле положительные
#  • Точки данных, которые мы предсказывали как положительные, но на самом деле отрицательные
#  • Точки данных, которые мы прогнозировали как отрицательные, но на самом деле положительные
#  • Точки данных, которые мы прогнозировали как отрицательные, но на самом деле отрицательные
#  Первая и четвертая — это точки данных, которые мы предсказали правильно, а вторая и третья — точки данных,
#  которые мы предсказали неправильно.
#  В нашем наборе данных Титаника у нас есть 887 пассажиров, 342 выжили (положительно) и 545 не выжили (отрицательно).
#  Модель, которую мы построили в предыдущем модуле, имеет следующую матрицу путаницы.
#      См. Рис: ConfusionMatrixPicture_1.png
#  Заштрихованные синим квадраты — это количество верных прогнозов.
#  Таким образом, из 342 выживших пассажиров мы предсказали 233 или их правильно (и 109 из них неверно).
#  Из 545 пассажиров, которые не выжили, мы правильно предсказали 480 (и 65 неверно).
#  Мы можем использовать матрицу путаницы для вычисления точности.
#  Напоминаем, что точность — это количество правильно предсказанных точек данных,
#  деленное на общее количество точек данных.
#      (233+480)/(233+65+109+480) = 713/887 = 80.38%
# TODO: Это действительно то же самое значение, которое мы получили в предыдущем модуле.
#  Матрица путаницы полностью описывает, как модель работает с набором данных,
#  хотя ее сложно использовать для сравнения моделей.

# TODO: True Positives, True Negatives, False Positives, False Negatives (TP Истинные позитивы, TN Истинные негативы,
#  FP Ложноположительные, FN Ложноотрицательные)
#  У нас есть имена для каждого квадрата матрицы путаницы.
#  Истинный положительный результат (TP) — это точка данных,
#  которую мы предсказали положительно и в отношении которой мы были правы.
#  Истинный отрицательный результат (TN) — это точка данных, которую мы предсказали отрицательно
#  и в отношении которой мы были правы.
#  Ложноположительный результат (FP) — это точка данных, которую мы предсказали положительно,
#  в отношении которой мы ошиблись.
#  Ложноотрицательный результат (FN) — это точка данных, которую мы предсказали отрицательно,
#  но в отношении которой мы ошиблись.
#  Условия могут быть немного сложными для отслеживания.
#  Способ запомнить, что второе слово — это наш прогноз (положительный или отрицательный),
#  а первое слово — то, был ли этот прогноз правильным (истинным или ложным).
#  Вы часто будете видеть матрицу путаницы, описанную следующим образом:
#      См. Рис: ConfusionMatrixPicture_2.png
#  TODO: Четыре значения матрицы путаницы (TP, TN, FP, FN) используются для вычисления нескольких различных показателей,
#   которые мы будем использовать позже.

# TODO: Precision (достоверность)
#  Двумя широко используемыми показателями для классификации являются precision (достоверность) и recall (полнота).
#  Концептуально Precision относится к проценту положительных результатов,
#  которые относятся к актуальной полноте процента правильно классифицированных положительных результатов.
#  Оба могут быть определены с использованием квадрантов из матрицы путаницы, которая, как мы помним,
#  выглядит следующим образом: (См. Рис: PrecisionPicture_1.png)
#  Precision — это процент правильных положительных прогнозов модели. Мы определяем его следующим образом:
#  (См. Рис: PrecisionPicture_2.png)
#  #  Если мы посмотрим на нашу матрицу путаницы для нашей модели набора данных Титаника,
#  мы сможем рассчитать Precision.
#  (См. Рис: PrecisionPicture_3.png)
#      precision = TP / (TP + FP)
#      precision = 233 / (233 + 65) = 0.7819
# TODO: Precision (достоверность) — это мера того, насколько точна модель с ее положительными предсказаниями

# TODO: Recall (Полнота)
#  Recall — это процент положительных случаев, которые модель предсказывает правильно.
#  Опять же, мы будем использовать матрицу путаницы для вычисления нашего результата.
#      (См. Рис: RecallPicture_1.png)
#  Здесь мы математически определяем Recall: Давайте рассчитаем Recall для нашей модели для набора данных Титаник.
#      (См. Рис: RecallPicture_3.png)
#  recall = TP / (TP + FN)
#  recall = 233 / (233 + 109) = 0.6813
#      (См. Рис: RecallPicture_4.png)
#      (См. Рис: RecallPicture_4.png)
#  Recall — это мера того, сколько положительных случаев может вспомнить модель.

# TODO: Precision & Recall Trade-off (Компромисс достоверности и полнота)
#  Мы часто оказываемся в ситуации выбора между увеличением Recall - (при снижении Precision)
#  или повышением Precision (и снижением Recall). Это будет зависеть от ситуации,
#  которую мы хотим максимизировать. Например, предположим, что мы создаем модель,
#  чтобы предсказать, является ли списание средств с кредитной карты мошенническим.
#  Положительные случаи для нашей модели — это мошеннические обвинения, а отрицательные — законные обвинения.
#  Давайте рассмотрим два сценария:
#  1. Если мы предскажем, что платеж является мошенническим, мы отклоним платеж.
#  2. Если мы обнаружим, что платеж является мошенническим, мы позвоним клиенту, чтобы подтвердить платеж.
#  В случае 1 покупателю доставляет огромное неудобство, когда модель неверно предсказывает мошенничество
#  (ложное срабатывание).
#  В случае 2 ложное срабатывание является незначительным неудобством для клиента.
#  Чем больше ложных срабатываний, тем ниже Precision.
#  Из-за высокой стоимости ложных срабатываний в первом случае было бы целесообразно иметь низкий Recall,
#  чтобы иметь очень высокую Precision.
#  В случае 2 вам понадобится больше баланса между Precision и Recall.
#  Не существует жестких и быстрых правил относительно того, на какие значения Precision и Recall вы должны действовать.
#  Это всегда зависит от набора данных и приложения.

# TODO: F1 Score (Оценка F1)
#  Precision была привлекательной метрикой, потому что это было единственное число.
#  Precision и Recall — это два числа, поэтому не всегда очевидно, как выбрать между двумя моделями,
#  если одна из них имеет более высокую Precision, а другая — более высокую Recall.
#  Оценка F1 представляет собой среднее значение Precision и Recall, поэтому у нас есть единая оценка для нашей модели.
#  Вот математическая формула для оценки F1.
#      (См. Рис: F1ScorePicture.png)
#      F1 = 2 * ((precision * recall) / (precision + recall))
#  Давайте посчитаем оценку F1 для нашей модели для набора данных «Титаник».
#  Мы будем использовать значения Precision и Recall, которые мы рассчитали ранее.
#  Precision = 0.7819
#  Recall = 0.6813
#  Оценка F1 выглядит следующим образом.
#      F1 = 2 * ((0.7819) * (0.6813) / (0.7819 + 0.6813)) = 0.7281
#  Показатель F1 представляет собой среднее гармоническое значений Precision и Recall.

# TODO: Accuracy, Precision, Recall & F1 Score in Sklearn (Точность, достоверность, полнота и Оценка F1 в Sklearn)
#  В Scikit-learn встроена функция для каждой из введенных нами метрик.
#  У нас есть отдельная функция для каждой Accuracy, Precision, Recall и F1 Score.
#  Чтобы использовать их, давайте начнем с вызова нашего кода из предыдущего модуля
#  для построения модели логистической регрессии. Код считывает набор данных Titanic из CSV-файла и
#  помещает его в Pandas DataFrame. Затем мы создаем матрицу признаков X и целевые значения y.
#  Мы создаем модель логистической регрессии и подгоняем ее к нашему набору данных.
#  Наконец, мы создаем переменную y_pred наших прогнозов (y_pred — это прогнозируемые значения.
#  Что бы понять, насколько хороша наша модель, подсчитав количество точек данных,
#  которые она правильно предсказывает, называется показателем точности).
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# model = LogisticRegression()
# model.fit(X, y)
# y_pred = model.predict(X)
# TODO: Теперь мы готовы использовать наши метрические функции. Давайте импортируем их из scikit-learn.
#      from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# TODO: Каждая функция принимает два одномерных массива numpy: истинные значения цели и прогнозируемые значения цели.
#  У нас есть истинные значения цели и предсказанные значения цели.
#  Таким образом, мы можем использовать метрические функции следующим образом.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# model = LogisticRegression()
# model.fit(X, y)
# y_pred = model.predict(X)
#
# print("accuracy:", accuracy_score(y, y_pred))
# print("precision:", precision_score(y, y_pred))
# print("recall:", recall_score(y, y_pred))
# print("f1 score:", f1_score(y, y_pred))
# TODO: Мы видим, что Accuracy составляет 80%, что означает, что 80% предсказаний модели верны.
#  Precision (достоверность) составляет 77%, что, как мы помним,
#  является процентом правильных положительных прогнозов модели.
#  Recall составляет 69%, что является процентом положительных случаев, которые модель предсказала правильно.
#  F1 Score составляет 73%, что является средним значением precision (достоверность) и recall (полнота).
#  В случае одной модели значения метрик мало что нам говорят. Для некоторых задач хорошо значение 60%,
#  а для других хорошо значение 90%, в зависимости от сложности задачи.
#  Мы будем использовать значения метрик для сравнения различных моделей, чтобы выбрать лучшую.

# TODO: Confusion Matrix in Sklearn (Матрица путаницы в Sklearn)
#  В Scikit-learn есть функция матрицы путаницы,
#  которую мы можем использовать для получения четырех значений в матрице путаницы:
#  - TP истинно положительные,
#  - FP ложноположительные,
#  - FN ложноотрицательные,
#  - TN истинно отрицательные
#  Предполагая, что y — это наши истинные целевые значения, а y_pred — это прогнозируемые значения,
#  мы можем использовать функцию путаницы_матрицы следующим образом:
# import pandas as pd
# from sklearn.metrics import confusion_matrix
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# model = LogisticRegression()
# model.fit(X, y)
# y_pred = model.predict(X)
# print(confusion_matrix(y, y_pred))
# TODO: Scikit-learn переворачивает матрицу путаницы, чтобы сначала показать отрицательные значения!
#  Вот как следует обозначить эту матрицу путаницы.
#      (См. Рис: ConfusionMatrixSklearnPicture_1.jpg)
#  Вот как мы обычно рисуем матрицу путаницы.
#      (См. Рис: ConfusionMatrixSklearnPicture_2.jpg)
#  Поскольку отрицательные целевые значения соответствуют 0, а положительные — 1,
#  scikit-learn упорядочил их в таком порядке. Убедитесь, что вы перепроверили правильность интерпретации значений!

# TODO: Overfitting (Переобучение)
#  До сих пор мы построили модель со всеми нашими данными, а затем увидели,
#  насколько хорошо она работает с теми же данными.
#  Это искусственно завышает наши цифры, поскольку наша модель, по сути,
#  должна была увидеть ответы на викторину до того, как мы дали ей викторину.
#  Это может привести к тому, что мы называем переобучение.
#  Переобучение — это когда мы хорошо работаем с данными,
#  которые модель уже видела, но плохо работаем с новыми данными.
#  Мы можем визуально увидеть модель переобучения следующим образом.
#  Линия слишком близко пытается получить каждую точку данных на правильной стороне линии, но ей не хватает сути данных.
#  См. Рис: OverfittingPicture_1.png
#  На графике вы можете видеть, что мы проделали довольно хорошую работу,
#  получив желтые точки вверху и фиолетовые точки внизу, но это не отражает того, что происходит.
#  Единственная точка выброса действительно может сбить с толку расположение линии.
#  Несмотря на то, что модель получит высокую оценку на уже просмотренных данных,
#  вряд ли она будет хорошо работать на новых данных.
#  Чем больше функций у нас есть в нашем наборе данных, тем больше мы будем подвержены переобучению.

# TODO: Training Set and Test Set (Обучающий набор и Тестовый набор)
#  Чтобы дать модели справедливую оценку, мы хотели бы знать, насколько хорошо наши данные будут работать с данными,
#  которые она еще не видела. В действии наша модель будет делать прогнозы на основе данных,
#  на которые мы не знаем ответа, поэтому мы хотели бы оценить, насколько хорошо наша модель работает с новыми данными,
#  а не только с данными, которые она уже видела. Чтобы имитировать прогнозирование новых невидимых данных,
#  мы можем разбить наш набор данных на обучающий набор и тестовый набор.
#  Обучающий набор используется для построения моделей. Тестовый набор используется для оценки моделей.
#  Мы разделяем наши данные перед построением модели, поэтому модель ничего не знает о тестовом наборе,
#  и мы дадим ей справедливую оценку.
#  Если в нашем наборе данных 200 точек данных, разбивка его на обучающий набор
#  и тестовый набор может выглядеть следующим образом.
#      См. Рис: TrainingSetTestSetPicture.png
#  Стандартная разбивка состоит в том, чтобы поместить 70-80% наших данных в обучающий набор и 20-30% в тестовый набор.
#  Использование меньшего количества данных в обучающем наборе означает, что у нашей модели будет не так много
#  данных для обучения, поэтому мы хотим предоставить их как можно больше, но при этом оставить достаточно для оценки.

# TODO: Training and Testing in Sklearn (Обучение и тестирование в Sklearn)
#  Scikit-learn имеет встроенную функцию для разделения данных на обучающий набор и тестовый набор.
#  Предполагая, что у нас есть двумерный массив X - наших функций и одномерный массив y - цели,
#  мы можем использовать функцию train_test_split.
#  Он случайным образом поместит каждую точку данных либо в обучающий набор, либо в тестовый набор.
#  По умолчанию обучающий набор составляет 75 % данных, а тестовый набор оставшиеся 25 % данных.
# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(X, y)
# TODO: Давайте воспользуемся атрибутом shape, чтобы увидеть размеры наших наборов данных.
# print("whole dataset:", X.shape, y.shape)
# print("training set:", X_train.shape, y_train.shape)
# print("test set:", X_test.shape, y_test.shape)
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# print("whole dataset:", X.shape, y.shape)
# print("training set:", X_train.shape, y_train.shape)
# print("test set:", X_test.shape, y_test.shape)
# TODO: Мы видим, что из 887 точек данных в нашем наборе данных 665 из них находятся в нашем обучающем наборе,
#  а 222 — в тестовом наборе. Каждая точка данных из нашего набора данных используется
#  ровно один раз либо в обучающем наборе, либо в тестовом наборе.
#  Обратите внимание, что у нас есть 6 функций в нашем наборе данных, поэтому у нас все еще есть 6 функций
#  как в нашем обучающем наборе, так и в тестовом наборе.
#  Мы можем изменить размер нашего обучающего набора, используя параметр train_size.
#  Например, train_test_split(X, y, train_size=0.6) поместит 60% данных в обучающий набор и 40% в тестовый набор.

# TODO: Building a Scikit-learn Model Using a Training Set
#  (Построение модели Scikit-learn с использованием обучающего набора)
#  Теперь, когда мы знаем, как разделить наши данные на обучающий набор и тестовый набор,
#  нам нужно изменить способ построения и оценки модели.
#  Все построение модели выполняется с помощью обучающего набора, а вся оценка выполняется с помощью тестового набора.
#  В последнем модуле мы построили модель и оценили ее на том же наборе данных.
#  Теперь мы строим модель, используя обучающую выборку.
# model = LogisticRegression()
# model.fit(X_train, y_train)
# TODO: И мы оцениваем модель с помощью набора тестов.
# print(model.score(X_test, y_test))
# TODO: На самом деле все метрики, которые мы рассчитывали в предыдущих частях,
#  должны быть рассчитаны на тестовом наборе.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# from sklearn.model_selection import train_test_split
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# # building the model
# model = LogisticRegression()
# model.fit(X_train, y_train)
#
# # evaluating the model
# print("accuracy:", model.score(X_test, y_test))
# y_pred = model.predict(X_test)
# print("accuracy:", accuracy_score(y_test, y_pred))
# print("precision:", precision_score(y_test, y_pred))
# print("recall:", recall_score(y_test, y_pred))
# print("f1 score:", f1_score(y_test, y_pred))
# TODO: Наши значения accuracy, precision (достоверность), recall и оценки F1 на самом деле очень похожи на значения,
#  когда мы использовали весь набор данных. Это признак того, что наша модель не переобучена!
#  Если вы запустите код, вы заметите, что каждый раз вы получаете разные оценки.
#  Это связано с тем, что разделение обучение/тестирование выполняется случайным образом,
#  и в зависимости от того, какие точки попадут в обучающий набор и тест, оценки будут разными.
#  Когда мы доберемся до урока перекрестной проверки, мы увидим,
#  что у нас есть более точные средства измерения этих оценок.

# TODO: Using a Random State (Использование случайного состояния)
#  Как мы заметили в предыдущей части, когда мы случайным образом разделяем данные на обучающий набор и тестовый набор,
#  мы получаем разные точки данных в каждом наборе каждый раз, когда запускаем код.
#  Это результат случайности, и нам нужно, чтобы он был случайным, чтобы он был эффективным,
#  но иногда это может затруднить тестирование кода.
#  Например, каждый раз, когда мы запускаем следующий код, мы получаем разные результаты.
# from sklearn.model_selection import train_test_split
#
# X = [[1, 1], [2, 2], [3, 3], [4, 4]]
# y = [0, 0, 1, 1]
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
# # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)
# print('X_train', X_train)
# print('X_test', X_test)
# TODO: Чтобы каждый раз получать одно и то же разделение, мы можем использовать атрибут random_state.
#  Мы выбираем произвольное число, чтобы каждый раз использовать только его,
#  когда мы запускаем код, и мы будем получать одно и то же разделение.
# from sklearn.model_selection import train_test_split
#
# X = [[1, 1], [2, 2], [3, 3], [4, 4]]
# y = [0, 0, 1, 1]
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)
# print('X_train', X_train)
# print('X_test', X_test)
# TODO: Случайное состояние также называется семенем.

# TODO: Logistic Regression Threshold (Порог логистической регрессии)
#  Если вы помните, в Уроке 2 мы говорили о компромиссе между precision (достоверностью) и recall.
#  С помощью модели логистической регрессии у нас есть простой способ переключения
#  между упором на точность и упором на припоминание. Модель логистической регрессии не просто возвращает прогноз,
#  но возвращает значение вероятности от 0 до 1. Обычно мы говорим, что если значение >=0.5,
#  мы прогнозируем выжившего пассажира, а если значение <0.5, пассажир не выжил.
#  Однако мы можем выбрать любой порог от 0 до 1. Если мы увеличим порог, у нас будет меньше положительных прогнозов,
#  но наши положительные прогнозы с большей вероятностью окажутся правильными.
#  Это означает, что precision (достоверность) будет выше, а recall ниже.
#  С другой стороны, если мы снизим порог, у нас будет больше положительных прогнозов,
#  поэтому мы с большей вероятностью поймаем все положительные случаи.
#  Это означает, что recall будет выше, а precision (достоверность) ниже.
#  Каждый выбор порога представляет собой другую модель.
#  Кривая ROC (рабочая характеристика приемника) представляет собой график,
#  показывающий все возможные модели и их характеристики.

# TODO: Sensitivity & Specificity (Чувствительность и специфичность)
#  Кривая ROC представляет собой график зависимости чувствительности от специфичности.
#  Эти значения демонстрируют тот же компромисс, что и precision (достоверность) и recall.
#  Давайте вернемся к матрице путаницы, так как мы будем использовать ее для
#  определения чувствительности и специфичности.
#      См. Рис: SensitivitySpecificityPicture_1.png
#  Чувствительность - это еще один термин для recall, который представляет собой TP истинный положительный показатель.
#  Напомним, что он рассчитывается следующим образом:
#      sensitivity = recall = positives predicted correctly / positive cases = TP / (TP + FN)
#      См. Рис: SensitivitySpecificityPicture_2.png
#  Специфичностью является TN истинный отрицательный показатель. Он рассчитывается следующим образом.
#      specificity = negatives predicted correctly / negative cases = TN / (TN + FP)
#      См. Рис: SensitivitySpecificityPicture_3.png
#  Мы провели тестовое разделение нашего набора данных «Титаник» и получили следующую матрицу путаницы.
#      См. Рис: SensitivitySpecificityPicture_4.png
#  У нас есть 96 положительных случаев и 126 отрицательных случаев в нашем тестовом наборе.
#  Рассчитаем чувствительность и специфичность.
#      Sensitivity = 61 / 96 = 0.6354
#      Specificity = 105 / 126 = 0.8333
# TODO: Цель состоит в том, чтобы максимизировать эти два значения, хотя, как правило,
#  увеличение одного приводит к уменьшению другого. Это будет зависеть от ситуации,
#  будем ли мы уделять больше внимания чувствительности или специфичности.
#  В то время как мы обычно смотрим на значения positive и recall,
#  для построения графиков стандартом является использование чувствительности и специфичности.
#  Можно построить кривую precision (достоверность) и recall (полнота), но обычно это не делается.

# TODO: Sensitivity & Specificity in Scikit-learn (Чувствительность и специфичность в Scikit-learn)
#  В Scikit-learn не определены функции для чувствительности и специфичности, но мы можем сделать это сами.
#  Чувствительность аналогична recall, поэтому ее легко определить.
# from sklearn.metrics import recall_score
# sensitivity_score = recall_score
# print(sensitivity_score(y_test, y_pred))
# # 0.6829268292682927
# TODO: Теперь, чтобы определить специфичность, если мы понимаем, что это также recall отрицательного класса,
#  мы можем получить значение из функции sklearn precision_recall_fscore_support.
#  Давайте посмотрим на выходные данные Precision_recall_fscore_support.
# from sklearn.metrics import precision_recall_fscore_support
#
# print(precision_recall_fscore_support(y, y_pred))
# TODO: Второй массив — это recall, поэтому мы можем игнорировать остальные три массива.
#  Есть два значения. Первый — это recall отрицательного класса, а второй — recall положительного класса.
#  Второе значение — это стандартное значение recall или чувствительности, и вы можете видеть,
#  что это значение соответствует тому, что мы получили выше. Первая ценность – специфичность.
#  Итак, давайте напишем функцию для получения именно этого значения.
# def specificity_score(y_true, y_pred):
#     p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
#     return r[0]
#
#
# print(specificity_score(y_test, y_pred))
# # 0.9214285714285714
# TODO: Обратите внимание, что в примере кода мы используем random_state в разделении обучение/тестирование,
#  поэтому каждый раз, когда вы запускаете код, вы получаете одни и те же результаты.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import recall_score, precision_recall_fscore_support
#
# sensitivity_score = recall_score
#
#
# def specificity_score(y_true, y_pred):
#     p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
#     return r[0]
#
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)
#
# model = LogisticRegression()
# model.fit(X_train, y_train)
# y_pred = model.predict(X_test)
#
# print("sensitivity:", sensitivity_score(y_test, y_pred))
# print("specificity:", specificity_score(y_test, y_pred))
# TODO: Чувствительность такая же, как recall (или recall положительного класса),
#  а специфичность - это recall отрицательного класса.

# TODO: Adjusting the Logistic Regression Threshold in Sklearn (Настройка порога логистической регрессии в Sklearn)
#  Когда вы используете метод прогнозирования scikit-learn, вам даются значения прогноза 0 и 1.
#  Однако за кулисами модель логистической регрессии получает значение вероятности от 0 до 1 для каждой точки данных,
#  а затем округляет до 0 или 1.
#  Если мы хотим выбрать другой порог, отличный от 0.5, нам понадобятся эти значения вероятности.
#  Мы можем использовать функцию predict_proba, чтобы получить их.
#      (model.predict_proba(X_test)
# TODO: Результатом является пустой массив с двумя значениями для каждой точки данных (например, [0.78, 0.22]).
#  Вы заметите, что сумма двух значений равна 1. Первое значение — это вероятность того,
#  что точка данных находится в классе 0 (не сохранилась), а второе — это вероятность того,
#  что точка данных находится в классе 1 (сохранилась).
#  Нам нужен только второй столбец этого результата, который мы можем получить с помощью следующего синтаксиса numpy.
#      model.predict_proba(X_test)[:, 1]
# TODO: Теперь мы просто хотим сравнить эти значения вероятности с нашим порогом.
#  Скажем, нам нужен порог 0.75. Мы сравниваем приведенный выше массив с 0.75.
#  Это даст нам массив значений True/False, который будет нашим массивом прогнозируемых целевых значений.
#      y_pred = model.predict_proba(X_test)[:, 1] > 0.75
# TODO: Порог 0.75 означает, что нам нужно быть более уверенными, чтобы сделать положительный прогноз.
#  Это приводит к меньшему количеству положительных прогнозов и большему количеству отрицательных прогнозов.
#  Теперь мы можем использовать любые метрики scikit-learn до использования y_test
#  в качестве наших истинных значений и y_pred в качестве наших прогнозируемых значений.
#      print("precision:", precision_score(y_test, y_pred))
#      print("recall:", recall_score(y_test, y_pred))
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import precision_score, recall_score
# from sklearn.model_selection import train_test_split
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# model = LogisticRegression()
# model.fit(X_train, y_train)
#
# y_pred = model.predict_proba(X_test)[:, 1] > 0.75
#
# print("precision:", precision_score(y_test, y_pred))
# print("recall:", recall_score(y_test, y_pred))
# TODO: Установив пороговое значение 0.5, мы получим исходную модель логистической регрессии.
#  Любое другое пороговое значение дает альтернативную модель.

# TODO: How to Build an ROC Curve (Как построить кривую ROC)
#  Кривая ROC представляет собой график зависимости специфичности от чувствительности.
#  Мы строим модель логистической регрессии, а затем рассчитываем специфичность
#  и чувствительность для каждого возможного порога. Каждая предсказанная вероятность является порогом.
#  Если у нас есть 5 точек данных со следующими прогнозируемыми вероятностями: 0.3, 0.4, 0.6, 0.7, 0.8,
#  мы будем использовать каждое из этих 5 значений в качестве порога.
#  Обратите внимание, что мы на самом деле строим график зависимости чувствительности от (1-специфичности).
#  Нет веской причины делать это таким образом, кроме того, что это стандарт.
#  Давайте начнем с просмотра кода для построения кривой ROC.
#  В Scikit-learn есть функция roc_curve, которую мы можем использовать.
#  Функция берет истинные целевые значения и предсказанные вероятности из нашей модели.
#  Сначала мы используем метод predict_proba для модели, чтобы получить вероятности.
#  Затем мы вызываем функцию roc_curve. Функция roc_curve возвращает массив ложных срабатываний,
#  массив истинных срабатываний и пороговые значения.
#  Частота ложноположительных результатов соответствует 1-специфичности (ось X),
#  а частота истинных положительных результатов — это еще один термин для чувствительности (ось Y).
#  Пороговые значения не будут нужны на графике. Вот код для построения кривой ROC в matplotlib.
#  Обратите внимание, что у нас также есть код для построения диагональной линии.
#  Это может помочь нам визуально увидеть, насколько наша модель далека от модели,
#  которая предсказывает случайным образом. Мы предполагаем, что у нас уже есть набор данных,
#  разделенный на обучающий набор и тестовый набор.
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import roc_curve
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# model = LogisticRegression()
# model.fit(X_train, y_train)
# y_pred_proba = model.predict_proba(X_test)
# fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])
#
# plt.plot(fpr, tpr)
# plt.plot([0, 1], [0, 1], linestyle='--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.0])
# plt.xlabel('1 - specificity')
# plt.ylabel('sensitivity')
# plt.show()
# TODO: Поскольку мы не используем пороговые значения для построения графика,
#  график не сообщает нам, какое пороговое значение даст каждая из возможных моделей.

# TODO: ROC Curve Interpretation (Интерпретация кривой ROC)
#  Кривая ROC показывает производительность не одной модели, а многих моделей.
#  Каждый выбор порога представляет собой другую модель.
#  Давайте посмотрим на нашу кривую ROC с выделенными точками.
#  Каждая точка A, B и C относится к модели с другим порогом.
#  Модель А имеет чувствительность 0.6 и специфичность 0.9 (напомним, что на графике показана 1-специфичность).
#  Модель B имеет чувствительность 0.8 и специфичность 0.7. Модель C имеет чувствительность 0.9 и специфичность 0.5.
#  Как выбрать между этими моделями, будет зависеть от специфики нашей ситуации.
#      См. Рис: ROCCurveInterpretationPicture.png
#  Чем ближе кривая подходит к верхнему левому углу, тем выше производительность.
#  Линия никогда не должна опускаться ниже диагональной линии,
#  так как это будет означать, что она работает хуже, чем случайная модель.

# TODO: Picking a Model from the ROC Curve (Выбор модели из кривой ROC)
#  Когда мы будем готовы завершить нашу модель, мы должны выбрать один порог,
#  который мы будем использовать для наших прогнозов.
#  Кривая ROC помогает нам выбрать идеальный порог для нашей проблемы.
#  Давайте снова посмотрим на нашу кривую ROC с выделенными тремя точками:
#  если мы находимся в ситуации, когда более важно, чтобы все наши положительные прогнозы были правильными,
#  чем чтобы мы уловили все положительные случаи (это означает, что мы правильно предсказываем
#  большинство отрицательных случаев), мы должны выбрать модель с более высокой специфичностью (модель A).
#  Если мы находимся в ситуации, когда важно выявить как можно больше положительных случаев,
#  мы должны выбрать модель с более высокой чувствительностью (модель C).
#  Если нам нужен баланс между чувствительностью и специфичностью, мы должны выбрать модель B.
#      См. Рис: ROCCurveInterpretationPicture.png
#  Уследить за всеми этими терминами может быть сложно. Даже экспертам приходится просматривать их снова,
#  чтобы убедиться, что они правильно интерпретируют значения.

# TODO: Area Under the Curve (Площадь под кривой)
#  Иногда мы будем использовать кривую ROC для сравнения двух разных моделей. Вот сравнение кривых ROC двух моделей.
#      См. Рис: AreaUnderCurvePicture.png
#  Вы можете видеть, что синяя кривая превосходит оранжевую, поскольку синяя линия почти всегда выше оранжевой.
#  Чтобы получить эмпирическую меру этого, мы вычисляем площадь под кривой,
#  также называемую AUC. Это площадь под ROC-кривой. Это значение от 0 до 1, чем выше, тем лучше.
#  Поскольку ROC представляет собой график всех различных моделей логистической регрессии
#  с разными пороговыми значениями, AUC не измеряет производительность одной модели.
#  Это дает общее представление о том, насколько хорошо работает модель логистической регрессии.
#  Чтобы получить единую модель, вам все равно нужно найти оптимальный порог для вашей задачи.
#  Давайте воспользуемся scikit-learn, чтобы рассчитать площадь под кривой.
#  Мы можем использовать функцию roc_auc_score.
#      (roc_auc_score(y_test, y_pred_proba[:,1])
# TODO: Вот значения для двух строк:
#      Blue AUC: 0.8379
#      Orange AUC: 0.7385
# TODO: Эмпирически видно, что синий лучше. Мы можем использовать функцию roc_auc_score
#  для вычисления оценки AUC модели логистической регрессии в наборе данных Titanic.
#  Мы строим две модели логистической регрессии: model1 с 6 характеристиками
#  и model2 только с Pclass и мужскими характеристиками.
#  Мы видим, что показатель AUC модели 1 выше. Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import roc_auc_score
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# model1 = LogisticRegression()
# model1.fit(X_train, y_train)
# y_pred_proba1 = model1.predict_proba(X_test)
# print("model 1 AUC score:", roc_auc_score(y_test, y_pred_proba1[:, 1]))
#
# model2 = LogisticRegression()
# model2.fit(X_train[:, 0:2], y_train)
# y_pred_proba2 = model2.predict_proba(X_test[:, 0:2])
# print("model 2 AUC score:", roc_auc_score(y_test, y_pred_proba2[:, 1]))
# TODO: Важно отметить, что эта метрика говорит нам, насколько хорошо в целом модель
#  логистической регрессии работает с нашими данными. Поскольку кривая ROC показывает
#  производительность нескольких моделей, AUC не измеряет производительность одной модели.

# TODO: Concerns with Training and Test Set (Проблемы с обучающим и тестовым набором)
#  Мы проводим оценку, потому что хотим получить точную оценку того, насколько хорошо работает модель.
#  Если наш набор данных небольшой, наш набор тестов будет маленьким.
#  Таким образом, это может быть нехороший случайный набор точек данных,
#  и по случайному стечению обстоятельств в нашем оценочном наборе могут оказаться простые или сложные точки данных.
#  Поскольку наша цель состоит в том, чтобы получить наилучшее возможное измерение наших показателей
#  (accuracy (точность), precision (достоверность), recall (полнота) и F1 score (оценка F1),
#  мы можем сделать немного лучше, чем просто один обучающий и тестовый набор.
#  Напомним, что наше разделение обучающей и тестовой выборки выглядит следующим образом.
#  Как мы видим, все значения в обучающем наборе никогда не используются для оценки.
#  Было бы несправедливо строить модель с обучающим набором, а затем оценивать с помощью обучающего набора,
#  но мы не получаем максимально полную картину производительности модели.
#      См. Рис: ConcernsTrainingTestSetPicture_1.png
# TODO: Чтобы убедиться в этом эмпирически, давайте попробуем запустить код из урока 3,
#  который разделяет обучение и тестирование. Мы повторим это несколько раз и посмотрим на результаты.
#  Каждая строка является результатом разного случайного разделения обучение/тестирование.
#      См. Рис: ConcernsTrainingTestSetPicture_2.png
#  Вы можете видеть, что каждый раз, когда мы запускаем его, мы получаем разные значения показателей.
#  Точность колеблется от 0.79 до 0.84, достоверность от 0.75 до 0.81 и полнота от 0.63 до 0.75.
#  Это широкие диапазоны, которые зависят только от того, насколько нам повезло или не повезло,
#  какие точки данных оказались в тестовом наборе.
#  Вот код, если вы хотите попробовать запустить себя и увидеть различные значения показателей.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# from sklearn.model_selection import train_test_split
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# # building the model (построение модели & Training set (Обучающий набор))
# model = LogisticRegression()
# model.fit(X_train, y_train)
#
# # evaluating the model (оценка модели & Test set (Тестовый набор))
# y_pred = model.predict(X_test)
# # print(" accuracy: {0:.5f}".format(accuracy_score(y_test, y_pred)))
# # print("precision: {0:.5f}".format(precision_score(y_test, y_pred)))
# # print("   recall: {0:.5f}".format(recall_score(y_test, y_pred)))
# # print(" f1 score: {0:.5f}".format(f1_score(y_test, y_pred)))
# print(f' accuracy: {(accuracy_score(y_test, y_pred)).round(5)}')
# print(f'precision: {(precision_score(y_test, y_pred)).round(5)}')
# print(f'   recall: {(recall_score(y_test, y_pred)).round(5)}')
# print(f' f1 score: {(f1_score(y_test, y_pred)).round(5)}')
# TODO: Вместо того, чтобы делать одно разделение обучение/тестирование,
#  мы разделим наши данные на обучающий набор и тестовый набор несколько раз.

# TODO: Multiple Training and Test Sets (Несколько обучающих и тестовых наборов)
#  В предыдущей части мы узнали, что в зависимости от нашего набора тестов
#  мы можем получить разные значения метрик оценки.
#  Мы хотим получить представление о том, насколько хорошо работает наша модель в целом,
#  а не только о том, насколько хорошо она работает на одном конкретном наборе тестов.
#  Вместо того, чтобы просто взять часть данных в качестве тестового набора,
#  давайте разобьем наш набор данных на 5 частей. Предположим, у нас есть 200 точек данных в нашем наборе данных.
#  Каждый из этих 5 фрагментов будет служить тестовым набором. Когда фрагмент 1 является тестовым набором,
#  мы используем оставшиеся 4 фрагмента в качестве обучающего набора.
#  Таким образом, у нас есть 5 обучающих и тестовых наборов следующим образом.
#  Каждый из 5 раз у нас есть тестовый набор 20% (40 точек данных) и обучающий набор 80% (160 точек данных).
#      См. Рис: MultipleTrainingTestSetsPicture_1.png
#      См. Рис: MultipleTrainingTestSetsPicture_2.png
#  Каждая точка данных находится ровно в 1 тестовом наборе.

# TODO: Building and Evaluating with Multiple Training and Test Sets
#  (Создание и оценка с помощью нескольких обучающих и тестовых наборов)
#  В предыдущей части мы увидели, как мы можем сделать 5 тестовых наборов,
#  каждый из которых имеет свой тренировочный набор.
#  Теперь для каждого обучающего набора мы строим модель и оцениваем ее с помощью связанного тестового набора.
#  Таким образом, мы строим 5 моделей и вычисляем 5 баллов.
#  Допустим, мы пытаемся рассчитать показатель Accuracy (точности) для нашей модели.
#  Мы сообщаем точность как среднее значение 5 значений:
#      См. Рис: BuildingEvaluatingMultipleTrainingTestSetsPicture_1.jpg
#  (0.83+0.79+0.78+0.80+0.75)/5 = 0.79
#  Если бы мы только что выполнили один обучающий и тестовый набор и случайно получили первый,
#  мы бы сообщили о точности 0.83. Если бы мы получили последний случайным образом, мы бы сообщили о точности 0.75.
#  Усреднение всех этих возможных значений помогает устранить влияние того, в какой набор тестов попадает точка данных.
#  Вы увидите такие разные значения только при наличии небольшого набора данных.
#  С большими наборами данных мы часто просто делаем обучающий и тестовый набор для простоты.
#  Этот процесс создания нескольких обучающих и тестовых наборов называется k-кратной перекрестной проверкой.
#  k — это количество фрагментов, на которые мы разделили наш набор данных.
#  Стандартное число — 5, как мы сделали в нашем примере выше.
#  Наша цель при перекрестной проверке — получить точные измерения для наших метрик
#  (accuracy (точность), precision(достоверность, воспроизводимость), recall (полнота)).
#  Мы строим дополнительные модели, чтобы быть уверенными в цифрах, которые мы рассчитываем и сообщаем.

# TODO: Final Model Choice in k-fold Cross Validation (Окончательный выбор модели в k-кратной перекрестной проверке)
#  Теперь мы построили 5 моделей вместо одной. Как мы выбираем единую модель для использования?
#  Эти 5 моделей были созданы только для целей оценки, чтобы мы могли сообщать о значениях метрик.
#  На самом деле нам не нужны эти модели, и мы хотим построить наилучшую возможную модель.
#  Наилучшей возможной моделью будет модель, которая использует все данные.
#  Поэтому мы отслеживаем наши расчетные значения для наших показателей оценки,
#  а затем строим модель, используя все данные. Это может показаться невероятно расточительным,
#  но компьютеры обладают большой вычислительной мощностью, поэтому стоит использовать немного больше,
#  чтобы убедиться, что мы сообщаем правильные значения для наших показателей оценки.
#  Мы будем использовать эти значения для принятия решений, поэтому их правильный расчет очень важен.
#  Вычислительная мощность для построения модели может быть проблемой, когда набор данных большой.
#  В этих случаях мы просто делаем тестовый сплит.

# TODO: KFold Class (Класс KFold)
#  Scikit-learn уже реализовал код для разбиения набора данных на k фрагментов и создания k обучающих и тестовых наборов
#  Для простоты давайте возьмем набор данных всего с 6 точками данных и 2 характеристиками
#  и трехкратной перекрестной проверкой набора данных.
#  Мы возьмем первые 6 строк из набора данных Titanic и будем использовать только столбцы Age и Fare.
# X = df[['Age', 'Fare']].values[:6]
# y = df['Survived'].values[:6]
# TODO: Начнем с создания экземпляра объекта класса KFold. Он принимает два параметра: n_splits
#  (это k, количество фрагментов для создания) и shuffle (независимо от того, рандомизировать порядок данных или нет).
#  Как правило, рекомендуется перемешивать данные, так как вы часто получаете набор данных в отсортированном порядке.
# kf = KFold(n_splits=3, shuffle=True)
# TODO: Класс KFold имеет метод разделения, который создает 3 разделения для наших данных.
#  Давайте посмотрим на вывод метода разделения.
#  Метод split возвращает генератор, поэтому мы используем функцию списка, чтобы превратить его в список.
# list(kf.split(X))
# TODO: Результат:
# from sklearn.model_selection import KFold
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# X = df[['Age', 'Fare']].values[:6]
# y = df['Survived'].values[:6]
#
# kf = KFold(n_splits=3, shuffle=True)
# print(list(kf.split(X)))
# TODO: Как мы видим, у нас есть 3 обучающих и тестовых набора, как и ожидалось.
#  Первый обучающий набор состоит из точек данных 0, 2, 3, 5, а тестовый набор состоит из точек данных 1, 4.
#  Разделение выполняется случайным образом, поэтому ожидайте увидеть разные точки данных
#  в наборах каждый раз, когда вы запускаете код.

# TODO: Creating Training and Test Sets with the Folds (Создание обучающих и тестовых наборов со сгибами)
#  Мы использовали класс KFold и метод разделения, чтобы получить индексы, которые находятся в каждом из разделений.
#  Теперь давайте используем этот результат, чтобы получить наше первое (из 3) разделение обучение/тест.
#  Сначала вытащим первый сплит.
# splits = list(kf.split(X))
# first_split = splits[0]
# print(first_split)
# # (array([0, 2, 3, 5]), array([1, 4]))
# TODO: Первый массив — это индексы обучающего набора, а второй — индексы тестового набора.
#  Давайте создадим эти переменные.
# train_indices, test_indices = first_split
# print("training set indices:", train_indices)
# print("test set indices:", test_indices)
# # training set indices: [0, 2, 3, 5]
# # test set indices: [1, 4]
# TODO: Теперь мы можем создать X_train, y_train, X_test и y_test на основе этих индексов.
# X_train = X[train_indices]
# X_test = X[test_indices]
# y_train = y[train_indices]
# y_test = y[test_indices]
# TODO: Если мы распечатаем каждую из них, мы увидим,
#  что у нас есть четыре точки данных в X_train и их целевые значения в y_train.
#  Остальные 2 точки данных находятся в X_test, а их целевые значения — в y_test.
# print("X_train")
# print(X_train)
# print("y_train", y_train)
# print("X_test")
# print(X_test)
# print("y_test", y_test)
# TODO: Запустите этот код, чтобы увидеть результаты:
# from sklearn.model_selection import KFold
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# X = df[['Age', 'Fare']].values[:6]
# y = df['Survived'].values[:6]
#
# kf = KFold(n_splits=3, shuffle=True)
#
# splits = list(kf.split(X))
# print(splits)
# first_split = splits[0]
# train_indices, test_indices = first_split
# print("training set indices:", train_indices)
# print("test set indices:", test_indices)
#
# X_train = X[train_indices]
# X_test = X[test_indices]
# y_train = y[train_indices]
# y_test = y[test_indices]
# print("X_train\n", X_train)
# print("y_train\n", y_train)
# print("X_test\n", X_test)
# print("y_test\n", y_test)
# TODO: На данный момент у нас есть обучающие и тестовые наборы в том же формате,
#  что и при использовании функции train_test_split.

# TODO: Build a Model (Построить модель)
#  Теперь мы можем использовать обучающие и тестовые наборы для построения модели и прогнозирования, как и раньше.
#  Вернемся к использованию всего набора данных (поскольку 4 точек данных недостаточно для построения приличной модели).
#  Вот весь код для построения и оценки модели при первом сгибе 5-кратной перекрестной проверки.
#  Обратите внимание, что код для подбора и оценки модели точно такой же,
#  как и при использовании функции train_test_split.
#  Попробуй это сейчас:
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# kf = KFold(n_splits=5, shuffle=True)
#
# splits = list(kf.split(X))
# train_indices, test_indices = splits[0]
# X_train = X[train_indices]
# X_test = X[test_indices]
# y_train = y[train_indices]
# y_test = y[test_indices]
#
# model = LogisticRegression()
# model.fit(X_train, y_train)
# print(model.score(X_test, y_test))
# TODO: До сих пор мы, по сути, разделяли один обучение/тест.
#  Чтобы выполнить k-кратную перекрестную проверку,
#  нам нужно использовать каждое из остальных 4 разделений для построения модели и оценки модели.

# TODO: Loop Over All the Folds (Цикл по всем сгибам)
#  Мы делали сгибы по одному, но на самом деле мы хотим перебрать все сгибы, чтобы получить все значения.
#  Мы поместим код из предыдущей части внутрь нашего цикла for.
# scores = []
# kf = KFold(n_splits=5, shuffle=True)
# for train_index, test_index in kf.split(X):
#     X_train, X_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]
#     model = LogisticRegression()
#     model.fit(X_train, y_train)
#     scores.append(model.score(X_test, y_test))
# print(scores)
# [0.75847, 0.83146, 0.85876, 0.76271, 0.74011]
# TODO: Поскольку у нас 5 кратностей, мы получаем 5 значений точности.
#  Напомним, чтобы получить единственное конечное значение, нам нужно взять среднее значение этих значений.
# print(np.mean(scores))
# 0.79029
# TODO: Теперь, когда мы рассчитали точность, нам больше не нужны 5 разных моделей, которые мы построили.
#  Для будущего использования нам нужна только одна модель.
#  Чтобы получить единственную наилучшую модель, мы строим модель на всем наборе данных.
#  Если нас спросят о достоверности этой модели, мы используем достоверность,
#  рассчитанную перекрестной проверкой (0.79029),
#  хотя на самом деле мы не проверяли эту конкретную модель с помощью тестового набора.
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# import pandas as pd
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# scores = []
# kf = KFold(n_splits=5, shuffle=True)
# for train_index, test_index in kf.split(X):
#     X_train, X_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]
#     model = LogisticRegression()
#     model.fit(X_train, y_train)
#     scores.append(model.score(X_test, y_test))
# print(scores)
# print(np.mean(scores))
# final_model = LogisticRegression()
# final_model.fit(X, y)
# TODO: Ожидайте получать немного разные значения каждый раз, когда вы запускаете код.
#  Класс KFold каждый раз случайным образом разбивает данные, поэтому другое разбиение приведет к разным оценкам,
#  хотя вы должны ожидать, что среднее значение 5 оценок будет примерно одинаковым.

# TODO: Comparing Different Models (Сравнение различных моделей
#  До сих пор мы использовали наши методы оценки, чтобы получить оценки для одной модели.
#  Эти методы станут невероятно полезными, когда мы представим больше моделей и захотим определить,
#  какая из них лучше всего подходит для конкретной задачи.
#  Давайте используем наши методы для сравнения трех моделей:
#  • Модель логистической регрессии, использующая все характеристики в нашем наборе данных
#  • Модель логистической регрессии, использующая только столбцы Pclass, Age и Sex
#  • Модель логистической регрессии, использующая только столбцы Fare и Age.
#  Не ожидал бы, что вторая или третья модель будут работать лучше, поскольку в них меньше информации,
#  но мы можем определить, что использование только этих двух или трех столбцов дает производительность,
#  сравнимую с использованием всех столбцов.
#  Методы оценки необходимы для выбора между несколькими вариантами модели.

# TODO: Building the Models with Scikit-learn (Построение моделей с помощью Scikit-learn)
#  Давайте напишем код для построения двух моделей в scikit-learn.
#  Затем мы будем использовать k-кратную перекрестную проверку для расчета точности,
#  достоверности, полноты и оценки F1 для двух моделей, чтобы мы могли их сравнить.
#  Во-первых, мы импортируем необходимые модули и подготавливаем данные, как делали это раньше.
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# import pandas as pd
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# TODO: Теперь мы можем построить объект KFold. Мы будем использовать 5 сплитов как стандарт.
#  Обратите внимание, что мы хотим создать один объект KFold, который будут использовать все модели.
#  Было бы несправедливо, если бы разные модели получали разное разделение данных.
# kf = KFold(n_splits=5, shuffle=True)
# TODO: Теперь мы создадим три разные матрицы признаков X1, X2 и X3. У всех будет одна и та же цель y.
# X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# X2 = df[['Pclass', 'male', 'Age']].values
# X3 = df[['Fare', 'Age']].values
# y = df['Survived'].values
# TODO: Поскольку мы будем делать это несколько раз, давайте напишем функцию для оценки модели.
#  Эта функция использует объект KFold для вычисления точности, достоверности, полноты и оценки F1
#  для модели логистической регрессии с заданной матрицей признаков X и целевым массивом y.
#  Затем мы вызываем нашу функцию три раза для каждой из наших трех матриц признаков и видим результаты.
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# import pandas as pd
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
#
# kf = KFold(n_splits=5, shuffle=True)
#
# X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# X2 = df[['Pclass', 'male', 'Age']].values
# X3 = df[['Fare', 'Age']].values
# y = df['Survived'].values
#
#
# def score_model(X, y, kf):
#     accuracy_scores = []
#     precision_scores = []
#     recall_scores = []
#     f1_scores = []
#     for train_index, test_index in kf.split(X):
#         X_train, X_test = X[train_index], X[test_index]
#         y_train, y_test = y[train_index], y[test_index]
#         model = LogisticRegression()
#         model.fit(X_train, y_train)
#         y_pred = model.predict(X_test)
#         accuracy_scores.append(accuracy_score(y_test, y_pred))
#         precision_scores.append(precision_score(y_test, y_pred))
#         recall_scores.append(recall_score(y_test, y_pred))
#         f1_scores.append(f1_score(y_test, y_pred))
#     print("accuracy:", np.mean(accuracy_scores))
#     print("precision:", np.mean(precision_scores))
#     print("recall:", np.mean(recall_scores))
#     print("f1 score:", np.mean(f1_scores))
#
#
# print("Logistic Regression with all features")
# score_model(X1, y, kf)
# print()
# print("Logistic Regression with Pclass, Sex & Age features")
# score_model(X2, y, kf)
# print()
# print("Logistic Regression with Fare & Age features")
# score_model(X3, y, kf)
# TODO: Мы интерпретируем эти результаты в следующей части.
#  Ожидайте получать немного разные результаты каждый раз, когда вы запускаете код.
#  Разделения в k-кратном порядке выбираются случайным образом, поэтому будут небольшие вариации в зависимости от того,
#  в каком разделении окажется каждая точка данных.

# TODO: Choosing a Best Model (Выбор лучшей модели)
#  Посмотрим на результаты предыдущей части.
#      Logistic Regression with all features
#      accuracy: 0.7959055418015616
#      precision: 0.764272127669388
#      recall: 0.6783206767486641
#      f1 score: 0.7163036778464393
#  .
#      Logistic Regression with Pclass, Sex & Age features
#      accuracy: 0.7981908207960389
#      precision: 0.7715749823848419
#      recall: 0.6830371999703425
#      f1 score: 0.7232930032930033
#  .
#      Logistic Regression with Fare & Age features
#      accuracy: 0.6538944962864216
#      precision: 0.6519918328980114
#      recall: 0.23722965720416847
#      f1 score: 0.34438594236494796
# TODO: Если сравнивать первые две модели, то у них почти одинаковые баллы.
#  Третья модель имеет более низкие оценки по всем четырем показателям.
#  Таким образом, первые два варианта намного лучше, чем третий.
#  Это соответствует интуиции, поскольку третья модель не имеет доступа к полу пассажира.
#  Мы ожидаем, что у женщин больше шансов выжить, поэтому пол будет очень ценным предиктором.
#  Поскольку первые две модели дают эквивалентные результаты, имеет смысл выбрать более простую модель,
#  которая использует функции Pclass, Sex & Age.
#  Теперь, когда мы выбрали лучшую модель, мы строим единую окончательную модель, используя все данные.
# model = LogisticRegression()
# model.fit(X1, y)
# TODO: Теперь мы можем сделать прогноз с помощью нашей модели.
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# import pandas as pd
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
#
# kf = KFold(n_splits=5, shuffle=True)
#
# X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# X2 = df[['Pclass', 'male', 'Age']].values
# X3 = df[['Fare', 'Age']].values
# y = df['Survived'].values
#
#
# def score_model(X, y, kf):
#     accuracy_scores = []
#     precision_scores = []
#     recall_scores = []
#     f1_scores = []
#     for train_index, test_index in kf.split(X):
#         X_train, X_test = X[train_index], X[test_index]
#         y_train, y_test = y[train_index], y[test_index]
#         model = LogisticRegression()
#         model.fit(X_train, y_train)
#         y_pred = model.predict(X_test)
#         accuracy_scores.append(accuracy_score(y_test, y_pred))
#         precision_scores.append(precision_score(y_test, y_pred))
#         recall_scores.append(recall_score(y_test, y_pred))
#         f1_scores.append(f1_score(y_test, y_pred))
#     print("accuracy:", np.mean(accuracy_scores))
#     print("precision:", np.mean(precision_scores))
#     print("recall:", np.mean(recall_scores))
#     print("f1 score:", np.mean(f1_scores))
#
#
# print("Logistic Regression with all features")
# score_model(X1, y, kf)
# print()
# print("Logistic Regression with Pclass, Sex & Age features")
# score_model(X2, y, kf)
# print()
# print("Logistic Regression with Fare & Age features")
# score_model(X3, y, kf)
#
# model = LogisticRegression()
# model.fit(X1, y)
# print(model.predict([[3, False, 25, 0, 1, 2]]))
#
# model = LogisticRegression()
# model.fit(X2, y)
# print(model.predict([[3, False, 25]]))
#
# model = LogisticRegression()
# model.fit(X3, y)
# print(model.predict([[3, 25]]))  # характеристика: sex (пол) - играет важную роль в прогнозировании модели!!!

# TODO: Мы пробовали только три различных комбинации характеристик.
#  Возможно, сработает и другая комбинация.

# TODO: Machine Learning - Welcome to the Matrix (Машинное обучение — добро пожаловать в матрицу)
#  Расчет показателей оценки с использованием матрицы путаницы.
#  Задание: Вам будут даны значения матрицы путаницы
#  (FP истинно положительные, FP ложноположительные, FN ложноотрицательные и TN истинно отрицательные).
#  Ваша задача состоит в том,
#  чтобы вычислить: Accuracy (точность), Precision (достоверность), Recall (полнота), F1 score (оценка f1),
#  а также распечатать значения, округленные до 4 знаков после запятой.
#  Чтобы округлить, вы можете использовать round(x, 4).
#  Формат ввода:  Значения tp, fp, fn, tn в указанном порядке, разделенные пробелами
#  Формат вывода: Каждое значение в отдельной строке, округленное до 4 знаков после запятой,
#  в следующем порядке: Accuracy (точность), Precision (достоверность), Recall (полнота), F1 score (оценка f1)
#  Sample Input:
#  233 65 109 480
#  Sample Output:
#  0.8038
#  0.7819
#  0.6813
#  0.7281
#  Объяснение:
#  Accuracy is (tp + tn) / total = (233+480)/(233+65+109+480)=0.8038
#  Precision is tp / (tp + fp) = 233/(233+65) = 0.7819
#  Recall is tp / (tp + fn) = 233/(233+109) = 0.6813
#  F1 score is 2 * precision * recall / (precision + recall) = 2*0.7819*0.6813/(0.7819+0.6813) = 0.7281
# tp, fp, fn, tn = [int(x) for x in input().split()]
#
# accuracy = (tp + tn) / (tp + fp + fn + tn)
# precision = tp / (tp + fp)
# recall = tp / (tp + fn)
# f1_score = 2 * precision * recall / (precision + recall)
#
# print(accuracy.__round__(4))
# print(precision.__round__(4))
# print(recall.__round__(4))
# print(f1_score.__round__(4))

# TODO: A Nonparametric Machine Learning Algorithm (Непараметрический алгоритм машинного обучения)
#  До сих пор мы имели дело с логистической регрессией.
#  В логистической регрессии мы смотрим на данные графически и рисуем линию, чтобы разделить данные.
#  Модель определяется коэффициентами, определяющими линию. Эти коэффициенты называются параметрами.
#  Поскольку модель определяется этими параметрами,
#  логистическая регрессия является параметрическим алгоритмом машинного обучения.
#  В этом модуле мы представим деревья решений,
#  которые являются примером непараметрического алгоритма машинного обучения.
#  Деревья решений не будут определяться списком параметров, как мы увидим в следующих уроках.
#  Каждый алгоритм машинного обучения является либо параметрическим, либо непараметрическим.

# TODO: Tree Terminology (Древовидная терминология)
#  Причина, по которой многие люди любят деревья решений, заключается в том, что их очень легко интерпретировать.
#  По сути, это блок-схема вопросов, на которые вы отвечаете о точке данных, пока не получите прогноз.
#  Вот пример дерева решений для набора данных Titanic. В следующем уроке мы увидим, как строится это дерево.
#  Каждый из прямоугольников называется узлом. Узлы, которые имеют характеристики разделения,
#  называются внутренними узлами. Самый первый внутренний узел вверху называется корневым узлом.
#  Конечные узлы, в которых мы делаем прогнозы выжившего/не выжившего, называются листовыми узлами.
#  У всех внутренних узлов есть два узла под ними, которые мы называем дочерними узлами.
#      См. Рис: TreeTerminologyPicture.png
#  Термины для деревьев (корень, лист) происходят от настоящего дерева, хотя оно перевернуто,
#  так как мы обычно рисуем корень наверху. Мы также используем термины,
#  которые рассматривают дерево как генеалогическое древо (дочерний узел и родительский узел).

# TODO: Interpreting a Decision Tree (Интерпретация дерева решений)
#  Чтобы интерпретировать это дерево решений, давайте рассмотрим пример.
#  Допустим, мы хотим узнать прогноз для 10-летнего пассажира мужского пола в классе P 2.
#  В первом узле, поскольку пол пассажира мужской, мы переходим к правому дочернему элементу.
#  Затем, начиная с их 10-летнего возраста, что составляет <= 13, мы переходим к левому дочернему элементу,
#  а в третьем узле мы переходим к правому дочернему элементу, поскольку P-класс равен 2.
#  На следующей диаграмме мы выделяем путь для этого пассажира.
#      См. Рис: InterpretingDecisionTreePicture.png
#  Обратите внимание, что нет правил, согласно которым мы используем каждую характеристику,
#  или в каком порядке мы используем характеристики, или для непрерывного значения (например, Возраст),
#  где мы делаем разделение. Стандартным в дереве решений является то, что каждое разделение имеет только 2 варианта.
#  Деревья решений часто предпочитают, если у вас есть нетехническая аудитория,
#  поскольку они могут легко интерпретировать модель.

# TODO: How did we get the Decision Tree? (Как мы получили дерево решений?)
#  При построении дерева решений мы не просто случайным образом выбираем, какую характеристику разделить первой.
#  Мы хотим начать с выбора характеристики с наибольшей прогностической силой.
#  Давайте снова посмотрим на наше то же самое Дерево решений.
#      См. Рис: HowDecisionTreePicture.png
#  Интуитивно понятно, что для нашего набора данных «Титаник»,
#  поскольку женщинам часто отдавалось предпочтение в спасательных шлюпках,
#  мы ожидаем, что пол будет очень важной характеристикой.
#  Поэтому использование этой характеристики в первую очередь имеет смысл.
#  На каждой стороне дерева решений мы независимо определим, какую характеристику разделить следующей.
#  В нашем примере выше второй сплит для женщин относится к классу P.
#  Второй сплит для мужчин – по возрасту.
#  Мы также отмечаем, что в некоторых случаях мы делаем три splits (сплита), а в некоторых — только два.
#  Для любого заданного набора данных существует множество различных возможных деревьев решений,
#  которые могут быть созданы в зависимости от порядка использования характеристик.
#  В следующих уроках мы увидим, как математически выбрать лучшее дерево решений.

# TODO: What makes a Good Split (Как сделать хороший сплит)
#  Чтобы определить, какую функцию мы должны разделить в первую очередь, нам нужно оценить каждое возможное разделение,
#  чтобы мы могли выбрать разделение с наивысшим баллом.
#  Нашей целью было бы идеально разделить данные.
#  Если, например, все женщины выжили в кораблекрушении, а все мужчины не выжили,
#  расщепление по полу было бы идеальным расщеплением.
#  Это редко случается с реальным набором данных, но мы хотим максимально приблизиться к этому.
#  Математический термин, который мы будем измерять, называется приростом информации.
#  Это будет значение от 0 до 1, где 0 — прирост информации при бесполезном расщеплении,
#  а 1 — прирост информации при идеальном расщеплении.
#  В следующих парах частей мы определим примесь Джини и энтропию,
#  который мы будем использовать для определения прироста информации.
#  Сначала мы обсудим интуицию о том, что делает хороший раскол.
#  Давайте рассмотрим пару возможных разбиений для набора данных Титаник.
#  Мы увидим, как он разделяет данные и почему один лучше другого.
#  Во-первых, давайте попробуем разделить по возрасту.
#  Поскольку возраст — это числовая характеристика, нам нужно выбрать порог для разделения.
#  Допустим, мы разделились на Возраст<=30 и Возраст>30.
#  Посмотрим, сколько у нас пассажиров с каждой стороны, и сколько из них выжило, а сколько нет.
#  С обеих сторон у нас около 40% пассажиров выживают.
#  Таким образом, мы ничего не выиграли от такого разделения данных.
#  Теперь давайте попробуем разделить на Sex.
#  См. Рис: WhatMakesGoodSplitPicture_1.png
#  См. Рис: WhatMakesGoodSplitPicture_2.png
#  По женской части мы видим, что подавляющее большинство выжило.
#  Что касается мужчин, подавляющее большинство не выжило. Это хороший раскол.
#  Мы стремимся к однородности (или чистоте) с каждой стороны.
#  В идеале мы бы отправили всех выживших пассажиров на одну сторону, а тех, кто не выжил, на другую.
#  Мы рассмотрим два разных математических измерения чистоты.
#  Мы будем использовать значения чистоты для расчета прироста информации.
#  Хороший выбор характеристики для разделения приводит к тому, что каждая сторона разделения является чистой.
#  Набор является чистым, если все точки данных принадлежат одному и тому же классу (выжили или не выжили).

# TODO: Gini Impurity (Примесь Джини)
#  Gini impurity (примесь Джини) является мерой чистоты набора.
#  Позже мы увидим, как можно использовать примесь Джини для расчета прироста информации.
#  Мы рассчитываем примесь Джини на подмножестве наших данных, исходя из того,
#  сколько точек данных в наборе составляют выжившие пассажиры и сколько пассажиров не выжили.
#  Это будет значение от 0 до 0.5, где 0.5 означает полную нечистоту (50% выжили и 50% не выжили),
#  а 0 означает полную чистоту (100% того же класса).
#  Формула Джини выглядит следующим образом:
#      См. Рис: GiniImpurityPicture_1.png
#  где:
#      p - процент выживших пассажиров
#      (1-p) - процент не выживших пассажиров
#  Вот график примеси Джини:
#      См. Рис: GiniImpurityPicture_2.png
#  Мы видим, что максимальное значение равно 0.5, когда выжило ровно 50% пассажиров в наборе.
#  Если все пассажиры выжили или не выжили (процент равен 0 или 1), то значение равно 0.
#  Рассчитаем примесь Джини для наших примеров из предыдущей части.
#  Сначала у нас было разделение на Возраст <= 30 и Возраст> 30.
#  Давайте посчитаем примеси Джини двух созданных нами наборов.
#  Слева для пассажиров с возрастом <= 30 сначала рассчитаем процент выживших пассажиров:
#  Процент выживших пассажиров = 197/(197+328) = 0.3752
#  Процент не выживших пассажиров = 1 - 0.375 = 0.6248
#  Теперь воспользуемся этим для вычисления примеси Джини:
#      См. Рис: GiniImpurityPicture_3.png
#      2 * 0.3752 * 0.6248 = 0.4688
# p = 0.3752
# x = 2 * p * (1-p)
# print(x.__round__(4))
# print(type(x))
# TODO: Мы видим, что это значение близко к 0.5, максимальному значению примеси Джини.
#  Это означает, что множество нечисто.
#  Теперь рассчитаем примесь Джини для правой стороны пассажиров с возрастом >30 лет.
#      2 * 145/(145+217) * 217/(145+217) = 0.4802
#  Это значение также близко к 0.5, так что снова у нас нечистый набор.
#  Теперь давайте посмотрим на значения Джини для другого разбиения, которое мы пробовали, расщепления по полу.
#  Слева для женщин-пассажиров мы вычисляем следующее значение примеси Джини.
#      См. Рис: GiniImpurityPicture_4.png
#      2 * 233/(233+81) * 81/(233+81) = 0.3828
#  С правой стороны для пассажиров мужского пола мы получаем следующее значение.
#      2 * 109/(109+464) * 464/(109+464) = 0.3081
#  Оба эти значения меньше, чем значения Джини для разделения по возрасту,
#  поэтому мы определяем, что разделение по признаку пола является лучшим выбором.
#  Прямо сейчас у нас есть два значения для каждого потенциального разделения.
#  Прирост информации будет способом объединения их в единую ценность.
#  Процент выживших пассажиров = 197/(197+328) = 0.3752
#  Процент не выживших пассажиров = 1 - 0.375 = 0.6248
#  Теперь воспользуемся этим для вычисления примеси Джини:
#      См. Рис: GiniImpurityPicture_3.png
#      2 * 0.3752 * 0.6248 = 0.4688
# p = 0.3752
# x = 2 * p * (1-p)
# print(x.__round__(4))
# print(type(x))

# TODO: ЗАДАЧА: Gini Impurity (Примесь Джини)
#  Скажем, мы сравниваем два разделения.
#  Разделение A:
#      Левая сторона: 10 выжили, 40 не выжили
#      Правая сторона: 40 выжили, 10 не выжили
#  Разделение B:
#      Левая сторона: 5 выжили, 5 не выжили
#      Правая сторона 45 выжили, 45
#  Посчитайте Gini Impurity для левой и правой сторон A и B:
# AL = 2 * 10/(10+40) * 40/(10+40)
# AR = 2 * 40/(10+40) * 10/(10+40)
# print(f'AL: {AL.__round__(4)}')
# print(f'AR: {AR.__round__(4)}')
#
# BL = 2 * 5/(5+5) * 5/(5+5)
# BR = 2 * 45/(45+45) * 45/(45+45)
# print(f'BL: {BL.__round__(4)}')
# print(f'BR: {BR.__round__(4)}')

# TODO: Entropy (Энтропия)
#  Энтропия — еще одна мера чистоты.
#  Это будет значение от 0 до 1, где:
#  1 — полностью нечистый (50% выжили и 50% не выжили),
#  0 — полностью чистый (100% того же класса).
#  Формула энтропии взята из физики:
#      Entropy = -(p * math.log2(p) + (1-p) * math.log2(1-p))
# import math
#
# p = 233/(233+81)
# # p = 109 / (109 + 464)
# Entropy = -(p * math.log2(p) + (1 - p) * math.log2(1 - p))
# print(Entropy.__round__(4))
# TODO: См. Рис: EntropyPicture_1.png
#  где:
#      p - процент выживших пассажиров
#      (1-p) - процент не выживших пассажиров
#  Вот график функции энтропии:
#      См. Рис: EntropyPicture_2.png
#  Вы можете видеть, что она имеет форму, аналогичную функции Джини.
#  Как и примесь Джини, максимальное значение имеет место, когда выжило 50% пассажиров в нашем наборе,
#  а минимальное значение — когда выжили все пассажиры или никто из них.
#  Форма графиков немного отличается. Вы можете видеть, что график энтропии стал немного толще.
#  Теперь давайте посчитаем значения энтропии для тех же двух потенциальных расщеплений.
#      См. Рис: EntropyPicture_3.png
#      On the left (Age<=30):
#      p = 197/(197+328) = 0.3752
#      Entropy = -(0.375 * log(0.375) + (1-0.375) * log(1-0.375)) = 0.9546
# TODO:
#      And on the right (Age>30):
#      p = 145/(145+217) = 0.4006
#      Entropy =  -(0.401 * log(0.401) + (1-0.401) * log(1-0.401)) =  0.9713
#  Оба эти значения близки к 1, что означает, что наборы нечисты.
#  Теперь давайте проделаем те же расчеты для разделения по признаку пола.
#      См. Рис: EntropyPicture_4.png
#      On the left (female):
#      p = 233/(233+81) = 0.7420
#      Entropy = -(p * log(p) + (1-p) * log(1-p)) = 0.8237
# TODO:
#      And on the right (male):
#      p = 109/(109+464) = 0.1902
#      Entropy =  -(p * log(p) + (1-p) * log(1-p)) = 0.7019
#  Вы можете видеть, что эти значения энтропии меньше, чем значения энтропии выше, так что это лучшее разделение.
#  Не очевидно, что лучше — Джини или энтропия. Часто это не имеет значения,
#  но вы всегда можете провести перекрестную проверку, чтобы сравнить дерево решений с энтропией
#  и дерево решений с Джини, чтобы увидеть, какое из них работает лучше.

# TODO: ЗАДАЧА: Энтропия
#  Каковы правильная левосторонняя и правосторонняя энтропия следующего разделения:
#  Левая сторона: 20 выжили, 0 не выжили
#  Правая сторона: 40 выжили, 40 не выжили
#  Мной написаная функция, не совсем верная если одно из значений равно нулю и при этом второе значение
#  тоже очень близко к нулю, но в остальном верно считает:
# def entropy(el: float, er: float):
#     import math
#     if el == er:
#         return f'EL: {1.0}\nER: {1.0}'
#     elif el == 0 and er > 0:
#         ell = el + 0.1
#         en_l = -((ell / (ell + er)) * math.log2(ell / (ell + er)) + (1 - (ell / (ell + er))) * math.log2(
#             1 - (ell / (ell + er))))
#         en_r = -((er / (ell + er)) * math.log2(er / (ell + er)) + (1 - (er / (ell + er))) * math.log2(
#             1 - (er / (ell + er))))
#         return f'EL: {en_l.__round__(4)}\nER: {en_r.__round__(4)}'
#     elif el > 0 and er == 0:
#         err = er + 0.1
#         en_l = -((el / (el + err)) * math.log2(el / (el + err)) + (1 - (el / (el + err))) * math.log2(
#             1 - (el / (el + err))))
#         en_r = -((err / (el + err)) * math.log2(err / (el + err)) + (1 - (err / (el + err))) * math.log2(
#             1 - (err / (el + err))))
#         return f'EL: {en_l.__round__(4)}\nER: {en_r.__round__(4)}'
#     else:
#         en_l = -((el / (el + er)) * math.log2(el / (el + er)) + (1 - (el / (el + er))) * math.log2(
#             1 - (el / (el + er))))
#         en_r = -((er / (el + er)) * math.log2(er / (el + er)) + (1 - (er / (el + er))) * math.log2(
#             1 - (er / (el + er))))
#         return f'EL: {en_l.__round__(4)}\nER: {en_r.__round__(4)}'
#
#
# print(entropy(20, 0))

# TODO: Information Gain (Получение информации)
#  Теперь, когда у нас есть способ вычисления числового значения примеси, мы можем определить прирост информации:
#      Information gain = H*(S) - |A|/|S| * H*(A) - |B|/|S| * H*(B)
#      См. Рис: InformationGainPicture_1.png
#  где:
#      H - наша мера примеси (либо примесь Джини, либо энтропия)
#      S - исходный набор данных
#      A и B - два набора, на которые мы разбиваем набор данных S.
#  В первом приведенном выше примере:
#      A - это пассажиры с возрастом <= 30
#      B - пассажиры с возрастом > 30.
#  Во втором примере:
#      A - пассажиры женского пола
#      B - пассажиры мужского пола
#      |А| означает размер A
#  Давайте вычислим это значение для наших двух примеров.
#  Давайте используем примесь Джини в качестве нашей меры примеси.
#  Мы уже вычислили большинство значений примеси Джини, хотя нам нужно вычислить примесь Джини для всего набора.
#  Из 887 пассажиров выжило 342 пассажира и не выжило 545 пассажиров, поэтому примесь Джини выглядит следующим образом:
#      Gini = 2 * 342/887 * 545/887 = 0.4738
#  Опять же, вот первый потенциальный раскол.
#      См. Рис: InformationGainPicture_2.png
#  Обратите внимание, что у нас 197+328=525 пассажиров слева (возраст <= 30)
#  и 145+217=362 пассажира справа (возраст > 30).
#  Подтягивая значения примеси Джини, которые мы рассчитали ранее, мы получаем следующий прирост информации:
#      Information gain = 0.4738 - 525/887 * 0.4689 - 362/887 * 0.4802 = 0.0003
#  Это значение очень мало, что означает, что мы очень мало получаем от этого разделения.
#  Теперь посчитаем информационный прирост при разбиении по полу.
#      См. Рис: InformationGainPicture_3.png
#  У нас 233+81=314 пассажиров слева (женщины) и 109+464=573 пассажиров справа (мужчины). Вот прирост информации:
#      Information gain = 0.4738 - 314/887 * 0.3828 - 573/887 * 0.3081 = 0.1393
#  Таким образом, мы видим, что прирост информации намного лучше для этого разделения.
#  Следовательно, разделение по полу - гораздо лучший выбор при построении нашего дерева решений,
#  чем разделение по возрасту с порогом 30.
#  Работа, которую мы проделали, заключалась лишь в сравнении двух возможных расщеплений.
#  Нам нужно будет сделать те же самые расчеты для каждого возможного разделения, чтобы найти наилучшее.
#  К счастью, нам не нужно делать вычисления вручную!

# TODO: Building the Decision Tree (Построение дерева решений
#  Мы заложили основы, необходимые для построения дерева решений.
#  Вот процесс, через который мы проходим: Чтобы определить, как выполнить первое разбиение,
#  мы просматриваем все возможные разбиения и вычисляем прирост информации, если мы использовали это разбиение.
#  Для числовых характеристик, таких как возраст, PClass и стоимость проезда,
#  мы пробуем все возможные пороговые значения.
#  Разделение по возрастному порогу 50 означает, что точки данных с возрастом <= 50 составляют одну группу,
#  а точки с возрастом > 50 другую.
#  Таким образом, поскольку в нашем наборе данных 89 разных возрастов, у нас есть 88 разных разбиений,
#  чтобы попробовать характеристику возраста!
#  Нам нужно попробовать все эти потенциальные разделения:
#  1. Пол (мужской | женский)
#  2. P-класс (1 или 2 | 3)
#  3. P-класс (1 | 2 или 3)
#  4. Возраст (0 | >0)
#  5. Возраст (<=1 | >1)
#  6. Возраст (<=2 | >2)
#  7. и т.д.….
#  Существует:
#  1 потенциальное разделение по полу,
#  2 потенциальных разделения по P-классу
#  88 потенциальных разделений по возрасту.
#  Существует 248 различных значений тарифа, поэтому для этой характеристики существует 247 возможных сплитов.
#  Если мы рассматриваем только эти четыре характеристик, нам нужно рассмотреть 338 потенциальных разделений.
#  Для каждого из этих расщеплений мы рассчитываем прирост информации и выбираем расщепление с наибольшим значением.
#  Теперь мы делаем то же самое для следующего уровня. Скажем, мы сделали первый сплит на пол.
#  Теперь для всех женщин-пассажиров мы пробуем все возможные разделения для каждой из характеристик и выбираем ту,
#  которая дает наибольший прирост информации.
#  Мы можем разделить одну и ту же характеристику дважды, если эта характеристика имеет несколько возможных порогов.
#  Пол можно разделить только один раз, но характеристики «Проезд» и «Возраст» можно разделить на несколько раз.
#  Независимо, мы делаем аналогичный расчет для пассажиров мужского пола
#  и выбираем разделение с наибольшим информационным приростом.
#  Таким образом, у нас может быть другое второе разделение для пассажиров-мужчин и пассажиров-женщин.
#  Мы продолжаем этот процесс до тех пор, пока у нас не останется больше характеристик для разделения.
#  Это много вещей - которые нужно попробовать, но нам просто нужно использовать вычислительную мощность.
#  Это делает деревья решений немного медленными для построения,
#  но как только дерево построено, очень быстро можно сделать прогноз.

# TODO: Decision Tree Diagram (Схема дерева решений)
#  Давайте посмотрим на пример дерева решений для набора данных Titanic.
#  В каждом внутреннем узле у нас есть характеристика и порог для разделения,
#  количество выборок и распределение одинаковых (# не выжило против выжило).
#      См. Рис: DecisionTreeDiagramPicture.png
#  Чтобы интерпретировать это, давайте начнем с рассмотрения корневого узла.
#  Он говорит: мужчина <= 0.5 выборки = 887 значение = [545, 342]
#  Это означает, что первое разделение будет в столбце мужчины.
#  Если значение <=0.5 (что означает, что пассажир — женщина), мы идем к левому ребенку,
#  а если значение > 0.5 (что означает, что пассажир — мужчина), мы идем к правому ребенку.
#  Есть 887 точек данных для старта, 545 отрицательных случаев (не выжили) и 342 положительных (выжили).
#  Если вы посмотрите на двух дочерних элементов корневого узла, мы увидим,
#  сколько точек данных было отправлено в каждом направлении на основе разделения по полу.
#  В нашем наборе данных 314 пассажиров женского пола и 573 пассажира мужского пола.
#  Вы можете видеть, что второе разделение для пассажиров женского пола
#  отличается от второго разделения для пассажиров мужского пола.
#  Эта диаграмма была создана с помощью graphviz, которым мы научимся пользоваться в следующем уроке.

# TODO: How to Make a Prediction (Как сделать прогноз)
#  Давайте снова посмотрим на ту же диаграмму дерева решений:
#      См. Рис: HowMakePredictionPicture_1.png
#  Допустим, мы хотели бы использовать это дерево решений,
#  чтобы сделать прогноз для пассажира со следующими значениями:
#  Пол: женский
#  Pclass: 3
#  Стоимость проезда: 25
#  Возраст: 30
#  Мы задаем вопрос в каждом узле:
#  если ответ да - переходим к левому дочернему элементу,
#  если ответ нет - переходим к правому дочернему элементу.
#  Мы начинаем с корневого узла.
#  Является ли значение мужского признака <= 0.5? (Этот вопрос также можно было бы задать как «Пассажирка женщина?»)
#  Поскольку ответ «да», мы идем к левому ребенку.
#  Является ли Pclass <= 0.5? Поскольку ответ нет, идем к правому потомку.
#  Стоимость проезда <= 23.35? Поскольку ответ нет, идем к правому потомку.
#  Теперь мы находимся на листовом узле. Вот путь, который мы выбрали, выделен.
#      См. Рис: HowMakePredictionPicture_2.png
#  Листовой узел, на котором мы заканчиваем, имеет следующий текст:
#      27
#      [24, 3]
#  Это означает, что в нашем наборе данных есть 27 точек данных, которые также попадают в этот конечный узел.
#  24 из них не выжили, а трое выжили. Это означает, что наш прогноз таков, что пассажир не выжил.
#  Поскольку не существует правил построения дерева,
#  дерево решений задает пассажиру-женщине совершенно другие вопросы, чем пассажиру-мужчине.

# TODO: DecisionTreeClassifier Class (Класс классификатора дерева решений)
#  Как и в случае с логистической регрессией, в scikit-learn есть класс дерева решений.
#  Код для построения модели дерева решений очень похож на построение модели логистической регрессии.
#  Scikit-learn сделал это намеренно, чтобы можно было легко создавать
#  и сравнивать разные модели для одного и того же набора данных.
#  Вот заявление об импорте.
#      from sklearn.tree import DecisionTreeClassifier
#  Теперь мы можем применить те же методы, которые мы использовали с классом LogisticRegression:
#      - fit (для обучения модели)
#      - score (для расчета показателя точности)
#      - predict (для прогнозирования)
#  Сначала мы создаем объект DecisionTreeClassifier.
#      model = DecisionTreeClassifier()
#  Мы делаем разделение обучения/тестирования, используя random_state,
#  чтобы каждый раз, когда мы запускаем код, мы получали одно и то же разделение.
#      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)
#  Затем мы используем метод подгонки для обучения модели.
#      model.fit(X_train, y_train)
# TODO: Мы можем использовать метод прогнозирования, чтобы увидеть, что предсказывает модель.
#  Здесь мы можем увидеть прогноз для пассажира-мужчины в Pclass 3, которому 22 года,
#  у него на борту 1 брат/сестра/супруга, 0 родителей/детей на борту, и он заплатил за проезд 7.25.
# import pandas as pd
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.model_selection import train_test_split
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)
# model = DecisionTreeClassifier()
# model.fit(X_train, y_train)
# print(model.predict([[3, True, 22, 1, 0, 7.25]]))
# TODO: Мы видим, что модель предсказывает, что пассажир не выжил.
#  Это тот же прогноз, который дала модель логистической регрессии.
#  Обратите внимание, что у нас есть те же методы для DecisionTreeClassifier, что и для объекта LogisticRegression.

# TODO: Scoring a Decision Tree Model (Оценка модели дерева решений)
#  Мы можем использовать методы оценки и прогнозирования, чтобы получить оценки точности, достоверности и полноты.
# import pandas as pd
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import KFold
# from sklearn.metrics import precision_score, recall_score
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# kf = KFold(n_splits=5, shuffle=True, random_state=10)
# dt_accuracy_scores = []
# dt_precision_scores = []
# dt_recall_scores = []
# lr_accuracy_scores = []
# lr_precision_scores = []
# lr_recall_scores = []
# for train_index, test_index in kf.split(X):
#     X_train, X_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]
#     dt = DecisionTreeClassifier()
#     dt.fit(X_train, y_train)
#     dt_accuracy_scores.append(dt.score(X_test, y_test))
#     dt_y_pred = dt.predict(X_test)
#     dt_precision_scores.append(precision_score(y_test, dt_y_pred))
#     dt_recall_scores.append(recall_score(y_test, dt_y_pred))
#     lr = LogisticRegression()
#     lr.fit(X_train, y_train)
#     lr_accuracy_scores.append(lr.score(X_test, y_test))
#     lr_y_pred = lr.predict(X_test)
#     lr_precision_scores.append(precision_score(y_test, lr_y_pred))
#     lr_recall_scores.append(recall_score(y_test, lr_y_pred))
# print("Decision Tree")
# print("  accuracy:", np.mean(dt_accuracy_scores))
# print("  precision:", np.mean(dt_precision_scores))
# print("  recall:", np.mean(dt_recall_scores))
# print("Logistic Regression")
# print("  accuracy:", np.mean(lr_accuracy_scores))
# print("  precision:", np.mean(lr_precision_scores))
# print("  recall:", np.mean(lr_recall_scores))
# TODO: Мы можем использовать k-кратную перекрестную проверку, чтобы получить точную меру показателей
#  и сравнить значения с моделью логистической регрессии.
#  Мы используем random_state при создании объекта KFold, чтобы каждый раз получать одинаковые результаты.
#  Вы можете видеть, что точность и достоверность модели логистической регрессии выше,
#  а количество полноты двух моделей примерно одинаковое.
#  Модель логистической регрессии работает лучше,
#  хотя мы все еще можем использовать дерево решений для его интерпретируемости.

# TODO: Gini vs Entropy (Джини против Энтропии)
#  Критерием примеси по умолчанию в алгоритме дерева решений scikit-learn является примесь Джини.
#  Однако они также реализовали энтропию, и вы можете выбрать,
#  какую из них вы хотите использовать при создании объекта DecisionTreeClassifier.
#  Если вы перейдете к документам, вы увидите, что одним из параметров является критерий.
#      См. Рис: GiniEntropyPicture_1.png
#  Документы находятся здесь:
#  https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
#  Чтобы построить дерево решений, использующее энтропию, нам нужно установить для параметра критерия значение энтропии.
#  Вот код для построения дерева решений, в котором вместо примеси Джини используется энтропия.
#      dt = DecisionTreeClassifer(criterion='entropy')
#  Теперь мы можем сравнить дерево решений с использованием Джини с деревом решений с использованием энтропии.
#  Сначала мы создаем k-кратное разделение, поскольку, когда мы сравниваем две модели, мы хотим,
#  чтобы они использовали одни и те же разделения обучения/тестирования, чтобы быть справедливыми.
#  Затем мы проводим k-кратную перекрестную проверку с каждой из двух возможных моделей.
#  Мы рассчитываем точность, достоверность и полноту для каждого из двух вариантов.
#  Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# import numpy as np
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.model_selection import KFold
# from sklearn.metrics import accuracy_score, precision_score, recall_score
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# kf = KFold(n_splits=5, shuffle=True)
# for criterion in ['gini', 'entropy']:
#     print("Decision Tree - {}".format(criterion))
#     accuracy = []
#     precision = []
#     recall = []
#     for train_index, test_index in kf.split(X):
#         X_train, X_test = X[train_index], X[test_index]
#         y_train, y_test = y[train_index], y[test_index]
#         dt = DecisionTreeClassifier(criterion=criterion)
#         dt.fit(X_train, y_train)
#         y_pred = dt.predict(X_test)
#         accuracy.append(accuracy_score(y_test, y_pred))
#         precision.append(precision_score(y_test, y_pred))
#         recall.append(recall_score(y_test, y_pred))
#     print("accuracy:", np.mean(accuracy))
#     print("precision:", np.mean(precision))
#     print("recall:", np.mean(recall), '\n')
#     print()
# TODO: Мы видим очень небольшую разницу в производительности Gini и Entropy.
#  Это ожидаемо, поскольку на самом деле они не очень разные функции.
#  Редко можно найти набор данных, в котором выбор имел бы значение.

# TODO: Visualizing Decision Trees (Визуализация деревьев решений)
#  Если вы хотите создать изображение вашего графика в формате png, как показано в этом модуле,
#  вы можете использовать функцию export_graphviz scikit-learn.
#  Сначала мы импортируем его.
#      from sklearn.tree import export_graphviz
#      dot_file = export_graphviz(dt, feature_names=feature_names)
# TODO: Затем мы используем функцию export_graphviz. Здесь dt — объект дерева решений,
#  а feature_names — список имен характеристик.
#  Объекты Graph хранятся в виде файлов .dot, которые могут быть программой GraphViz.
#  Наша цель — сохранить файл изображения png. Мы сможем преобразовать точечный файл в png-файл,
#  поэтому мы сначала сохраняем точечный файл в переменную, поэтому мы сохраняем точечный файл,
#  созданный функцией export_graphviz, чтобы мы могли преобразовать его в png.
#  Затем мы можем использовать модуль graphviz, чтобы преобразовать его в формат изображения png.
#      import graphviz
#      graph = graphviz.Source(dot_file)
# TODO: Наконец, мы можем использовать метод рендеринга для создания файла.
#  Мы сообщаем ему имя файла и формат файла. По умолчанию он создаст дополнительные файлы,
#  которые нам не интересны, поэтому мы добавляем очистку, чтобы он избавился от них и возможность сразу увидеть
#  изображение png дерева решений view=True.
#      graph.render(filename='tree', format='png', cleanup=True, view=True)
# TODO: Теперь у вас должен быть файл с именем tree.png на вашем компьютере:
#  C:/Users/vitaly/PycharmProjects/SQLite/tree.png
#  Вот код для визуализации дерева для набора данных Titanic только с характеристиками Sex и Pclass.
# from sklearn.tree import export_graphviz, DecisionTreeClassifier
# import graphviz
# from IPython.display import Image
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
#
# feature_names = ['Pclass', 'male']
# X = df[feature_names].values
# y = df['Survived'].values
#
# dt = DecisionTreeClassifier()
# dt.fit(X, y)
#
# dot_file = export_graphviz(dt, feature_names=feature_names)
# graph = graphviz.Source(dot_file)
# graph.render(filename='tree', format='png', cleanup=True, view=True)
# TODO: Вот результат:
#  См. Рис: VisualizingDecisionTreesPicture.png либо См. Рис: C:/Users/vitaly/PycharmProjects/SQLite/tree.png
#  Если вы собираетесь запускать это на своем компьютере, обязательно сначала установите graphviz.
#  Вы можете сделать это с помощью команды: choco install graphviz, запущенной в Windows PowerShell (под админом).
#  Вот код для визуализации дерева для набора данных President USA только с характеристиками height и age.
# from sklearn.tree import export_graphviz, DecisionTreeClassifier
# import graphviz
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv')
# feature_names = ['height', 'age']
# # feature_names = ['height']
# # feature_names = ['age']
#
# X = df[feature_names].values
# y = df['party'].values
# # y = df['name'].values
#
# dt = DecisionTreeClassifier()
# dt.fit(X, y)
#
# dot_file = export_graphviz(dt, feature_names=feature_names)
# graph = graphviz.Source(dot_file)
# graph.render(filename='tree', format='png', cleanup=True, view=True)

# TODO: Tendency to Overfit (Склонность к переобучению)
#  Напомним, что overfitting (переобучение) — это когда мы хорошо построим модель для тренировочного набора,
#  но она плохо работает на тестовом наборе. Деревья решений невероятно склонны к переоснащению.
#  Поскольку они могут продолжать иметь дополнительные узлы в дереве, которые разбиваются на характеристики,
#  модель действительно может углубиться в специфику обучающего набора.
#  В зависимости от данных это может привести к модели, которая не отражает истинную природу данных и не обобщает.
#  Может быть, у нас есть только одна точка данных, которая идет к конечному узлу.
#  Может быть, нет смысла иметь это дополнительное разделение.
#  Давайте посмотрим на диаграмму дерева решений для набора данных Titanic.
#  Это результирующее дерево, когда мы строим дерево решений с помощью scikit-learn для всего набора данных.
#  Мы просто смотрим на часть дерева решений, поскольку оно очень большое. Мы выделили конкретный путь интереса.
#      См. Рис: TendencyOverfitPicture.png
#  Если вы пойдете по выделенному пути, вы увидите, что мы разделились на пол, класс,
#  а затем разделились на возраст 9 раз подряд с разными пороговыми значениями.
#  В результате получается график, который очень придирчив к возрасту.
#  Женщина-пассажир P-класса 3 в возрасте 31 года направляется к другому листовому узлу,
#  чем аналогичный пассажир в возрасте 30.5, 30 или 29 лет.
#  Модель предсказывает, что женщина-пассажир в возрасте 35 лет выживает, 32-летняя не выживает,
#  31-летняя выживает и 30 лет не выживает.
#  Это, вероятно, слишком мелкозернисто и придает отдельным точкам данных из нашего набора данных слишком много силы.
#  Вы можете видеть, что все конечные узлы имеют несколько точек данных и часто только одну.
#  Если вы позволите дереву решений продолжать строиться, оно может создать переоснащенное дерево,
#  которое не будет отражать сущность данных.

# TODO: Pruning (Обрезка)
#  Чтобы решить эти проблемы, мы делаем так называемую обрезку дерева.
#  Это означает, что мы уменьшаем дерево с целью уменьшения переобучения.
#  Обрезка бывает двух видов: Pre-pruning (предварительная) и Post-pruning (постобрезная).
#  При предварительной обрезке у нас есть правила, когда прекращать построение дерева,
#  поэтому мы прекращаем построение до того, как дерево станет слишком большим.
#  При постобрезке мы строим все дерево, а затем просматриваем дерево и решаем,
#  какие листья удалить, чтобы сделать дерево меньше.
#  Термин обрезка происходит от того же термина в сельском хозяйстве.
#  Фермеры обрезают ветки деревьев, и мы делаем то же самое с нашим деревом решений.

# TODO: Pre-pruning (Предварительная обрезка)
#  Мы сосредоточимся на методах предварительной обрезки, так как их проще реализовать.
#  У нас есть несколько вариантов, как ограничить рост дерева.
#  Вот некоторые часто используемые методы предварительной обрезки:
#  • Максимальная глубина: выращивание дерева только до определенной глубины или высоты дерева.
#  Если максимальная глубина равна 3, для каждой точки данных будет не более 3 разделений.
#  • Размер листьев: не разделять узел, если количество выборок в этом узле ниже порогового значения.
#  • Количество конечных узлов: ограничить общее количество конечных узлов, разрешенных в дереве.
#  Сокращение — это баланс. Например, если вы установите слишком маленькую максимальную глубину,
#  у вас не будет много дерева, и у вас не будет никакой предсказательной силы - это называется недообучением.
#  Точно так же, если размер листьев слишком велик или количество узлов листьев слишком мало,
#  у вас будет неподходящая модель.
#  Не существует точной науки о том, какой метод предварительной обрезки даст лучшие результаты.
#  На практике мы пробуем несколько разных значений для каждого параметра и проводим перекрестную проверку,
#  чтобы сравнить их производительность.

# TODO: Pre-pruning Parameters (Параметры предварительной обрезки)
#  Scikit-learn реализовал довольно много методов предварительной обрезки.
#  В частности, мы рассмотрим три параметра:
#  - max_depth (максимальная глубина)
#  - min_samples_leaf (минимальное количество выборок на лист)
#  - max_leaf_nodes (максимальное количество конечных узлов)
#  Посмотрите документы для деревьев решений, чтобы найти полное объяснение этих трех параметров.
#  Техника предварительной обрезки 1: ограничение глубины Мы используем параметр max_depth,
#  чтобы ограничить количество шагов, которые дерево может иметь между корневым узлом и конечными узлами.
#  Техника предварительной обрезки 2: избегание листьев с небольшим количеством точек данных.
#  Мы используем параметр min_samples_leaf, чтобы указать модели прекратить построение дерева досрочно,
#  если количество точек данных в листе будет ниже порогового значения.
#  Техника предварительной обрезки 3: ограничение количества листовых узлов
#  Мы используем max_leaf_nodes, чтобы установить ограничение на количество листовых узлов в дереве.
#  Вот код для создания дерева решений со следующими свойствами:
#  • максимальная глубина 3
#  • минимальное количество выборок на лист 2
#  • максимальное количество конечных узлов 10
#      dt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=2, max_leaf_nodes=10)
#  Теперь вы можете обучить модель и протестировать ее, как мы делали это раньше.
#  Вы можете использовать столько параметров, сколько хотите.
#  Чтобы определить наилучшие значения параметров предварительной обрезки,
#  мы будем использовать перекрестную проверку для сравнения нескольких потенциальных вариантов.

# TODO: Grid Search (Поиск по сетке)
#  Мы не сможем интуитивно определить наилучшие значения для параметров предварительной обрезки.
#  Чтобы решить, что использовать, мы используем перекрестную проверку и сравниваем метрики.
#  Мы могли бы перебирать различные варианты, как мы это делали в уроке по деревьям решений в Scikit-learn,
#  но в scikit-learn встроен класс поиска по сетке, который сделает это за нас.
#  Класс называется GridSearchCV. Начнем с импорта.
#      from sklearn.model_selection import GridSearchCV
# TODO: GridSearchCV имеет четыре параметра, которые мы будем использовать:
#  1. Модель (в данном случае DecisionTreeClassifier)
#  2. Сетка параметров: словарь имен параметров и всех возможных значений
#  3. Какую метрику использовать (по умолчанию accuracy (точность))
#  4. Сколько сгибов для k-кратной перекрестной проверки Давайте создадим переменную сетки param.
#  Мы дадим список всех возможных значений для каждого параметра, который мы хотим попробовать.
#      param_grid = {
#          'max_depth': [5, 15, 25],
#          'min_samples_leaf': [1, 3],
#          'max_leaf_nodes': [10, 20, 35, 50]}
# TODO: Теперь мы создаем объект поиска по сетке. Мы будем использовать приведенную выше сетку параметров,
#  установим метрику оценки на оценку F1 и проведем 5-кратную перекрестную проверку.
#      dt = DecisionTreeClassifier()
#      gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5)
# TODO: Теперь мы можем подогнать объект поиска по сетке.
#  Это может занять некоторое время, так как он пытается использовать все возможные комбинации параметров.
#      gs.fit(X, y)
# TODO: Поскольку у нас есть 3 возможных значения для max_depth, 2 для min_samples_leaf и 4 для max_leaf_nodes,
#  у нас есть 3 * 2 * 4 = 24 различных комбинации, которые нужно попробовать:
#  max_depth: 5, min_samples_leaf: 1, max_leaf_nodes: 10
#  max_depth: 15, min_samples_leaf: 1, max_leaf_nodes: 10
#  max_depth: 25, min_samples_leaf: 1, max_leaf_nodes: 10
#  max_depth: 5, min_samples_leaf: 3, max_leaf_nodes: 10
#  ...
#  Мы используем атрибут best_params_ , чтобы увидеть, какая модель выиграла.
# import pandas as pd
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.model_selection import GridSearchCV
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# param_grid = {
#     'max_depth': [5, 15, 25],
#     'min_samples_leaf': [1, 3],
#     'max_leaf_nodes': [10, 20, 35, 50]}
# dt = DecisionTreeClassifier()
# gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5)
# gs.fit(X, y)
# print("best params:", gs.best_params_)
# TODO: Таким образом, мы видим, что лучшая модель имеет максимальную глубину 15,
#  максимальное количество листовых узлов 35 и минимальное количество выборок на лист 1.
#  Атрибут best_score_ сообщает нам оценку модели-победителя.
# import pandas as pd
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.model_selection import GridSearchCV
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# param_grid = {
#     'max_depth': [5, 15, 25],
#     'min_samples_leaf': [1, 3],
#     'max_leaf_nodes': [10, 20, 35, 50]}
# dt = DecisionTreeClassifier()
# gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5)
# gs.fit(X, y)
# print("best score:", gs.best_score_)
# TODO: Часто есть несколько моделей, которые имеют очень похожие характеристики.
#  Если вы запустите это несколько раз, вы можете получить несколько разные результаты
#  в зависимости от случайности распределения точек между складками.
#  Как правило, если у нас есть несколько моделей с сопоставимой производительностью,
#  мы выбираем более простую модель.

# TODO: Computation (Вычисление)
#  Говоря о том, сколько вычислений требуется для алгоритма машинного обучения, мы разделяем его на два вопроса:
#  сколько вычислений требуется для построения модели и сколько требуется для прогнозирования.
#  Построение дерева решений требует больших вычислительных затрат.
#  Это связано с тем, что в каждом узле мы пробуем каждую характеристику и порог как возможное разделение.
#  Мы должны каждый раз рассчитывать информационный прирост каждого из этих возможных расщеплений.
#  Это очень затратно в вычислительном отношении.
#  С другой стороны, прогнозирование с помощью дерева решений требует очень мало вычислительных ресурсов.
#  Вам просто нужно задать серию вопросов да/нет о точке данных, чтобы получить прогноз.
#  Как правило, нас гораздо больше волнует время вычислений для прогнозирования, чем обучение.
#  Прогнозы часто должны происходить в режиме реального времени, пока пользователь ожидает результата.

# TODO: Performance (Производительность)
#  Деревья решений могут работать достаточно хорошо в зависимости от данных, хотя, как мы уже говорили,
#  они склонны к overfitting (перезаполнению). Поскольку листовой узел может иметь только одну точку данных,
#  которая попадает туда, он дает слишком много мощности отдельным точкам данных.
#  Чтобы исправить проблемы переобучения, деревья решений обычно требуют некоторой настройки,
#  чтобы получить наилучшую возможную модель.
#  Методы обрезки используются для ограничения размера дерева и помогают уменьшить перезаполнение.
#  Деревья решений часто требуют работы, чтобы работать наравне с другими моделями без настройки.

# TODO: Interpretability (Интерпретируемость)
#  Основная причина, по которой людям нравится выбирать деревья решений, заключается в том,
#  что они легко интерпретируются. В зависимости от того, для чего вы строите модель,
#  вам может потребоваться указать причину, по которой вы сделали определенный прогноз.
#  Нетехнический специалист может интерпретировать дерево решений, чтобы легко дать объяснение прогнозу.
#  Особенно это проявляется в юридических ситуациях.
#  Скажем, вы банк и у вас есть модель, предсказывающая, следует ли человеку давать ссуду или нет.
#  Важно уметь объяснить, почему модель приняла такое решение,
#  иначе вы можете скрыть дискриминационные практики внутри модели.
#  Интерпретируемость — самое большое преимущество деревьев решений.
#  Это будет зависеть от ситуации, важно ли это для вашей проблемы.

# TODO: ЗАДАЧА: Machine Learning - Split to Achieve Gain (Машинное обучение — разделение для получения прибыли)
#  Рассчитать прирост информации.
#  Задача Учитывая набор данных и разделение набора данных, рассчитайте прирост информации, используя примесь Джини.
#  Первая строка ввода представляет собой список целевых значений в исходном наборе данных.
#  Вторая строка — это целевые значения левого разделения, а третья строка — целевые значения правого разделения.
#  Округлите результат до 5 знаков после запятой. Вы можете использовать round(x, 5).
#  Формат ввода: Три строки из 1 и 0, разделенные пробелами
#  Формат вывода: Число с плавающей запятой (округлено до 5 знаков после запятой)
#  Sample Input:
#  1 0 1 0 1 0
#  1 1 1
#  0 0 0
#  Sample Output:
#  0.5
#  Объяснение:
#  В начальном наборе есть 3 положительных случая и 3 отрицательных случая.
#  Таким образом, примесь Джини составляет: gini impurity is 2*0.5*0.5=0.5
#  Левый набор имеет 3 положительных случая и 0 отрицательных случаев. Таким образом, примесь Джини равна: 2*1*0=0.
#  В правом наборе 0 положительных случаев и 3 отрицательных случая. Таким образом, примесь Джини равна: 2*0*1=0.
#  Прирост информации: The information gain is 0.5-0-0=0.5
# TODO: РЕШЕНИЕ №1:
# S = [int(x) for x in input().split()]
# A = [int(x) for x in input().split()]
# B = [int(x) for x in input().split()]
#
#
# def p(a):
#     x = sum(a) / len(a)
#     return x
#
#
# def gini(a):
#     h = 2 * p(a) * (1 - p(a))
#     return h
#
#
# info_gain = gini(S) - (len(A) / len(S)) * gini(A) - (len(B) / len(S)) * gini(B)
# print(info_gain.__round__(5))
# TODO: РЕШЕНИЕ №2:
# S = [int(x) for x in input().split()]
# A = [int(x) for x in input().split()]
# B = [int(x) for x in input().split()]
#
# print(((2 * sum(S) / len(S) * (1 - (sum(S) / len(S)))) - (len(A) / len(S)) * (
#             2 * sum(A) / len(A) * (1 - (sum(A) / len(A)))) - (len(B) / len(S)) * (
#                         2 * sum(B) / len(B) * (1 - (sum(B) / len(B))))).__round__(5))
# TODO: РЕШЕНИЕ №3:
# S = [int(x) for x in input().split()]
# A = [int(x) for x in input().split()]
# B = [int(x) for x in input().split()]
#
# ps = sum(S) / len(S)
# gini_s = 2 * ps * (1 - ps)
#
# left = len(A) / len(S)
#
# pa = sum(A) / len(A)
# gini_a = 2 * pa * (1 - pa)
#
# right = len(B) / len(S)
#
# pb = sum(B) / len(B)
# gini_b = 2 * pb * (1 - pb)
#
# info_gain = gini_s - left * gini_a - right * gini_b
# print(info_gain.__round__(5))

# TODO: Improving on Decision Trees (Улучшение деревьев решений)
#  Как мы узнали из предыдущего модуля, главный недостаток деревьев решений заключается в том,
#  что они склонны к переобучению. Мы увидели, что можем улучшить их производительность с помощью обрезки,
#  но в этом модуле мы увидим способ использования деревьев решений для создания лучшей модели.
#  Деревья решений очень восприимчивы к случайным особенностям в наборе обучающих данных.
#  Мы говорим, что деревья решений имеют высокую дисперсию, поскольку,
#  если вы случайно измените набор обучающих данных, вы можете получить совсем другое дерево.
#  Одно из преимуществ деревьев решений по сравнению с такой моделью, как логистическая регрессия,
#  заключается в том, что они не делают предположений о том, как структурированы данные.
#  В логистической регрессии мы предполагаем, что можем провести линию для разделения данных.
#  Иногда наши данные просто не структурированы таким образом.
#  Дерево решений имеет потенциал для понимания сущности данных, независимо от того, как они структурированы.
#  В этом модуле мы будем изучать случайные леса, которые, как вы можете догадаться из названия,
#  представляют собой модель, построенную из нескольких деревьев. Цель случайных лесов состоит в том,
#  чтобы воспользоваться преимуществами деревьев решений, смягчив при этом проблемы дисперсии.
#  Случайный лес — это пример группы, поскольку он использует
#  несколько моделей машинного обучения для создания одной модели.

# TODO: Bootstrapping (Начальная загрузка)
#  Начальная выборка — это случайная выборка точек данных, в которой мы случайным образом выбираем точки данных
#  для замены из нашего исходного набора данных, чтобы создать набор данных того же размера.
#  Случайный выбор с заменой означает, что мы можем выбирать одну и ту же точку данных несколько раз.
#  Это означает, что в образце с начальной загрузкой некоторые точки данных из исходного набора данных
#  будут появляться несколько раз, а некоторые не будут отображаться вообще.
#  Например, если у нас есть четыре точки данных A, B, C, D, это могут быть 3 передискретизации:
#  A, A, B, C
#  B, B, B, D
#  A, A, C, C
#  Мы бы предпочли получить больше выборки данных из популяции, но поскольку все, что у нас есть,
#  это наш обучающий набор, мы используем его для создания дополнительных наборов данных.
#  Мы используем начальную загрузку, чтобы имитировать создание нескольких образцов.

# TODO: Bagging Decision Trees (Деревья принятия решений)
#  Bootstrap Aggregation (Агрегация начальной загрузки) или Bagging - это метод уменьшения дисперсии в отдельной модели,
#  путем создания группы из нескольких моделей, построенных на бутстрепных образцах.
#  Чтобы упаковать деревья решений, мы создаем несколько (скажем, 10) бутстрапированных повторных выборок нашего
#  обучающего набора данных. Таким образом, если у нас есть 100 точек данных в нашем обучающем наборе,
#  каждая повторная выборка будет иметь 100 точек данных, случайно выбранных из нашего обучающего набора.
#  Напомним, что мы выбираем случайным образом с заменой, а это означает,
#  что некоторые точки данных будут появляться несколько раз, а некоторые не будут появляться вовсе.
#  Мы создаем дерево решений с каждой из этих 10 повторных выборок. Чтобы сделать прогноз, мы делаем прогноз
#  с каждым из 10 деревьев решений, а затем каждое дерево решений получает голос.
#  Прогноз, набравший наибольшее количество голосов, является окончательным прогнозом.
#  Когда мы загружаем тренировочный набор, мы пытаемся смыть дисперсию дерева решений.
#  Среднее значение нескольких деревьев с разными обучающими наборами создаст модель,
#  которая более точно улавливает суть данных.
#  Создание деревьев решений с баггингом — это способ уменьшить дисперсию модели.

# TODO: Decorrelate the Trees (Декорреляция деревьев)
#  Деревья решений с баггингом могут быть слишком похожими на деревья, чтобы полностью создать идеальную модель.
#  Они построены на разных ресемплах, но все они имеют доступ к одним и тем же характеристикам.
#  Таким образом, мы добавим некоторые ограничения в модель при построении каждого дерева решений,
#  чтобы деревья имели больше вариаций. Мы называем это декорреляцией деревьев.
#  Если вы помните, при построении дерева решений в каждом узле мы сравниваем
#  все пороги разделения для каждой характеристики, чтобы найти единственную лучшую характеристику и порог разделения.
#  В дереве решений для случайного леса в каждом узле мы случайным образом выбираем
#  подмножество признаков для рассмотрения.
#  Это приведет к тому, что мы выберем хорошую, но не лучшую характеристику для разделения на каждом этапе.
#  Важно отметить, что случайный выбор характеристик происходит в каждом узле.
#  Так что, может быть, в первом узле мы рассматриваем характеристики пола и платы за проезд,
#  а затем во втором узле признаки платы за проезд и возраста. Стандартный выбор количества признаков,
#  которые следует учитывать при каждом разбиении, — это квадратный корень из числа признаков.
#  Итак, если у нас есть 9 характеристик, мы будем рассматривать 3 из них в каждом узле (выбранные случайным образом).
#  Если мы упакуем эти деревья решений, мы получим случайный лес.
#  Каждое дерево решений в случайном лесу, вероятно, хуже, чем стандартное дерево решений.
#  Но когда мы их усредняем, мы получаем очень сильную модель!

# TODO: Review of Breast Cancer Dataset (Обзор набора данных по раку молочной железы)
#  В этом уроке мы будем использовать набор данных о раке молочной железы.
#  Напомним, что в наборе данных есть измерения различных атрибутов комка в ткани молочной железы и метка того,
#  является ли опухоль раковой. Вот код для извлечения данных из sklearn:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
# print('data dimensions', X.shape)
# TODO: Набор данных содержит 569 точек данных и 30 функций.

# TODO: Random Forest with Sklearn (Случайный лес со Sklearn)
#  Синтаксис для построения и использования модели случайного леса такой же,
#  как и для логистической регрессии и деревьев решений.
#  Разработчики scikit-learn специально сделали так,
#  чтобы можно было легко переключаться между разными моделями и сравнивать их.
#  Вот оператор импорта для импорта модели Random Forest для классификации.
#      from sklearn.ensemble import RandomForestClassifier
# TODO: Сначала мы разделим набор данных на обучающий набор и тестовый набор.
#      from sklearn.model_selection import train_test_split
#      X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)
# TODO: Мы добавили здесь параметр случайного состояния, чтобы он выполнял одно и то же разделение каждый раз,
#  когда мы запускаем код. Без случайного состояния мы ожидали бы разные точки данных в наборах для обучения
#  и тестирования каждый раз, когда мы запускаем код, что может затруднить тестирование кода.
#  Затем мы создаем объект RandomForestClassifier и используем метод подгонки для построения модели на обучающем наборе.
#      rf = RandomForestClassifier()
#      rf.fit(X_train, y_train)
# TODO: Теперь мы можем использовать модель, чтобы сделать прогноз.
#  Например, возьмем первую строку тестового набора и посмотрим, каков прогноз.
#  Напомним, что метод predict принимает массив точек, поэтому,
#  даже если у нас есть только одна точка, мы должны поместить ее в список.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)
#
# rf = RandomForestClassifier()
# rf.fit(X_train, y_train)
#
# first_row = X_test[0]
# print("prediction:", rf.predict([first_row]))
# print("true value:", y_test[0])
# TODO: Эти результаты означают, что модель предсказала, что опухоль была раковой, и это было правильно.
#  Мы можем использовать метод оценки для расчета точности по всему набору тестов.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)
#
# rf = RandomForestClassifier()
# rf.fit(X_train, y_train)
# # first_row = X_test[0]
#
# print("random forest accuracy:", rf.score(X_test, y_test))
# TODO: Таким образом, точность составляет 96.5%.
#  Мы можем видеть, как это соотносится с моделью дерева решений.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.model_selection import train_test_split
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)
#
# dt = DecisionTreeClassifier()
# dt.fit(X_train, y_train)
# print("decision tree accuracy:", dt.score(X_test, y_test))
# TODO: Таким образом, точность модели дерева решений составляет 90.2%, что намного хуже, чем для случайного леса.
#  Обратите внимание, насколько код scikit-learn похож на код для логистической регрессии и деревьев решений.
#  Это позволяет очень легко пробовать и сравнивать разные модели.

# TODO: Random Forest Parameters (Параметры случайного леса)
#  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
#  Когда вы посмотрите на документацию scikit-learn для RandomForestClassifier, вы увидите довольно много параметров,
#  которыми вы можете управлять. Мы рассмотрим некоторые из них, но вы можете узнать обо всех из них в документации.
#  Поскольку случайный лес состоит из деревьев решений,
#  у нас есть все те же параметры настройки для предварительной обрезки, что и для деревьев решений:
#      - max_depth
#      - min_samples_leaf
#      - max_leaf_nodes
#  В случайных лесах, как правило, нет необходимости их настраивать, так как переоснащение обычно не является проблемой.
#  Мы рассмотрим два новых параметра настройки:
#      - n_estimators (количество деревьев)
#      - max_features (максимум характеристик)
#  (количество характеристик, которые необходимо учитывать при каждом разделении).
#  По умолчанию для максимальных признаков используется квадратный корень из p,
#  где p — количество признаков (или предикторов).
#  Значение по умолчанию, как правило, является хорошим выбором для максимальных характеристик,
#  и нам обычно не нужно его менять, но вы можете установить его на фиксированное число с помощью следующего кода.
#      rf = RandomForestClassifier(max_features=5)
# TODO: Количество оценщиков (деревьев решений) по умолчанию равно 10.
#  Часто это работает хорошо, но в некоторых случаях может быть слишком мало.
#  Вы можете установить его на другой номер следующим образом.
#  В следующих частях мы увидим, как выбрать лучшее значение.
#      rf = RandomForestClassifier(n_estimators=15)
# TODO: Одним из больших преимуществ Random Forests является то, что они редко требуют серьезной настройки.
#  Значения по умолчанию хорошо подходят для большинства наборов данных.

# TODO: Grid Search (Поиск по сетке)
#  Если вы помните из модуля Decision Tree, в scikit-learn встроен класс Grid Search,
#  чтобы помочь нам найти оптимальный выбор параметров. Давайте воспользуемся поиском по сетке,
#  чтобы сравнить производительность случайного леса с разным количеством деревьев.
#  Напомним, что нам нужно определить сетку параметров, которые мы хотим изменить,
#  и дать список значений, которые нужно попробовать.
#      param_grid = {
#          'n_estimators': [10, 25, 50, 75, 100],
#      }
# TODO: Теперь мы можем создать классификатор случайного леса и поиск по сетке.
#  Напомним, что поиск по сетке выполнит для нас k-кратную перекрестную проверку.
#  Мы устанавливаем cv=5 для 5-кратной перекрестной проверки.
#      rf = RandomForestClassifier()
#      gs = GridSearchCV(rf, param_grid, cv=5)
# TODO: Теперь мы используем метод подгонки для запуска поиска по сетке.
#  Лучшие параметры будут сохранены в атрибуте best_params_.
#      gs.fit(X, y)
#      print("best params:", gs.best_params_)
#      # best params: {'n_estimators': 50}
# TODO: Это параметры, которые обеспечивают наивысшую точность, поскольку это метрика по умолчанию.
#  Обратите внимание, что вы можете получать немного разные результаты каждый раз, когда запускаете это,
#  поскольку случайное разделение на 5 сгибов может повлиять на то, какой из них имеет лучший показатель точности.
#  В этом случае точность нам подойдет, поскольку классы в наборе данных по раку молочной железы
#  достаточно сбалансированы. Если классы несбалансированы, мы хотели бы использовать альтернативную метрику,
#  такую как оценка f1. Мы можем изменить метрику, оценив параметр как «f1» следующим образом.
#  Чтобы каждый раз не выводить разные лучшие параметры, можно установить random_state в классификаторе.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# param_grid = {
#     'n_estimators': [10, 25, 50, 75, 100],
# }
#
# rf = RandomForestClassifier(random_state=123)
# gs = GridSearchCV(rf, param_grid, scoring='f1', cv=5)
# gs.fit(X, y)
# print("best params:", gs.best_params_)
# TODO: Вы можете добавить дополнительные параметры, например max_features,
#  и значения параметров в словарь param_grid, чтобы сравнить больше деревьев решений.

# TODO: Elbow Graph (График локтя)
#  С таким параметром, как количество деревьев в случайном лесу,
#  увеличение количества деревьев никогда не повредит производительности.
#  Увеличение числовых деревьев повысит производительность до точки, в которой она выровняется.
#  Однако чем больше деревьев, тем сложнее алгоритм. Более сложный алгоритм требует больше ресурсов для использования.
#  Как правило, стоит усложнить модель, если это улучшит производительность,
#  но мы не хотим усложнять ее без необходимости. Мы можем использовать то, что называется Elbow Graph,
#  чтобы найти золотую середину. Elbow Graph — это модель, оптимизирующая производительность без излишней сложности.
#  Чтобы найти оптимальное значение, давайте выполним поиск по сетке, пробуя все значения от 1 до 100 для n_estimators.
#      n_estimators = list(range(1, 101))
#      param_grid = {
#          'n_estimators': n_estimators,
#      }
#      rf = RandomForestClassifier()
#      gs = GridSearchCV(rf, param_grid, cv=5)
#      gs.fit(X, y)
# TODO: Вместо того, чтобы просто смотреть на лучшие параметры, как мы делали раньше,
#  мы собираемся использовать весь результат поиска по сетке. Значения находятся в атрибуте cv_results_.
#  Это словарь с большим количеством данных, однако нам понадобится только один из ключей: mean_test_score.
#  Давайте вытащим эти значения и сохраним их как переменную.
#      scores = gs.cv_results_['mean_test_score']
#      # [0.91564148, 0.90685413, ...]
# TODO: Теперь давайте воспользуемся matplotlib для построения графика результатов.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# import matplotlib.pyplot as plt
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# n_estimators = list(range(1, 101))
# param_grid = {
#     'n_estimators': n_estimators,
# }
# rf = RandomForestClassifier()
# gs = GridSearchCV(rf, param_grid, cv=5)
# gs.fit(X, y)
#
# scores = gs.cv_results_['mean_test_score']
# plt.plot(n_estimators, scores)
# plt.xlabel("n_estimators")
# plt.ylabel("accuracy")
# plt.xlim(0, 100)
# plt.ylim(0.9, 1)
# plt.show()
# TODO: См. Рис: ElbowGraphPicture_1.png
#  Если мы посмотрим на этот график, то увидим, что около 10 деревьев график выравнивается.
#  Наилучшая модель возникла при n_estimators=33 и n_estimators=64, но, учитывая ее изменчивость,
#  это, вероятно, было связано со случайностью. Мы должны выбрать около 10 в качестве нашего количества оценок,
#  потому что нам нужно минимальное количество оценок, которые по-прежнему дают максимальную производительность.
# TODO: См. Рис: ElbowGraphPicture_2.png
#  Теперь мы можем построить нашу модель случайного леса с оптимальным количеством деревьев:
#     rf = RandomForestClassifier(n_estimators=10)
#     rf.fit(X, y)
# TODO: Вы увидите, что графики изгибов появляются во множестве различных ситуаций, когда мы усложняем модель
#  и хотим определить минимальный уровень сложности, обеспечивающий оптимальную производительность.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# import matplotlib.pyplot as plt
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# n_estimators = list(range(1, 101))
# param_grid = {
#     'n_estimators': n_estimators,
# }
# rf = RandomForestClassifier(n_estimators=10)
# gs = GridSearchCV(rf, param_grid, cv=5)
# gs.fit(X, y)
#
# scores = gs.cv_results_['mean_test_score']
# plt.plot(n_estimators, scores)
# plt.xlabel("n_estimators")
# plt.ylabel("accuracy")
# plt.xlim(0, 100)
# plt.ylim(0.9, 1)
# plt.show()

# TODO: Feature Importances (Важность характеристик)
#  В наборе данных о раке есть 30 характеристик.
#  Каждая ли характеристика в равной степени способствует построению модели?
#  Если нет, то какое подмножество характеристик мы должны использовать? Это вопрос выбора характеристик.
#  Случайные леса обеспечивают простой метод выбора характеристик: среднее уменьшение примеси.
#  Напомним, что случайный лес состоит из множества деревьев решений,
#  и что для каждого дерева выбирается узел для разделения набора данных на основе максимального уменьшения примеси,
#  обычно либо примеси Джини, либо энтропии в классификации.
#  Таким образом, для дерева можно вычислить, насколько уменьшается примесь каждой характеристики в дереве.
#  И тогда для леса можно усреднить уменьшение примеси от каждой характеристики.
#  Считайте эту меру метрикой важности каждой характеристики, затем мы можем ранжировать
#  и выбирать характеристики в соответствии с важностью характеристик.
#  Scikit-learn предоставляет переменную feature_importances_ с моделью,
#  которая показывает относительную важность каждой характеристики.
#  Оценки уменьшаются, так что сумма всех оценок равна 1.
#  Давайте найдем важность характеристик в случайном лесу с n_estimator = 10,
#  используя набор обучающих данных, и отобразим их в порядке убывания.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)
# rf = RandomForestClassifier(n_estimators=10, random_state=111)
# rf.fit(X_train, y_train)
#
# ft_imp = pd.Series(rf.feature_importances_, index=cancer_data.feature_names).sort_values(ascending=False)
# print(ft_imp.head(10))
# TODO: Из вывода мы видим, что среди всех признаков наиболее важным является наихудший радиус (0.31),
#  за которым следуют средние вогнутые точки и наихудшие вогнутые точки.
#  В регрессии мы рассчитываем важность характеристики, используя вместо этого дисперсию.

# TODO: New Model on Selected Features (Новая модель для выбранных характеристик)
#  Почему мы должны выполнять отбор характеристик?
#  Основные причины: это позволяет нам быстрее обучать модель; это снижает сложность модели,
#  тем самым облегчая ее интерпретацию.
#  И если выбрано правильное подмножество, это может повысить точность модели.
#  Выбор правильного подмножества часто зависит от знания предметной области, некоторого искусства и немного удачи.
#  В нашем наборе данных мы случайно заметили, что характеристики с «наихудшим» значением,
#  по-видимому, имеют более высокое значение.
#  В результате мы собираемся построить новую модель с выбранными характеристики и посмотреть, улучшит ли она точность.
#  Вспомним модель из прошлой части.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)
#
# rf = RandomForestClassifier(n_estimators=10, random_state=111)
# rf.fit(X_train, y_train)
# print(rf.score(X_test, y_test))
# TODO: Сначала мы находим характеристики, в названиях которых есть слово «worst» (наихудший):
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# worst_cols = [col for col in df.columns if 'worst' in col]
# print(worst_cols)
# # print(*worst_cols, sep='\n')
# print(len(worst_cols))
# TODO: Таких функций десять. Теперь мы создаем еще один кадр данных с выбранными функциями,
#  а затем разделяем тестовый поезд с тем же случайным состоянием.
# X_worst = df[worst_cols]
# X_train, X_test, y_train, y_test = train_test_split(X_worst, y, random_state=101)
# TODO: Заполняем модель и выведем точность.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# rf = RandomForestClassifier(n_estimators=10, random_state=111)
#
# worst_cols = [col for col in df.columns if 'worst' in col]
# X_worst = df[worst_cols]
# X_train, X_test, y_train, y_test = train_test_split(X_worst, y, random_state=101)
# rf.fit(X_train, y_train)
# print(rf.score(X_test, y_test))
# TODO: Здесь мы можем повысить точность, используя подмножество признаков, треть от общего числа признаков,
#  если быть точным. Это связано с тем, что мы удалили некоторые шумы и сильно коррелированные характеристики,
#  что привело к повышению точности.
#  Преимущество построения лучшей модели с использованием меньшего количества характеристик будет
#  более заметным при большом размере выборки.
#  Не существует лучшего метода выбора характеристик, по крайней мере, универсального.
#  Вместо этого мы должны выяснить, что лучше всего подходит для конкретной проблемы,
#  и использовать опыт предметной области для построения хорошей модели.
#  Scikit-learn предоставляет простой способ узнать важные характеристики.

# TODO: Performance (Производительность)
#  Вероятно, самым большим преимуществом Random Forests является то,
#  что они обычно хорошо работают без какой-либо настройки.
#  Они также будут неплохо работать почти с каждым набором данных.
#  Например, линейная модель не может хорошо работать с набором данных, который нельзя разделить линией.
#  Невозможно разделить следующий набор данных линией, не манипулируя объектами.
#  Тем не менее, случайный лес будет прекрасно работать с этим набором данных.
#  Мы можем увидеть это, взглянув на код для создания поддельного набора данных выше
#  и сравнив модель логистической регрессии с моделью случайного леса.
#      См. Рис: PerformancePicture_1.png
#  Функция make_circles создает набор данных классификации с концентрическими кругами.
#  Мы используем перекрестную проверку kfold для сравнения оценок точности и видим,
#  что модель логистической регрессии работает хуже, чем случайное угадывание,
#  но модель случайного леса работает довольно хорошо.
#  Попробуй это сейчас:
# from sklearn.datasets import make_circles
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# from sklearn.ensemble import RandomForestClassifier
# import numpy as np
#
# X, y = make_circles(noise=0.2, factor=0.5, random_state=1)
#
# kf = KFold(n_splits=5, shuffle=True, random_state=1)
# lr_scores = []
# rf_scores = []
# for train_index, test_index in kf.split(X):
#     X_train, X_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]
#     lr = LogisticRegression(solver='lbfgs')
#     lr.fit(X_train, y_train)
#     lr_scores.append(lr.score(X_test, y_test))
#     rf = RandomForestClassifier(n_estimators=100)
#     rf.fit(X_train, y_train)
#     rf_scores.append(rf.score(X_test, y_test))
# print("LR accuracy:", np.mean(lr_scores))
# print("RF accuracy:", np.mean(rf_scores))
# TODO: Output:
#      LR accuracy: 0.36
#      RF accuracy: 0.8299999999999998
# TODO: При поиске эталона для новой задачи классификации принято начинать с построения модели логистической регрессии
#  и модели случайного леса, поскольку обе эти модели могут хорошо работать без какой-либо настройки.
#  Это даст вам значения для ваших показателей, чтобы попытаться превзойти.
#  Часто почти невозможно добиться большего успеха, чем эти тесты.

# TODO: Interpretability (Интерпретируемость)
#  Случайные леса, несмотря на то, что они состоят из деревьев решений, нелегко интерпретировать.
#  Случайный лес имеет несколько деревьев решений, каждое из которых не очень хорошая модель,
#  но при усреднении создают отличную модель.
#  Таким образом, случайные леса не являются хорошим выбором при поиске интерпретируемости.
#  В большинстве случаев интерпретируемость не важна.

# TODO: Computation (Вычисление)
#  Создание случайных лесов может быть немного медленным, особенно если у вас много деревьев в случайном лесу.
#  Построение случайного леса включает в себя построение 10-100 (обычно) деревьев решений.
#  Каждое из деревьев решений строится быстрее, чем стандартное дерево решений,
#  потому что мы не сравниваем каждую характеристику при каждом разделении, однако,
#  учитывая количество деревьев решений, его построение часто занимает много времени.
#  Точно так же прогнозирование с помощью случайного леса будет медленнее, чем дерево решений,
#  поскольку нам нужно сделать прогноз с каждым из 10-100 деревьев решений, чтобы получить окончательный прогноз.
#  Random Forests — не самая быстрая модель, но в целом это не проблема,
#  так как вычислительная мощность компьютеров велика.

# TODO: ЗАДАЧА: Machine Learning - A Forest of Trees (Машинное обучение — лес деревьев)
#  Создайте модель случайного леса. Задача Вам будет предоставлена матрица признаков X и целевой массив y.
#  Ваша задача состоит в том, чтобы разделить данные на обучающий и тестовый наборы,
#  построить модель случайного леса с обучающим набором и сделать прогнозы для тестового набора.
#  Дайте случайному лесу 5 деревьев.
#  Вам будет дано целое число, которое будет использоваться в качестве случайного состояния.
#  Обязательно используйте его как в тестовом разделении поезда, так и в модели случайного леса.
#  Формат ввода:
#  Первая строка: целое число (случайное состояние для использования)
#  Вторая строка: целое число (количество точек данных)
#  Следующие n строк: значения строки в матрице признаков, разделенные пробелами
#  Последняя строка: целевые значения, разделенные пробелами
#  Формат вывода Массив Numpy единиц и нулей
#  Sample Input:
#  1
#  10
#  -1.53 -2.86
#  -4.42 0.71
#  -1.55 1.04
#  -0.6 -2.01
#  -3.43 1.5
#  1.45 -1.15
#  -1.6 -1.52
#  0.79 0.55
#  1.37 -0.23
#  1.23 1.72
#  0 1 1 0 1 0 0 1 0 1
#  Sample Output:
#  [1 0 0]
#  Объяснение:
#  Разделение теста поезда помещает эти три точки в тестовый набор:
#  [-1.55 1.04], [1.23 1.72], [-1.6 -1.52].
#  Истинные значения для этих точек равны [1 1 0],
#  и модель правильно предсказывает [1 1 0].
# import numpy as np
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split
#
# random_state = int(input())
# n = int(input())
# rows = []
# for i in range(n):
#     rows.append([float(a) for a in input().split()])
#
# X = np.array(rows)
# y = np.array([int(a) for a in input().split()])
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state)
#
# rf = RandomForestClassifier(n_estimators=5, random_state=random_state)
# rf.fit(X_train, y_train)
#
# print(rf.predict(X_test))

# TODO: Neural Network Use Cases (Варианты использования нейронной сети)
#  Нейронные сети — невероятно популярные и мощные модели машинного обучения.
#  Они часто хорошо работают в тех случаях, когда у нас много характеристик,
#  поскольку они автоматически выполняют разработку характеристик,
#  не требуя знаний предметной области для реструктуризации характеристик.
#  В этом модуле мы будем использовать данные изображения.
#  Поскольку каждый пиксель изображения является характеристикой, у нас может быть очень большой набор характеристик.
#  Все они обычно используются в текстовых данных, поскольку они также имеют большой набор характеристик.
#  Распознавание голоса — еще один пример, в котором часто блестят нейронные сети.
#  Нейронные сети часто хорошо работают без необходимости использования знаний
#  предметной области для разработки каких-либо характеристик.

# TODO: Biological Neural Network (Биологическая нейронная сеть)
#  Более точным термином для нейронных сетей является искусственная нейронная сеть (ИНС).
#  Они были вдохновлены тем, как биологические нейронные сети работают в человеческом мозгу.
#  Нейронная сеть мозга состоит примерно из 86 миллиардов нейронов. Нейроны связаны так называемыми синапсами.
#  В человеческом мозгу около 100 триллионов синапсов. Нейроны посылают сигналы друг другу через синапсы.
#  На следующих уроках мы увидим, как определяется искусственная нейронная сеть.
