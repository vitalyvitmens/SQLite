# TODO: Machine Learning (Машинное обучение)
#  Узнайте, как создавать интеллектуальные системы рекомендаций,
#  которые помогают нам в повседневной жизни.
#  Добро пожаловать в машинное обучение.

# TODO: Welcome to Machine Learning (Добро пожаловать в машинное обучение)
#  Поздравляем! Вы сделали большой шаг к тому, чтобы стать практиком машинного обучения!
#  В дополнение к прохождению этого курса не забудьте воспользоваться всей поддержкой обучения,
#  доступной вам на SoloLearn, включая ежедневные советы, практики «Попробуйте сами», задачи тренера по коду,
#  игровую площадку для кода и участие в нашем замечательном сообществе учащихся. Мы рады услышать от вас,
#  поэтому, пожалуйста, оставляйте комментарии и отзывы, когда вы учитесь с нами.
#  Python — это язык программирования, который мы будем использовать на протяжении всего курса.
#  Давайте начнем!

# TODO: Machine Learning Overview (Обзор машинного обучения)
#  Добро пожаловать на курс машинного обучения! Машинное обучение — это способ сбора данных и превращения их в идеи.
#  Мы используем мощность компьютера для анализа примеров из прошлого, чтобы построить модель,
#  которая может предсказать результат для новых примеров. Мы сталкиваемся с моделями машинного обучения каждый день.
#  Например, когда Netflix рекомендует вам шоу, они использовали модель, основанную на том,
#  что вы и другие пользователи смотрели, чтобы предсказать, что вы хотели бы.
#  Когда Amazon выбирает цену для товара, они используют модель,
#  основанную на том, как подобные товары продавались в прошлом.
#  Когда компания, выпустившая вашу кредитную карту, звонит вам из-за подозрительной активности,
#  она использует модель, основанную на вашей прошлой активности, для распознавания аномального поведения.
#  В этом курсе мы изучим несколько методов решения задач машинного обучения.
#  Машинное обучение можно использовать для создания чат-бота, обнаружения спама или распознавания изображений.

# TODO: Course Basics (Основы курса)
#  Одним из наиболее распространенных языков, используемых профессионалами в области машинного обучения, является Python
#  Это очень доступно и очень мощно, поэтому мы будем использовать его в этом курсе.
#  Мы предполагаем рабочее знание Python. В этом курсе мы будем использовать несколько пакетов Python,
#  которые помогут решить задачи машинного обучения. Мы будем использовать pandas, numpy, matplotlib и scikit-learn.
#  Pandas используется для чтения данных и обработки данных, numpy используется для вычисления числовых данных,
#  matplotlib используется для построения графиков данных, а scikit-learn используется для моделей машинного обучения.
#  Каждый из этих пакетов довольно обширен, но мы рассмотрим функции, которые будем использовать.
#  Мы также рассмотрим некоторые основные статистические данные, поскольку они являются основой машинного обучения.
#  Курс будет охватывать как теорию, так и практику методов машинного обучения, но сосредоточится на том,
#  как их использовать на реальных примерах.

# TODO: What's in this Course? (Что в этом курсе?)
#  В машинном обучении мы говорим о контролируемом и неконтролируемом обучении.
#  Обучение с учителем — это когда у нас есть известная цель на основе прошлых данных
#  (например, прогнозирование цены, по которой будет продаваться дом),
#  а обучение без учителя — это когда нет известного прошлого ответа
#  (например, определение тем, обсуждаемых в обзорах ресторанов).
#  В этом курсе мы сосредоточимся на контролируемом обучении.
#  В контролируемом обучении существуют проблемы классификации и регрессии.
#  Регрессия прогнозирует числовое значение (например, прогнозирует, по какой цене будет продаваться дом)
#  и классифицирует предсказывает, к какому классу что-либо принадлежит
#  (например, предсказывает, не выполнит ли заемщик дефолт по своему кредиту).
#  Мы сосредоточимся на проблемах классификации.
#  Это задачи, в которых мы предсказываем, к какому классу что-то принадлежит.
#  Наши примеры будут включать:
#  • Прогнозирование того, кто выживет при крушении «Титаника»
#  • Определение рукописной цифры по изображению
#  • Использование данных биопсии для определения злокачественности новообразования
#  Мы будем использовать ряд популярных методов для решения этих проблем.
#  Мы рассмотрим каждый из них более подробно в следующих модулях:
#  • Логистическая регрессия
#  • Деревья решений
#  • Случайные леса
#  • Нейронные сети
#  В конце этого курса вы сможете взять классификационный набор данных
#  и использовать Python для создания нескольких различных моделей,
#  чтобы определить лучшую модель для данной проблемы.
#  Машинное обучение можно использовать для решения широкого круга задач.
#  Этот курс будет посвящен контролируемому обучению и классификации.

# TODO: Averages (Средние значения)
#  При работе с данными нам часто нужно вычислить некоторые простые статистические данные.
#  Допустим, у нас есть список возрастов людей в классе.
#  Мы располагаем их в порядке возрастания, так будет проще производить расчеты.
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: Среднее значение является наиболее известным средним значением.
#  Сложите все значения и разделите на количество значений:
#      (15 + 16 + 18 + 19 + 22 + 24 + 29 + 30 + 34) / 9 =  207/9 = 23
# TODO: Медиана — это значение посередине упорядоченных чисел.
#  В этом случае, поскольку имеется 9 значений, среднее значение 5-тое, то есть 22.
#  В статистике как среднее, так и медиана называются средними.
#  Среднее значение для непрофессионала является средним.

# TODO: Percentiles (процентили)
#  Медиану также можно рассматривать как 50-й процентиль.
#  Это означает, что 50% данных меньше медианы, а 50% данных больше медианы.
#  Это говорит нам, где находится середина данных, но нам часто требуется больше понимания распределения данных.
#  Мы часто будем рассматривать 25 -й процентиль и 75 -й процентиль.
#  25 - й процентиль — это значение, которое составляет одну четверть пути через данные.
#  Это значение, при котором 25% данных меньше его (а 75% данных больше его).
#  Точно так же 75 -й процентиль составляет три четверти пути через данные.
#  Это значение, при котором 75% данных меньше его (а 25% данных больше его).
#  Если мы снова посмотрим на наш возраст:
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: У нас есть 9 значений, поэтому 25% данных будут составлять примерно 2 точки данных.
#  Таким образом, третья точка данных превышает 25% данных. Таким образом, 25-й процентиль равен 18 (3-я точка данных).
#  Точно так же 75% данных составляют примерно 6 точек данных. Таким образом, 7-я точка данных превышает 75% данных.
#  Таким образом, 75-й процентиль равен 29 (7-я точка данных).
#  Полный диапазон наших данных находится в диапазоне от 15 до 34.
#  25-й и 75-й процентили говорят нам, что половина наших данных находится в диапазоне от 18 до 29.
#  Это помогает нам понять, как распределяются данные.
#  Если имеется четное количество точек данных, чтобы найти медиану (или 50-й процентиль),
#  вы берете среднее значение двух значений в середине.

# TODO: Standard Deviation & Variance (Стандартное отклонение и дисперсия)
#  Мы можем получить более глубокое понимание распределения наших данных со стандартным отклонением и дисперсией.
#  Стандартное отклонение и дисперсия — это меры того, насколько разбросаны или рассредоточены данные.
#  Мы измеряем, насколько далеко каждая точка данных от среднего.
#  Давайте еще раз посмотрим на нашу возрастную группу:
#      15, 16, 18, 19, 22, 24, 29, 30, 34
# TODO: Напомним, что среднее равно 23. Давайте подсчитаем, насколько далеко каждое значение от среднего.
#  15 на 8 отличается от среднего (поскольку 23-15=8).
#  Вот список всех этих расстояний:
#      8, 7, 5, 4, 1, 1, 6, 7, 11
# TODO: Мы возводим эти значения в квадрат и складываем их вместе.
#  См. Рис: StandardDeviationPicture1.png
#  Мы делим это значение на общее количество значений, и это дает нам дисперсию.
#      362 / 9 = 40.22
# TODO: Чтобы получить стандартное отклонение, мы просто возьмем квадратный корень из этого числа и получим: 6.34
#  Если наши данные нормально распределены, как показано на графике ниже (См. Рис: StandardDeviationPicture2.png),
#  68% населения находится в пределах одного стандартного отклонения от среднего.
#  На графике мы выделили область в пределах одного стандартного отклонения от среднего значения.
#  Вы можете видеть, что заштрихованная область составляет около двух третей (точнее 68%) от общей площади под кривой.
#  Если мы предположим, что наши данные нормально распределены, мы можем сказать,
#  что 68% данных находятся в пределах 1 стандартного отклонения от среднего.
# TODO: В нашем примере с возрастом, хотя возрасты, вероятно, не совсем нормально распределены,
#  мы предполагаем, что это так, и говорим, что примерно 68% населения имеет возраст
#  в пределах одного стандартного отклонения от среднего. Поскольку среднее значение равно 23 годам,
#  а стандартное отклонение равно 6,34, мы можем сказать, что приблизительно 68% возрастов нашей популяции находятся
#  в диапазоне от 16,66 (23 минус 6,34) до 29,34 (23 плюс 6,34).
#  Несмотря на то, что данные никогда не бывают идеальным нормальным распределением,
#  мы все же можем использовать стандартное отклонение, чтобы получить представление о том, как распределяются данные.

# TODO: Statistics with Python (Статистика с Python)
#  Мы можем рассчитать все эти операции с помощью Python. Мы будем использовать пакет Python numpy.
#  Мы будем использовать numpy позже для работы с массивами, а сейчас мы просто будем использовать несколько функций
#  для статистических вычислений: mean, median, centile, std, var.
#  Сначала мы импортируем пакет. Стандартной практикой является псевдоним numpy как np.
#      import numpy as np
# TODO: Давайте инициализируем переменную data, чтобы иметь список возрастов.
#      data = [15, 16, 18, 19, 22, 24, 29, 30, 34]
# TODO: Теперь мы можем использовать функции numpy.
#  Для функций среднего, медианы, стандартного отклонения и дисперсии мы просто передаем список данных.
#  Для функции процентиля мы передаем список данных и процентиль (в виде числа от 0 до 100).
# import numpy as np
#
# data = [15, 16, 18, 19, 22, 24, 29, 30, 34]
#
# print("mean:", np.mean(data))
# print("median:", np.median(data))
# print("50th percentile (median):", np.percentile(data, 50))
# print("25th percentile:", np.percentile(data, 25))
# print("75th percentile:", np.percentile(data, 75))
# print("standard deviation:", np.std(data).round(2))
# print("variance:", np.var(data).round(2))
# TODO: Numpy — это библиотека Python, которая позволяет быстро и легко выполнять математические операции с массивами.

# TODO: What is Pandas? (Что такое Панды?)
#  Этот курс написан на Python, одном из наиболее часто используемых языков для машинного обучения.
#  Одна из причин, по которой он так популярен, заключается в том,
#  что существует множество полезных модулей Python для работы с данными.
#  Первый, который мы представим, называется Pandas.
#  Pandas — это модуль Python, который помогает нам читать данные и управлять ими.
#  Что хорошо в pandas, так это то, что вы можете брать данные и просматривать их в виде таблицы,
#  удобочитаемой для человека, но их также можно интерпретировать в числовом виде,
#  чтобы вы могли выполнять с ними множество вычислений. Мы называем таблицу данных DataFrame .
#  Python удовлетворит все наши потребности в машинном обучении.
#  Мы будем использовать модуль Pandas для обработки данных.

# TODO: Read in Your Data (Читать в ваших данных)
#  Нам нужно начать с импорта Pandas. Стандартной практикой является прозвище pd, чтобы потом быстрее печатать.
#      import pandas as pd
# TODO: Мы будем работать с набором данных пассажиров Титаника.
#  Для каждого пассажира у нас будут данные о нем, а также о том, выжили ли они в кораблекрушении.
#  Наши данные хранятся в виде файла CSV (значения, разделенные запятыми). Файл titanic.csv находится ниже.
#  Первая строка — это заголовок, а затем каждая последующая строка — это данные для одного пассажира.
#      Survived, Pclass, Sex, Age, Siblings/Spouses, Parents/Children, Fare
#      Выжившие, ПКласс, Пол, Возраст, Братья и сестры/супруги, Родители/Дети, Плата за проезд
#      0, 3, male, 22.0, 1, 0, 7.25
#      1, 1, female, 38.0, 1, 0, 71.2833
#      1, 3, female, 26.0, 0, 0, 7.925
#      1, 1, female, 35.0, 1, 0, 53.1
# TODO: Мы собираемся загружать данные в pandas, чтобы мы могли просматривать их как DataFrame.
#  Функция read_csv берет файл в формате csv и преобразует его в Pandas DataFrame.
#      df = pd.read_csv('titanic.csv')
# TODO: Объект df теперь является нашим фреймом данных pandas с набором данных Titanic.
#  Теперь мы можем использовать метод head для просмотра данных.
#  Метод head возвращает первые 5 строк DataFrame.
#      print(df.head())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head())
# TODO: Обычно данные хранятся в файлах CSV (значения, разделенные запятыми),
#  которые мы можем легко прочитать с помощью функции panda read_csv.
#  Метод head возвращает первые 5 строк.

# TODO: Summarize the Data (Суммарные данные)
#  Обычно наши данные слишком велики, чтобы мы могли отобразить их все.
#  Рассмотрение первых нескольких строк — это первый шаг к пониманию наших данных,
#  но затем мы хотим взглянуть на некоторую сводную статистику.
#  В пандах мы можем использовать метод описания describe().
#  Он возвращает таблицу статистики о столбцах.
#      print(df.describe())
# TODO: Мы добавляем строку в приведенный ниже код, чтобы заставить Python отображать все 6 столбцов.
#  Без линии это будет сокращать результаты.
# import pandas as pd
#
# pd.options.display.max_columns = 6
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.describe())
# TODO: Для каждого столбца мы видим несколько статистических данных.
#  Обратите внимание, что он дает статистику только для числовых столбцов.
#  Давайте рассмотрим, что означает каждая из этих статистических данных:
#  Count - это количество строк, которые имеют значение.
#  В нашем случае у каждого пассажира есть значение для каждого из столбцов,
#  поэтому значение равно 887 (общее количество пассажиров).
#  Среднее значение: Напомним, что среднее значение является стандартным средним значением.
#  Std : это сокращение от стандартного отклонения. Это мера того, насколько разбросаны данные.
#  Min : наименьшее значение
#  25% : 25-й процентиль
#  50% : 50-й процентиль, также известный как медиана.
#  75% : 75-й процентиль
#  Макс : наибольшее значение
#  Мы используем метод описания Pandas, чтобы начать интуитивно понимать наши данные.

# TODO: Selecting a Single Column (Выбор одного столбца)
#  Часто нам нужно иметь дело только с некоторыми столбцами, которые есть в нашем наборе данных.
#  Чтобы выбрать один столбец, мы используем квадратные скобки и имя столбца.
#  В этом примере мы выбираем только столбец с пассажирскими тарифами.
# col = df['Fare']
# print(col)
# TODO: Результатом является то, что мы называем Pandas Series.
#  Серия похожа на DataFrame, но это всего лишь один столбец.
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# col = df['Fare']
# print(col)
# TODO: Серия Pandas — это один столбец из Pandas DataFrame.

# TODO: Selecting Multiple Columns (Выбор нескольких столбцов)
#  Мы также можем выбрать несколько столбцов из нашего исходного DataFrame, создав меньший DataFrame.
#  Мы собираемся выбрать только столбцы Age, Sex и Survived из нашего исходного DataFrame.
#  Мы помещаем эти значения в список следующим образом:
#      ['Age', 'Sex', 'Survived']
# TODO: Теперь мы используем этот список внутри скобочной нотации df[...]
#  При печати большого DataFrame, который слишком велик для отображения,
#  вы можете использовать метод head для печати только первых 5 строк.
#     small_df = df[['Age', 'Sex', 'Survived']]
#     print(small_df.head())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# small_df = df[['Age', 'Sex', 'Survived']]
# print(small_df.head())
# TODO: При выборе одного столбца из Pandas DataFrame мы используем одинарные квадратные скобки.
#  При выборе нескольких столбцов мы используем двойные квадратные скобки.

# TODO: Creating a Column (Создание столбца)
#  Мы часто хотим, чтобы наши данные были в несколько ином формате, чем они были изначально.
#  Например, наши данные имеют пол пассажира в виде строки («мужской» или «женский»).
#  Это легко прочитать человеку, но когда мы позже будем выполнять вычисления с нашими данными,
#  нам понадобятся логические значения (истина и ложь).
#  Мы можем легко создать новый столбец в нашем DataFrame, который имеет значение True,
#  если пассажир — мужчина, и False, если он — женщина.
#  Вспомните синтаксис выбора столбца «Пол»:
#      df['Sex']
# TODO: Мы создаем серию панд, которая будет серией истин и ложностей
#  (истина, если пассажир — мужчина, и ложь, если пассажир — женщина ).
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df['Sex'] == 'male')
# TODO: Теперь мы хотим создать столбец с этим результатом.
#  Чтобы создать новый столбец, мы используем тот же синтаксис квадратных скобок (df['male']),
#  а затем присваиваем ему это новое значение.
#      df['male'] = df['Sex'] == 'male'
# import pandas as pd
#
# pd.options.display.max_columns = 8
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
#
# df['male'] = df['Sex'] == 'male'
# print(df.head())
# # print(df[['Age', 'male', 'Survived']].tail())
# TODO: Наш фрейм данных теперь выглядит следующим образом.
#  Обратите внимание на новый столбец в конце. Часто наши данные не в идеальном формате.
#  К счастью, Pandas позволяет нам легко создавать новые столбцы на основе наших данных,
#  чтобы мы могли их соответствующим образом отформатировать.

# TODO: What is Numpy? (Что такое Нампи?)
#  Numpy — это пакет Python для управления списками и таблицами числовых данных.
#  Мы можем использовать его для выполнения большого количества статистических вычислений.
#  Мы называем список или таблицу данных массивом numpy.
#  Мы часто берем данные из нашего панда DataFrame и помещаем их в массивы numpy.
#  Pandas DataFrames великолепны, потому что у нас есть имена столбцов
#  и другие текстовые данные, которые делают их удобочитаемыми.
#  DataFrame, хотя и легко читается человеком, не является идеальным форматом для выполнения вычислений.
#  Массивы numpy, как правило, менее удобочитаемы для человека,
#  но имеют формат, позволяющий выполнять необходимые вычисления.
#  Numpy — это модуль Python для выполнения вычислений в таблицах данных.
#  На самом деле Pandas был построен с использованием Numpy в качестве основы.

# TODO: Converting from a Pandas Series to a Numpy Array (Преобразование из серии Pandas в массив Numpy)
#  Мы часто начинаем с наших данных в Pandas DataFrame, но затем хотим преобразовать их в массив numpy.
#  Атрибут values делает это за нас. Давайте преобразуем столбец Fare в пустой массив.
#  Сначала мы вспомним, что мы можем использовать нотацию с одной скобкой,
#  чтобы получить серию pandas в столбце Fare следующим образом.
#      df['Fare']
# TODO: Затем мы используем атрибут values, чтобы получить значения в виде массива numpy.
#      df['Fare'].values
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df['Fare'].values)
# print(df['Fare'].head().values)
# TODO: Вот как выглядит приведенный выше массив:
#      array([ 7.25 , 71.2833,  7.925, 53.1, 8.05, 8.4583, …
# TODO: В результате получается одномерный массив.
#  Вы можете сказать это, так как есть только один набор скобок,
#  и он расширяется только по странице (а не вниз).
#  Атрибут значений серии Pandas предоставляет данные в виде массива numpy.

# TODO: Converting from a Pandas DataFrame to a Numpy Array (Преобразование из Pandas DataFrame в массив Numpy)
#  Если у нас есть DataFrame pandas (вместо Series, как в прошлой части),
#  мы все еще можем использовать атрибут values, но он возвращает двумерный массив numpy.
#  Напомним, что мы можем создать меньший DataFrame pandas со следующим синтаксисом.
#      df[['Pclass', 'Fare', 'Age']]
# TODO: Опять же, мы применяем атрибут values, чтобы получить пустой массив.
#      df[['Pclass', 'Fare', 'Age']].values
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df[['Pclass', 'Fare', 'Age']].values)
# TODO: Вот как выглядит приведенный выше массив:
#      array([[ 3.    ,  7.25  , 22.    ],
#             [ 1.    , 71.2833, 38.    ],
#             [ 3.    ,  7.925 , 26.    ],
#                          ...           ,
#             [ 3.    , 23.45  ,  7.    ],
#             [ 1.    , 30.    , 26.    ],
#             [ 3.    ,  7.75  , 32.    ]])
# TODO: Это двумерный массив numpy. Вы можете сказать, потому что есть два набора скобок,
#  и они расширяются как по странице, так и вниз.
#  Атрибут значений Pandas DataFrame предоставляет данные в виде массива 2d numpy.

# TODO: Numpy Shape Attribute (Атрибут Numpy Форма)
#  Мы используем атрибут numpy форма, чтобы определить размер нашего массива numpy.
#  Размер говорит нам, сколько строк и столбцов в наших данных.
#  Во-первых, давайте создадим массив numpy с Pclass, Fare и Age.
#      arr = df[['Pclass', 'Fare', 'Age']].values
# TODO: Если мы посмотрим на форму, мы получим количество строк и количество столбцов:
#      print(arr.shape) #(887, 3)
# import pandas as pd
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr.shape)
# print(df.shape)
# TODO: Этот результат означает, что у нас есть 887 строк и 3 столбца.
#  Используйте атрибут shape, чтобы найти количество строк и количество столбцов для массива Numpy.
#  Вы также можете использовать атрибут формы в DataFrame pandas (df.shape).

# TODO: Select from a Numpy Array (Выберите из массива Numpy)
#  Предположим, мы создали следующий массив numpy:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# # arr = df[['Pclass', 'Fare', 'Age']].tail(10).values
# print(arr)
# TODO: Мы можем выбрать один элемент из массива numpy следующим образом:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0, 1])
# TODO: Это будет 2-й столбец 1-й строки (помните, что мы начинаем считать с 0 ).
#  Таким образом, это будет тариф 1-го пассажира или 7,25.
#  Мы также можем выбрать одну строку, например, весь ряд первого пассажира:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0])
# # print(arr[0, :])
# TODO: Чтобы выбрать один столбец (в данном случае столбец «Возраст»), мы должны использовать специальный синтаксис:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[:, 2])
# TODO:
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age']].values
# print(arr[0, 1])
# print(arr[0])
# print(arr[:, 2])
# TODO: Синтаксис можно интерпретировать так, что мы берем все строки, но только столбец с индексом 2.
#  Используя другой синтаксис в скобках, мы можем выбрать отдельные значения, всю строку или весь столбец.

# TODO: Masking (Маскировка)
#  Часто требуется выбрать все строки, соответствующие определенному критерию.
#  В этом примере мы выберем все строки детей (пассажиры младше 18 лет).
#  Напоминание о нашем определении массива:
#      arr = df[['Pclass', 'Fare', 'Age']].values
# TODO: Напомним, что мы можем получить столбец Age с помощью следующего синтаксиса:
#      arr[:, 2]
# TODO: Сначала мы создаем то, что мы называем маской.
#  Это массив логических значений (True/False), указывающих, является ли пассажир ребенком или нет.
#      mask = arr[:, 2] < 18
# TODO: Давайте посмотрим на массив маски, чтобы убедиться, что мы его понимаем.
#      array([False, False, False, False, False, False, False, True, False, …
# TODO: Значения False означают взрослого, а значения True — ребенка,
#  поэтому первые 7 пассажиров — взрослые, затем 8-й — ребенок, а 9-й — взрослый.
#  Теперь мы используем нашу маску, чтобы выбрать только нужные нам строки:
#      arr[mask]
# TODO: Давайте посмотрим на этот новый массив.
#      array([[3., 21.075, 2.],
#             [2., 30.0708, 14.],
#             [3., 16.7, 4.],
#             [3., 7.8542, 14.],
# TODO: Если мы вспомним, что третий столбец — это возраст пассажиров,
#  мы увидим, что все строки здесь — для пассажиров-детей.
#  Как правило, нам не нужно определять переменную маски, и мы можем сделать это всего в одной строке:
#      arr[arr[:, 2] < 18]
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# # take first 10 values for simplicity
# arr = df[['Pclass', 'Fare', 'Age']].values
#
# mask = arr[:, 2] < 18
# print(arr[mask])
# print(arr[arr[:, 2] < 18])
# TODO: Маска — это логический массив (значения True/False),
#  который сообщает нам, какие значения из массива нас интересуют.

# TODO: Summing and Counting (Суммирование и подсчет)
#  Допустим, мы хотим знать, сколько среди наших пассажиров детей.
#  У нас все еще есть то же определение массива,
#  и мы можем взять нашу маску или логические значения из предыдущей части.
#      arr = df[['Pclass', 'Fare', 'Age']].values
#      mask = arr[:, 2] < 18
# TODO: Напомним, что значения True интерпретируются как 1, а значения False интерпретируются как 0.
#  Таким образом, мы можем просто суммировать массив, и это эквивалентно подсчету количества истинных значений.
#      print(mask.sum())
# TODO: Опять же, мы можем не определять переменную маски.
#      print((arr[:, 2] < 18).sum())
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# arr = df[['Pclass', 'Fare', 'Age', 'Survived', 'Sex']].values
# mask = arr[:, 2] < 18
#
# print(mask.sum())
# print((arr[:, 2] < 18).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1)).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] == 'male')).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] == 'female')).sum())
# print(((arr[:, 2] < 18) & (arr[:, 3] == 1) & (arr[:, 4] != 'male')).sum())
# TODO: Суммирование массива логических значений дает количество значений True.

# TODO: Scatter Plot (Точечная диаграмма)
#  Мы можем использовать библиотеку matplotlib для построения графика наших данных.
#  Нанесение данных на график часто может помочь нам создать интуицию в отношении наших данных.
#  Сначала нам нужно импортировать matplotlib. Это стандартная практика называть его plt.
#      import matplotlib.pyplot as plt
# TODO: Мы используем функцию разброса для построения наших данных.
#  Первым аргументом функции рассеяния является ось x (горизонтальное направление),
#  а вторым аргументом — ось y (вертикальное направление).
#      plt.scatter(df['Age'], df['Fare'])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'])
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Это отображает возраст по оси x и стоимость проезда по оси y.
#  Чтобы упростить интерпретацию, мы можем добавить метки x и y.
#      plt.xlabel('Age')
#      plt.ylabel('Fare')
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Мы также можем использовать наши данные для цветового кодирования нашей диаграммы рассеивания.
#  Это даст каждому из 3 классов другой цвет. Мы добавляем параметр c и присваиваем ему серию Pandas.
#  В этом случае наш ряд Pandas имеет 3 возможных значения (1-й, 2-й и 3-й класс),
#  поэтому мы увидим, что наши точки данных получают один из трех цветов.
#      plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# # plt.scatter(df['Age'], df['Fare'], c=[df['Sex'] == 'male'])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Фиолетовые точки относятся к 1-первому классу,
#  зеленые точки ко 2-второму классу,
#  желтые точки к 3-третьему классу.
#  Точечная диаграмма используется для отображения всех значений ваших данных на графике.
#  Чтобы получить визуальное представление наших данных, мы должны ограничить наши данные двумя функциями.

# TODO: Line (Линия)
#  Теперь, когда мы можем поместить отдельные точки данных на график, давайте посмотрим, как нарисовать линию.
#  Функция plot делает именно это. Далее проводится линия, примерно отделяющая 1-й класс от 2-го и 3-го классов.
#  На глазок проведем линию от (0, 85) до (80, 5). Наш синтаксис ниже имеет список значений x и список значений y:
#      plt.plot([0, 80], [85, 5])
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
# plt.plot([0, 80], [85, 5])
# plt.xlabel('Age')
# plt.ylabel('Fare')
#
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Вы можете видеть, что желтые (3-й класс) и зеленые (2-й класс) точки в основном находятся ниже линии,
#  а фиолетовые (1-й класс) — в основном выше.
#  Мы сделали это вручную, но в следующем модуле мы научимся делать это алгоритмически.
#  В matplotlib мы используем функцию scatter (рассеяния) для создания графика scatter
#  и функцию plot (график) для построения линии.

# TODO: ЗАДАЧА: Machine Learning - What's in a Column? (Машинное обучение — что в столбце?)
#  Получение столбца из массива numpy.
#  Задача Учитывая CSV-файл и имя столбца, распечатать элементы в данном столбце.
#  Формат ввода:
#  Первая строка: имя файла csv
#  Вторая строка: имя столбца
#  Формат вывода Массив Numpy
#  Sample Input:
#  https://sololearn.com/uploads/files/one.csv
#  a
#  File one.csv contents:
#  a,b
#  1,3
#  2,4
#  Sample Output:
#  [1 2]
#  Пояснение: a — это заголовок для первого столбца со значениями [1 2].
# import pandas as pd
#
# filename = input()
# column_name = input()
#
# df = pd.read_csv(filename)
# print(df[column_name].values)

# TODO: Where does Classification Fit in the World of Machine Learning?
#  (Какое место классификация Fit (занимает) в мире машинного обучения?)
#  Машинное обучение на высоком уровне состоит из контролируемого и неконтролируемого обучения.
#  Контролируемое обучение означает, что у нас будут помеченные исторические данные,
#  которые мы будем использовать для информирования нашей модели.
#  Мы называем ярлык или вещь, которую пытаемся предсказать - target (целью).
#  Таким образом, в контролируемом обучении есть известная цель для исторических данных,
#  а в неконтролируемом обучении нет известной цели.
#  В контролируемом обучении есть классификация и регрессия.
#  Проблемы классификации возникают, когда целью является категориальное значение
#  (часто True или False, но может быть несколько категорий).
#  Проблемы регрессии — это когда целью является числовое значение.
#  Например, прогнозирование цен на жилье — это проблема регрессии.
#  Это контролируется, так как у нас есть исторические данные о продажах домов в прошлом.
#  Это регрессия, потому что цена жилья является числовым значением.
#  Предсказание того, не выполнит ли кто-то дефолт по своему кредиту, является проблемой классификации.
#  Опять же, он находится под наблюдением, поскольку у нас есть исторические данные о дефолте прошлых кредиторов,
#  и это проблема классификации, потому что мы пытаемся предсказать,
#  относится ли кредит к одной из двух категорий (дефолт или нет).
#  Логистическая регрессия, хотя в ее названии есть регрессия,
#  представляет собой алгоритм решения задач классификации, а не проблем регрессии.

# TODO: Classification Terminology (Терминология классификации)
#  Давайте вернемся к нашему набору данных Титаника.
#  Вот снова Pandas DataFrame данные:
# import pandas as pd
#
# pd.options.display.max_columns = 7
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head(5))
# TODO:
#  столбец Survived — это то, что мы пытаемся предсказать. Мы называем это целью.
#  Вы можете видеть, что это список из 1 и 0.
#  1 означает, что пассажир выжил, а 0 означает, что пассажир не выжил.
#  Остальные столбцы — это информация о пассажире, которую мы можем использовать для прогнозирования цели.
#  Мы называем каждый из этих столбцов feature (характеристика).
#  Характеристики — это данные, которые мы используем для прогнозирования.
#  Хотя мы знаем, выжил ли каждый пассажир в наборе данных,
#  мы хотели бы иметь возможность делать прогнозы о дополнительных пассажирах,
#  для которых мы не смогли собрать эти данные.
#  Мы построим модель машинного обучения чтобы помочь нам сделать это.
#  Иногда вы будете слышать характеристики, называемые предикторами.

# TODO: Classification Graphically (Классификация графически)
#  В конечном итоге мы захотим использовать все характеристики, но для простоты давайте начнем только
#  с двух характеристик Fare and Age (плата за проезд и возраст).
#  Использование двух характеристик позволяет нам визуализировать данные на графике.
#  По оси абсцисс (X) отложена стоимость проезда пассажира, а по оси ординат (Y) — его возраст.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([32, 100], [0, 80])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Желтые точки — пассажиры, которые выжили, а фиолетовые точки — пассажиры, которые не выжили.
#  Вы можете видеть, что желтых точек внизу графика больше, чем вверху.
#  Это потому, что у детей было больше шансов выжить, чем у взрослых, что соответствует нашей интуиции.
#  Точно так же справа на графике больше желтых точек, что означает,
#  что люди, которые платили больше, имели больше шансов выжить.
# TODO: Задача линейной модели состоит в том, чтобы найти линию, которая наилучшим образом разделяет два класса так,
#  чтобы желтые точки находились с одной стороны, а фиолетовые — с другой. Вот пример хорошей линии:
#      plt.plot([32, 100], [0, 80])
#  Линия используется для прогнозирования появления новых пассажиров.
#  Если точка данных пассажира находится на правой стороне линии, мы прогнозируем, что он выживет.
#  Если на левой стороне, мы бы предсказали, что они не выжили.
#  Задача построения модели будет заключаться в том, чтобы определить наилучшую возможную линию.

# TODO: Equation for the Line (Уравнение для линии)
#  Линия определяется уравнением в следующем виде:
#      0 = ax + by + c
# TODO: Значения a, b и c являются коэффициентами. Любые три значения будут определять уникальную строку.
#  Давайте рассмотрим конкретный пример строки, где коэффициенты равны a=1, b=-1 и c=-30.
#      0 = (1)x + (-1)y + (-30)
# TODO: Вот три коэффициента: 1, -1, -30.
#  Вспомним, что мы наносили наши данные по оси x на тариф и по оси y на возраст пассажира.
#  Чтобы нарисовать прямую из уравнения, нам нужны две точки, лежащие на прямой.
#  Мы можем видеть, например, что точка (30, 0) лежит прямо на линии (Fare 30, Age 0).
#  Если мы подставим это в уравнение, оно сработает.
#      30 - 0 - 30 = 0
# TODO: Мы также можем видеть, что точка (50, 20) находится на линии (Fare 50, Age 20).
#      50 - 20 - 30 = 0
# TODO: Вот как наша линия выглядит на графике.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([30, 50], [0, 20])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Коэффициенты линии определяют, где находится линия.

# TODO: Making a Prediction Based on the Line (Делаем предсказание на основе линии)
#  Давайте снова посмотрим на ту же строку.
#      0 = (1)x + (-1)y - 30
# TODO: Если мы возьмем данные о пассажирах, мы можем использовать это уравнение,
#  чтобы определить, на какую сторону линии они попадают.
#  Например, предположим, что у нас есть пассажир, тариф которого равен 100, а возраст 20.
#  Подставим эти значения в наше уравнение:
#      (1)100 + (-1)20 - 30 = 100 - 20 - 30 = 50
# TODO: Поскольку это значение положительное, точка находится на правой стороне линии,
#  и мы предполагаем, что пассажир выжил.
#  Теперь предположим, что тариф для пассажира равен 10, а его возраст равен 50 годам.
#  Подставим эти значения в уравнение.
#      (1)10 + (-1)50 - 30 = -70
# TODO: Поскольку это значение отрицательное, точка находится на левой стороне линии,
#  и мы предполагаем, что пассажир не выжил. Мы можем видеть эти две зеленые точки на графике ниже.
#  С какой стороны линии находится точка, зависит, по нашему мнению, выживет этот пассажир или нет.
# import pandas as pd
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# plt.plot([32, 100], [0, 80])
# plt.scatter([100, 10], [20, 50], c='g')
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()

# TODO: What Makes a Good Line? (Что делает линию хорошей?)
#  Давайте посмотрим на две разные линии. Сначала у нас есть линия, с которой мы работали до сих пор.
#  Назовем эту линию 1.
#      0 = (1)x + (-1)y - 30
# TODO: Далее у нас есть еще одно уравнение для линии.
#  Назовем эту линию 2.
#      0 = (4)x + (5)y - 400
# TODO: Если мы посмотрим на две линии, то увидим, что в строке 1 справа больше желтых точек, а слева больше фиолетовых.
#  У линии 2 не так много точек справа от нее; большинство фиолетовых и желтых точек слева.
#  Это делает линию 1 предпочтительной, поскольку она лучше разделяет желтые и фиолетовые точки.
#  Нам нужно математически определить эту идею, чтобы мы могли алгоритмически найти лучшую линию.
#  Логистическая регрессия — это способ математического поиска наилучшей линии.

# TODO: Probability of Surviving (Вероятность выживания)
#  Чтобы определить наилучшую возможную линию для разделения наших данных, нам нужен способ оценки линии.
#  Во-первых, давайте посмотрим на одну точку данных. В идеале, если точка данных — это выживший пассажир,
#  он должен находиться с правой стороны очереди и далеко от нее. Если это точка данных для пассажира,
#  который не выжил, она будет далеко от линии слева. Чем дальше он от линии, тем больше мы уверены,
#  что он находится на правильной стороне линии.
#  Для каждой точки данных у нас будет оценка со значением от 0 до 1.
#  Мы можем думать об этом как о вероятности чтобы пассажир выжил.
#  Если значение близко к 0, эта точка будет далеко слева от линии,
#  и это означает, что мы уверены, что пассажир не выжил.
#  Если значение близко к 1, эта точка будет далеко справа от линии, и это означает, что мы уверены, что пассажир выжил.
#  Значение 0,5 означает, что точка падает прямо на линию, и мы не уверены, выживет ли пассажир.
#  Уравнение для расчета этой оценки приведено ниже, хотя интуиция для него гораздо важнее фактического уравнения.
#  Напомним, что уравнение для линии имеет вид:
#      0 = ax+by+c
#  x — это Тариф,
#  y — Возраст,
#  a, b, c — коэффициенты, которыми мы управляем.
#  См. Рис: ProbabilitySurvivingPicture.png
#  Число e — математическая константа, приблизительно равная 2.71828
#  Эта функция называется сигмоидой.
# TODO: Логистическая регрессия дает не просто прогноз (выжил или нет),
#  а вероятность (вероятность 80%, что этот человек выжил).

# TODO: Likelihood (Вероятность)
#  Чтобы рассчитать, насколько хороша наша линия, нам нужно оценить, верны ли наши прогнозы.
#  В идеале, если мы прогнозируем с высокой вероятностью, что пассажир выживет
#  (это означает, что точка данных находится далеко справа от линии), то этот пассажир действительно выживает.
#  Таким образом, мы будем вознаграждены - когда предскажем что-то правильно,
#  и наказаны - если предскажем что-то неправильно.
#  Вот уравнение вероятности. Хотя опять же, интуиция важнее уравнения.
#  См. Рис: LikelihoodPicture_1.png
#  Здесь p — прогнозируемая вероятность выживания из предыдущей части.
#  Вероятность будет иметь значение от 0 до 1.
#  Чем выше значение, тем лучше наша линия.
#  Давайте рассмотрим пару возможностей:
#  • Если прогнозируемая вероятность p равна 0.25, а пассажир не выжил, мы получаем 0.75 балла (хорошо).
#  • Если прогнозируемая вероятность p равна 0.25 и пассажир выжил, мы получаем оценку 0.25 (плохо).
#   Мы умножаем все отдельные оценки для каждой точки данных вместе, чтобы получить оценку для нашей линии.
#   Таким образом, мы можем сравнивать разные линии, чтобы определить лучшую.
#   Скажем для простоты вычислений, что у нас есть 4 точки данных.
#   Мы получаем общий балл, умножая четыре балла вместе:
#  См. Рис: LikelihoodPicture_2.png
#      0.25 * 0.75 * 0.6 * 0.8 = 0.09
#  Значение всегда будет очень маленьким, поскольку это вероятность того, что наша модель все предсказывает идеально.
#  Идеальная модель будет иметь прогнозируемую вероятность 1 для всех положительных случаев
#  и 0 для всех отрицательных случаев.
#  Вероятность — это то, как мы оцениваем и сравниваем возможные варианты наиболее подходящей линии.

# TODO: What is Scikit-learn? (Что такое Scikit-learn?)
#  Теперь, когда мы заложили основы работы логистической регрессии, давайте углубимся в код для построения модели.
#  Для этого мы представим новый модуль Python под названием scikit-learn. Scikit-learn, часто сокращаемый до sklearn,
#  — это наш научный инструментарий. Все основные алгоритмы машинного обучения реализованы в sklearn.
#  Мы увидим, что с помощью всего нескольких строк кода мы можем построить несколько различных мощных моделей.
#  Обратите внимание, что scikit-learn постоянно обновляется.
#  Если у вас на компьютере установлена немного другая версия модуля, все будет работать корректно,
#  но вы можете увидеть немного другие значения, чем на игровой площадке.
#  Scikit-learn — один из лучших документированных модулей Python.
#  Вы можете найти множество примеров кода на scikit-learn.org.
#  https://scikit-learn.org/stable/

# TODO: Prep Data with Pandas (Подготовьте данные с помощью Pandas)
#  Прежде чем мы сможем использовать sklearn для построения модели, нам нужно подготовить данные с помощью Pandas.
#  Вернемся к нашему полному набору данных и рассмотрим команды Pandas. Вот кадр данных Pandas со всеми столбцами:
# import pandas as pd
#
# pd.options.display.max_columns = 7
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# print(df.head(5))
# TODO: во- первых, нам нужно сделать все наши столбцы числовыми. Вспомните, как создать булев столбец для пола.
#      df['male'] = df['Sex'] == 'male'
# TODO: Теперь давайте возьмем все характеристики и создадим пустой массив с именем X.
#  Сначала мы выберем все интересующие нас столбцы,
#  а затем используем метод значений, чтобы преобразовать его в пустой массив.
#      X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# TODO: Теперь возьмем цель (столбец Survived) и сохраним ее в переменной y.
#      y = df['Survived'].values
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# print(X)
# print(y)
# TODO: Стандартной практикой является вызов нашего двумерного массива характеристик X
#  и одномерного массива целевых значений y.

# TODO: Build a Logistic Regression Model with Sklearn (Создайте модель логистической регрессии с помощью Sklearn)
#  Начнем с импорта модели логистической регрессии:
#      from sklearn.linear_model import LogisticRegression
# TODO: Все модели sklearn построены как классы Python. Сначала мы создаем экземпляр класса.
#      model = LogisticRegression()
# TODO: Теперь мы можем использовать наши данные, которые мы ранее подготовили, для обучения модели.
#  Для построения модели используется метод подгонки.
#  Он принимает два аргумента: X (характеристики в виде массива 2d numpy) и y (цель в виде массива 1d numpy).
#  Для простоты давайте сначала предположим, что мы строим модель логистической регрессии,
#  используя только столбцы «Тариф» и «Возраст».
#  Сначала мы определяем X как матрицу признаков, а y как целевой массив.
#      X = df[['Fare', 'Age']].values
#      y = df['Survived'].values
# TODO: Теперь мы используем метод подгонки для построения модели.
#      model.fit(X, y)
# TODO: Подгонка модели означает использование данных для выбора линии наилучшего соответствия.
#  Мы можем видеть коэффициенты с атрибутами coef_ и intercept_.
#      print(model.coef_, model.intercept_)
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# X = df[['Fare', 'Age']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# print(model.coef_, model.intercept_)
# # [[ 0.01615949 -0.01549065]] [-0.51037152]
#
# plt.scatter(df['Fare'], df['Age'], c=df['Survived'])
# # plt.plot([32, 100], [0, 80])
# plt.xlabel('Fare')
# plt.ylabel('Age')
# plt.savefig('ScatterPlot.png')
# plt.show()
# TODO: Эти значения означают, что уравнение выглядит следующим образом:
#      0 = 0.0161594x + -0.01549065y + -0.51037152
# TODO: Вот линия, нарисованная на графике.
#  Вы можете видеть, что он делает достойную (но не отличную) работу по разделению желтых и фиолетовых точек.
#  Мы немного поставили себя в тупик, используя только 2 из наших функций,
#  поэтому в следующих частях мы будем использовать все функции.
#  Может быть трудно запомнить операторы импорта для разных моделей sklearn.
#  Если не можете вспомнить, просто посмотрите документацию scikit-learn.
#  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

# TODO: Make Predictions with the Model (Делайте прогнозы с помощью модели)
#  Мы действительно усложнили нашу модель тем, что использовали только две характеристики из предыдущих частей,
#  поэтому давайте перестроим модель, используя их все.
#      X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
#      y = df['Survived'].values
#      model = LogisticRegression()
#      model.fit(X, y)
# TODO: Теперь мы можем использовать метод predict() для прогнозирования.
#      model.predict(X)
# TODO: Первый пассажир в наборе данных:
#      [3, True, 22.0, 1, 0, 7.25]
# TODO: Это означает, что пассажир относится к классу P 3, является мужчиной, ему 22 года,
#  у него на борту 1 брат/сестра/супруга, 0 родителей/ребенка на борту и он заплатил 7.25 долларов США.
#  Посмотрим, что предсказывает модель для этого пассажира.
#  Обратите внимание, что даже с одной точкой данных метод прогнозирования принимает
#  двумерный массив numpy и возвращает одномерный массив numpy.
#      print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
#      # [0]
# TODO: Результат равен 0, что означает, что модель предсказывает, что этот пассажир не выжил.
#  Давайте посмотрим, что предсказывает модель для первых 5 строк данных, и сравним их с нашим целевым массивом.
#  Мы получаем первые 5 строк данных с помощью X[:5] и первые 5 значений цели с помощью y[:5].
#      print(model.predict(X[:5]))
#      # [0 1 1 1 0]
#      print(y[:5])
#      # [0 1 1 1 0]
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
# print(model.predict(X[:5]))
# print(y[:5])
# TODO: Мы видим, что он получил все 5 правильных!
#  Метод предсказания возвращает массив из 1 и 0, где 1 означает, что модель предсказывает,
#  что пассажир выжил, а 0 означает, что модель предсказывает, что пассажир не выжил.

# TODO: Score the Model (Оценка модели)
#  Мы можем понять, насколько хороша наша модель, подсчитав количество точек данных, которые она правильно предсказывает
#  Это называется показателем точности. Давайте создадим массив с предсказанными значениями y.
#      y_pred = model.predict(X)
# TODO: Теперь мы создаем массив логических значений того, правильно ли наша модель предсказала каждого пассажира.
#      y == y_pred
# TODO: Чтобы получить число из них, которые верны, мы можем использовать метод numpy sum.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print(y.size)
# print(y.shape[0])
# print((y == y_pred).sum())
# TODO: Это означает, что из 887 точек данных модель делает правильный прогноз для 714 из них.
#  Чтобы получить правильный процент, мы делим его на общее количество пассажиров.
#  Мы получаем общее количество пассажиров, используя атрибут shape.
#      y.shape[0]
# TODO: Таким образом, наша оценка точности вычисляется следующим образом.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print((y == y_pred).sum() / y.shape[0])
# TODO: Таким образом, точность модели составляет 80%.
#  Другими словами, модель делает правильный прогноз для 80% точек данных.
#  Это достаточно распространенный расчет, который sklearn уже реализовал за нас.
#  Таким образом, мы можем получить тот же результат, используя метод score.
#  Метод оценки использует модель для предсказания X и подсчитывает, какой процент из них соответствует y.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print(model.score(X, y))
# TODO: При таком альтернативном методе расчета точности мы получаем то же значение, 80%.
#  Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# model = LogisticRegression()
# model.fit(X, y)
#
# y_pred = model.predict(X)
# print((y == y_pred).sum())
# print((y == y_pred).sum() / y.shape[0])
# print(model.score(X, y))
# TODO: В следующем модуле мы увидим, что оценка модели — это гораздо больше.

# TODO: Introducing the Breast Cancer Dataset (Представляем набор данных по раку молочной железы)
#  Теперь, когда мы создали инструменты для построения модели логистической регрессии для набора данных классификации,
#  мы представим новый набор данных. В наборе данных о раке молочной железы каждая точка данных имеет
#  измерения на основе изображения массы молочной железы и того, является ли она злокачественной.
#  Цель будет заключаться в том, чтобы использовать эти измерения, чтобы предсказать, является ли образование раковым.
#  Этот набор данных встроен прямо в scikit-learn, поэтому нам не нужно будет читать CSV.
#  Давайте начнем с загрузки набора данных и просмотра данных и их форматирования.
#      from sklearn.datasets import load_breast_cancer
#      cancer_data = load_breast_cancer()
# TODO: Возвращаемый объект (который мы сохранили в переменной Cancer_data) представляет собой объект,
#  похожий на словарь Python. Мы можем увидеть доступные ключи с помощью метода keys.
#      print(cancer_data.keys())
# TODO: Мы начнем с просмотра DESCR, который дает подробное описание набора данных.
#      print(cancer_data['DESCR'])
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
# print(cancer_data.keys())
# print(cancer_data['DESCR'])
# TODO: Мы видим, что есть 30 характеристик, 569 точек данных, и цель является либо злокачественной (раковой),
#  либо доброкачественной (не раковой). Для каждой из точек данных у нас есть измерения массы груди
#  (радиус, текстура, периметр и т. д.). Для каждого из 10 измерений было вычислено несколько значений,
#  поэтому у нас есть среднее значение, стандартная ошибка и наихудшее значение.
#  Это приводит к 10 * 3 или 30 общим характеристикам.
#  В наборе данных о раке молочной железы есть несколько характеристик, которые рассчитываются на основе других столбцов
#  Процесс выяснения того, какие дополнительные характеристики следует рассчитать,
#  называется проектированием характеристик.

# TODO: Loading the Data into Pandas (Загрузка данных в Pandas)
#  Давайте вытащим характеристики и целевые данные из объекта Cancer_data.
#  Во-первых, данные объекта сохраняются с помощью ключа «данные».
#  Когда мы смотрим на него, мы видим, что это пустой массив с 569 строками и 30 столбцами.
#  Это потому, что у нас есть 569 точек данных и 30 характеристик. Ниже приведен массив данных.
#      cancer_data['data']
# TODO: Мы используем shape (форму), чтобы увидеть, что это массив с 569 строками и 30 столбцами.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['data'].shape)
# TODO: Чтобы поместить это в Pandas DataFrame и сделать его более удобочитаемым, нам нужны имена столбцов.
#  Они сохраняются с ключом «feature_names».
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['feature_names'])
# TODO: Теперь мы можем создать Pandas DataFrame со всеми данными наших характеристик.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# # pd.options.display.max_columns = 30
# cancer_data = load_breast_cancer()
#
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# print(df.head())
# TODO: мы видим, что у нас есть 30 столбцов в DataFrame, так как у нас есть 30 характеристик.
#  Вывод усекается, чтобы поместиться на экране.
#  Мы использовали метод head(), поэтому наш результат имеет только 5 точек данных.
#  Нам все еще нужно поместить целевые данные в наш DataFrame, который можно найти с помощью ключа target.
#  Мы видим, что цель представляет собой одномерный массив numpy из 1 и 0.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target'])
# TODO: Если мы посмотрим на форму массива, то увидим, что это одномерный массив с 569 значениями
#  (именно столько у нас было точек данных).
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target'].shape)
# TODO: Чтобы интерпретировать эти 1 и 0, нам нужно знать, являются ли 1 или 0 доброкачественными или злокачественными.
#  Это дается target_names
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# print(cancer_data['target_names'])
# TODO: Это дает массив ['злокачественный' 'доброкачественный'], который говорит нам, что 0 означает злокачественный,
#  а 1 означает доброкачественный. Давайте добавим эти данные в Pandas DataFrame.
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# cancer_data = load_breast_cancer()
#
# df['target'] = cancer_data['target']
# print(df.head())
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
#
# # pd.options.display.max_columns = 30
# cancer_data = load_breast_cancer()
#
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
# print(df.head())
# TODO: Важно дважды проверить, правильно ли вы интерпретируете логические столбцы.
#  В нашем случае цель 0 означает злокачественную опухоль, а 1 означает доброкачественную.

# TODO: Build a Logistic Regression Model (Создайте модель логистической регрессии)
#  Теперь, когда мы просмотрели наши данные и привели их в удобный формат,
#  мы можем построить нашу матрицу характеристик X и целевой массив y,
#  чтобы мы могли построить модель логистической регрессии.
#      X = df[cancer_data.feature_names].values
#      y = df['target'].values
# TODO: Теперь мы создаем объект логистической регрессии и используем метод подгонки для построения модели.
#      model = LogisticRegression()
#      model.fit(X, y)
# TODO: Когда мы запускаем этот код, мы получаем предупреждение о конвергенции.
#  Это означает, что модели требуется больше времени, чтобы найти оптимальное решение.
#  Один из вариантов — увеличить количество итераций. Вы также можете переключиться на другой решатель,
#  что мы и сделаем. Решатель — это алгоритм, который модель использует для нахождения уравнения линии.
#  Вы можете увидеть возможные решатели в документации по логистической регрессии.
#  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
#                      decision_function(X) - Прогнозировать показатели достоверности для образцов.
#                      densify() - Преобразование матрицы коэффициентов в формат плотного массива.
#                      fit(X, y[, sample_weight]) - Соответствуйте модели в соответствии с данными обучения.
#                      get_params([deep]) - Получить параметры для этого оценщика.
#                      predict(X) - Предсказать метки класса для образцов в X.
#                      predict_log_proba(X) - Предсказать логарифм оценок вероятности.
#                      predict_proba(X) - Оценки вероятности.
#                      score(X, y[, sample_weight]) - Возвращает среднюю точность для заданных тестовых данных и меток.
#                      set_params(**params) - Установите параметры этой оценки.
#                      sparsify() - Преобразование матрицы коэффициентов в разреженный формат.
#      model = LogisticRegression(solver='liblinear')
#      model.fit(X, y)
# TODO: Давайте посмотрим, что предсказывает модель для первой точки данных в нашем наборе данных.
#  Напомним, что метод предсказания принимает двумерный массив, поэтому мы должны поместить точку данных в список.
#      model.predict([X[0]])
# TODO: Таким образом, модель предсказывает, что первая точка данных является злокачественной.
#  Чтобы увидеть, насколько хорошо модель работает со всем набором данных,
#  мы используем метод оценки, чтобы увидеть точность модели.
#      model.score(X, y)
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.datasets import load_breast_cancer
# from sklearn.linear_model import LogisticRegression
#
# cancer_data = load_breast_cancer()
# df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
# df['target'] = cancer_data['target']
#
# X = df[cancer_data.feature_names].values
# y = df['target'].values
#
# model = LogisticRegression(solver='liblinear')
# model.fit(X, y)
# print("prediction for datapoint 0:", model.predict([X[0]]))
# print(model.score(X, y))
# TODO: Мы видим, что модель правильно получает 96% точек данных.
#  С помощью разработанных нами инструментов мы можем построить модель для любого набора классификационных данных.

# TODO: ЗАДАЧА: Machine Learning - Bob the Builder (Машинное обучение — Боб Строитель)
#  Построение модели логистической регрессии.
#  Задача Вам дается матрица признаков и одна точка данных для прогнозирования.
#  Ваша задача будет состоять в том, чтобы построить модель логистической регрессии с матрицей признаков
#  и сделать прогноз (1 или 0) для одной точки данных.
#  Формат ввода:
#  Первая строка: количество точек данных в матрице признаков (n)
#  Следующие n строк: значения строки в матрице признаков, разделенные пробелами
#  Следующая строка: целевые значения, разделенные пробелами
#  Последняя строка: значения (разделенные пробелами) одна точка данных без целевого значения
#  Формат вывода 1 или 0
#  Sample Input:
#  6
#  1 3
#  3 5
#  5 7
#  3 1
#  5 3
#  7 5
#  1 1 1 0 0 0
#  2 4
#  Sample Output:
#  1
#  Пояснение:
#  Мы видим точки, нанесенные на график выше, и линию, разделяющую данные.
#  Точка (2, 4) отмечена на графике, и вы можете видеть,
#  что она находится на положительной стороне линии, поэтому результат равен 1.
# from sklearn.linear_model import LogisticRegression
#
# n = int(input())
# X = []
# for i in range(n):
#     X.append([float(x) for x in input().split()])
# y = [int(x) for x in input().split()]
# datapoint = [float(x) for x in input().split()]
#
# model = LogisticRegression()
# model.fit(X, y)
# print(*model.predict([datapoint]))

# TODO: Accuracy (Точность)
#  В предыдущем модуле мы рассчитали, насколько хорошо наша модель работает с точностью.
#  Точность — это процент верных прогнозов.
#  Если у вас есть 100 точек данных и вы предсказываете 70 из них правильно и 30 неправильно, точность составляет 70%.
#  Точность — очень простая и понятная метрика, однако она не всегда самая лучшая.
#  Например, предположим, что у меня есть модель для прогнозирования мошеннических списаний с кредитной карты.
#  Из 10000 кредитных карт у нас есть 9900 законных списаний и 100 мошеннических списаний.
#  Я мог бы построить модель, которая просто предсказывает, что каждое отдельное обвинение является законным,
#  и она оправдала бы 9900/10000 (99%) прогнозов!
#  Точность является хорошей мерой, если наши классы разделены поровну, но она вводит в заблуждение,
#  если у нас несбалансированные классы.
#  Всегда проявляйте осторожность с точностью.
#  Вам нужно знать распределение классов, чтобы знать, как интерпретировать значение.

# TODO: Confusion Matrix (Матрица путаницы)
#  Как мы заметили в предыдущей части, нас заботит не только то, для скольких точек данных мы предсказываем
#  правильный класс, нас заботит, сколько положительных точек данных мы предсказываем правильно,
#  а также сколько отрицательных точек данных мы предсказываем правильно.
#  Мы можем увидеть все важные значения в так называемой матрице путаницы (или матрице ошибок, или таблице путаницы).
#  Матрица путаницы представляет собой таблицу, показывающую четыре значения:
#  • Точки данных, которые мы предсказывали как положительные, но на самом деле положительные
#  • Точки данных, которые мы предсказывали как положительные, но на самом деле отрицательные
#  • Точки данных, которые мы прогнозировали как отрицательные, но на самом деле положительные
#  • Точки данных, которые мы прогнозировали как отрицательные, но на самом деле отрицательные
#  Первая и четвертая — это точки данных, которые мы предсказали правильно, а вторая и третья — точки данных,
#  которые мы предсказали неправильно.
#  В нашем наборе данных Титаника у нас есть 887 пассажиров, 342 выжили (положительно) и 545 не выжили (отрицательно).
#  Модель, которую мы построили в предыдущем модуле, имеет следующую матрицу путаницы.
#      См. Рис: ConfusionMatrixPicture_1.png
#  Заштрихованные синим квадраты — это количество верных прогнозов.
#  Таким образом, из 342 выживших пассажиров мы предсказали 233 или их правильно (и 109 из них неверно).
#  Из 545 пассажиров, которые не выжили, мы правильно предсказали 480 (и 65 неверно).
#  Мы можем использовать матрицу путаницы для вычисления точности.
#  Напоминаем, что точность — это количество правильно предсказанных точек данных,
#  деленное на общее количество точек данных.
#      (233+480)/(233+65+109+480) = 713/887 = 80.38%
# TODO: Это действительно то же самое значение, которое мы получили в предыдущем модуле.
#  Матрица путаницы полностью описывает, как модель работает с набором данных,
#  хотя ее сложно использовать для сравнения моделей.

# TODO: True Positives, True Negatives, False Positives, False Negatives (TP Истинные позитивы, TN Истинные негативы,
#  FP Ложноположительные, FN Ложноотрицательные)
#  У нас есть имена для каждого квадрата матрицы путаницы.
#  Истинный положительный результат (TP) — это точка данных,
#  которую мы предсказали положительно и в отношении которой мы были правы.
#  Истинный отрицательный результат (TN) — это точка данных, которую мы предсказали отрицательно
#  и в отношении которой мы были правы.
#  Ложноположительный результат (FP) — это точка данных, которую мы предсказали положительно,
#  в отношении которой мы ошиблись.
#  Ложноотрицательный результат (FN) — это точка данных, которую мы предсказали отрицательно,
#  но в отношении которой мы ошиблись.
#  Условия могут быть немного сложными для отслеживания.
#  Способ запомнить, что второе слово — это наш прогноз (положительный или отрицательный),
#  а первое слово — то, был ли этот прогноз правильным (истинным или ложным).
#  Вы часто будете видеть матрицу путаницы, описанную следующим образом:
#      См. Рис: ConfusionMatrixPicture_2.png
#  TODO: Четыре значения матрицы путаницы (TP, TN, FP, FN) используются для вычисления нескольких различных показателей,
#   которые мы будем использовать позже.

# TODO: Precision (достоверность)
#  Двумя широко используемыми показателями для классификации являются precision (достоверность) и recall (полнота).
#  Концептуально Precision относится к проценту положительных результатов,
#  которые относятся к актуальной полноте процента правильно классифицированных положительных результатов.
#  Оба могут быть определены с использованием квадрантов из матрицы путаницы, которая, как мы помним,
#  выглядит следующим образом: (См. Рис: PrecisionPicture_1.png)
#  Precision — это процент правильных положительных прогнозов модели. Мы определяем его следующим образом:
#  (См. Рис: PrecisionPicture_2.png)
#  #  Если мы посмотрим на нашу матрицу путаницы для нашей модели набора данных Титаника,
#  мы сможем рассчитать Precision.
#  (См. Рис: PrecisionPicture_3.png)
#      precision = TP / (TP + FP)
#      precision = 233 / (233 + 65) = 0.7819
# TODO: Precision (достоверность) — это мера того, насколько точна модель с ее положительными предсказаниями

# TODO: Recall (Полнота)
#  Recall — это процент положительных случаев, которые модель предсказывает правильно.
#  Опять же, мы будем использовать матрицу путаницы для вычисления нашего результата.
#      (См. Рис: RecallPicture_1.png)
#  Здесь мы математически определяем Recall: Давайте рассчитаем Recall для нашей модели для набора данных Титаник.
#      (См. Рис: RecallPicture_3.png)
#  recall = TP / (TP + FN)
#  recall = 233 / (233 + 109) = 0.6813
#      (См. Рис: RecallPicture_4.png)
#      (См. Рис: RecallPicture_4.png)
#  Recall — это мера того, сколько положительных случаев может вспомнить модель.

# TODO: Precision & Recall Trade-off (Компромисс достоверности и полнота)
#  Мы часто оказываемся в ситуации выбора между увеличением Recall - (при снижении Precision)
#  или повышением Precision (и снижением Recall). Это будет зависеть от ситуации,
#  которую мы хотим максимизировать. Например, предположим, что мы создаем модель,
#  чтобы предсказать, является ли списание средств с кредитной карты мошенническим.
#  Положительные случаи для нашей модели — это мошеннические обвинения, а отрицательные — законные обвинения.
#  Давайте рассмотрим два сценария:
#  1. Если мы предскажем, что платеж является мошенническим, мы отклоним платеж.
#  2. Если мы обнаружим, что платеж является мошенническим, мы позвоним клиенту, чтобы подтвердить платеж.
#  В случае 1 покупателю доставляет огромное неудобство, когда модель неверно предсказывает мошенничество
#  (ложное срабатывание).
#  В случае 2 ложное срабатывание является незначительным неудобством для клиента.
#  Чем больше ложных срабатываний, тем ниже Precision.
#  Из-за высокой стоимости ложных срабатываний в первом случае было бы целесообразно иметь низкий Recall,
#  чтобы иметь очень высокую Precision.
#  В случае 2 вам понадобится больше баланса между Precision и Recall.
#  Не существует жестких и быстрых правил относительно того, на какие значения Precision и Recall вы должны действовать.
#  Это всегда зависит от набора данных и приложения.

# TODO: F1 Score (Оценка F1)
#  Precision была привлекательной метрикой, потому что это было единственное число.
#  Precision и Recall — это два числа, поэтому не всегда очевидно, как выбрать между двумя моделями,
#  если одна из них имеет более высокую Precision, а другая — более высокую Recall.
#  Оценка F1 представляет собой среднее значение Precision и Recall, поэтому у нас есть единая оценка для нашей модели.
#  Вот математическая формула для оценки F1.
#      (См. Рис: F1ScorePicture.png)
#      F1 = 2 * ((precision * recall) / (precision + recall))
#  Давайте посчитаем оценку F1 для нашей модели для набора данных «Титаник».
#  Мы будем использовать значения Precision и Recall, которые мы рассчитали ранее.
#  Precision = 0.7819
#  Recall = 0.6813
#  Оценка F1 выглядит следующим образом.
#      F1 = 2 * ((0.7819) * (0.6813) / (0.7819 + 0.6813)) = 0.7281
#  Показатель F1 представляет собой среднее гармоническое значений Precision и Recall.

# TODO: Accuracy, Precision, Recall & F1 Score in Sklearn (Точность, достоверность, полнота и Оценка F1 в Sklearn)
#  В Scikit-learn встроена функция для каждой из введенных нами метрик.
#  У нас есть отдельная функция для каждой Accuracy, Precision, Recall и F1 Score.
#  Чтобы использовать их, давайте начнем с вызова нашего кода из предыдущего модуля
#  для построения модели логистической регрессии. Код считывает набор данных Titanic из CSV-файла и
#  помещает его в Pandas DataFrame. Затем мы создаем матрицу признаков X и целевые значения y.
#  Мы создаем модель логистической регрессии и подгоняем ее к нашему набору данных.
#  Наконец, мы создаем переменную y_pred наших прогнозов (y_pred — это прогнозируемые значения.
#  Что бы понять, насколько хороша наша модель, подсчитав количество точек данных,
#  которые она правильно предсказывает, называется показателем точности).
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# model = LogisticRegression()
# model.fit(X, y)
# y_pred = model.predict(X)
# TODO: Теперь мы готовы использовать наши метрические функции. Давайте импортируем их из scikit-learn.
#      from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# TODO: Каждая функция принимает два одномерных массива numpy: истинные значения цели и прогнозируемые значения цели.
#  У нас есть истинные значения цели и предсказанные значения цели.
#  Таким образом, мы можем использовать метрические функции следующим образом.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# model = LogisticRegression()
# model.fit(X, y)
# y_pred = model.predict(X)
#
# print("accuracy:", accuracy_score(y, y_pred))
# print("precision:", precision_score(y, y_pred))
# print("recall:", recall_score(y, y_pred))
# print("f1 score:", f1_score(y, y_pred))
# TODO: Мы видим, что Accuracy составляет 80%, что означает, что 80% предсказаний модели верны.
#  Precision (достоверность) составляет 77%, что, как мы помним,
#  является процентом правильных положительных прогнозов модели.
#  Recall составляет 69%, что является процентом положительных случаев, которые модель предсказала правильно.
#  F1 Score составляет 73%, что является средним значением precision (достоверность) и recall (полнота).
#  В случае одной модели значения метрик мало что нам говорят. Для некоторых задач хорошо значение 60%,
#  а для других хорошо значение 90%, в зависимости от сложности задачи.
#  Мы будем использовать значения метрик для сравнения различных моделей, чтобы выбрать лучшую.

# TODO: Confusion Matrix in Sklearn (Матрица путаницы в Sklearn)
#  В Scikit-learn есть функция матрицы путаницы,
#  которую мы можем использовать для получения четырех значений в матрице путаницы:
#  - TP истинно положительные,
#  - FP ложноположительные,
#  - FN ложноотрицательные,
#  - TN истинно отрицательные
#  Предполагая, что y — это наши истинные целевые значения, а y_pred — это прогнозируемые значения,
#  мы можем использовать функцию путаницы_матрицы следующим образом:
# import pandas as pd
# from sklearn.metrics import confusion_matrix
# from sklearn.linear_model import LogisticRegression
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# model = LogisticRegression()
# model.fit(X, y)
# y_pred = model.predict(X)
# print(confusion_matrix(y, y_pred))
# TODO: Scikit-learn переворачивает матрицу путаницы, чтобы сначала показать отрицательные значения!
#  Вот как следует обозначить эту матрицу путаницы.
#      (См. Рис: ConfusionMatrixSklearnPicture_1.jpg)
#  Вот как мы обычно рисуем матрицу путаницы.
#      (См. Рис: ConfusionMatrixSklearnPicture_2.jpg)
#  Поскольку отрицательные целевые значения соответствуют 0, а положительные — 1,
#  scikit-learn упорядочил их в таком порядке. Убедитесь, что вы перепроверили правильность интерпретации значений!

# TODO: Overfitting (Переобучение)
#  До сих пор мы построили модель со всеми нашими данными, а затем увидели,
#  насколько хорошо она работает с теми же данными.
#  Это искусственно завышает наши цифры, поскольку наша модель, по сути,
#  должна была увидеть ответы на викторину до того, как мы дали ей викторину.
#  Это может привести к тому, что мы называем переобучение.
#  Переобучение — это когда мы хорошо работаем с данными,
#  которые модель уже видела, но плохо работаем с новыми данными.
#  Мы можем визуально увидеть модель переобучения следующим образом.
#  Линия слишком близко пытается получить каждую точку данных на правильной стороне линии, но ей не хватает сути данных.
#  См. Рис: OverfittingPicture_1.png
#  На графике вы можете видеть, что мы проделали довольно хорошую работу,
#  получив желтые точки вверху и фиолетовые точки внизу, но это не отражает того, что происходит.
#  Единственная точка выброса действительно может сбить с толку расположение линии.
#  Несмотря на то, что модель получит высокую оценку на уже просмотренных данных,
#  вряд ли она будет хорошо работать на новых данных.
#  Чем больше функций у нас есть в нашем наборе данных, тем больше мы будем подвержены переобучению.

# TODO: Training Set and Test Set (Обучающий набор и Тестовый набор)
#  Чтобы дать модели справедливую оценку, мы хотели бы знать, насколько хорошо наши данные будут работать с данными,
#  которые она еще не видела. В действии наша модель будет делать прогнозы на основе данных,
#  на которые мы не знаем ответа, поэтому мы хотели бы оценить, насколько хорошо наша модель работает с новыми данными,
#  а не только с данными, которые она уже видела. Чтобы имитировать прогнозирование новых невидимых данных,
#  мы можем разбить наш набор данных на обучающий набор и тестовый набор.
#  Обучающий набор используется для построения моделей. Тестовый набор используется для оценки моделей.
#  Мы разделяем наши данные перед построением модели, поэтому модель ничего не знает о тестовом наборе,
#  и мы дадим ей справедливую оценку.
#  Если в нашем наборе данных 200 точек данных, разбивка его на обучающий набор
#  и тестовый набор может выглядеть следующим образом.
#      См. Рис: TrainingSetTestSetPicture.png
#  Стандартная разбивка состоит в том, чтобы поместить 70-80% наших данных в обучающий набор и 20-30% в тестовый набор.
#  Использование меньшего количества данных в обучающем наборе означает, что у нашей модели будет не так много
#  данных для обучения, поэтому мы хотим предоставить их как можно больше, но при этом оставить достаточно для оценки.

# TODO: Training and Testing in Sklearn (Обучение и тестирование в Sklearn)
#  Scikit-learn имеет встроенную функцию для разделения данных на обучающий набор и тестовый набор.
#  Предполагая, что у нас есть двумерный массив X - наших функций и одномерный массив y - цели,
#  мы можем использовать функцию train_test_split.
#  Он случайным образом поместит каждую точку данных либо в обучающий набор, либо в тестовый набор.
#  По умолчанию обучающий набор составляет 75 % данных, а тестовый набор оставшиеся 25 % данных.
# from sklearn.model_selection import train_test_split
# X_train, X_test, y_train, y_test = train_test_split(X, y)
# TODO: Давайте воспользуемся атрибутом shape, чтобы увидеть размеры наших наборов данных.
# print("whole dataset:", X.shape, y.shape)
# print("training set:", X_train.shape, y_train.shape)
# print("test set:", X_test.shape, y_test.shape)
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# print("whole dataset:", X.shape, y.shape)
# print("training set:", X_train.shape, y_train.shape)
# print("test set:", X_test.shape, y_test.shape)
# TODO: Мы видим, что из 887 точек данных в нашем наборе данных 665 из них находятся в нашем обучающем наборе,
#  а 222 — в тестовом наборе. Каждая точка данных из нашего набора данных используется
#  ровно один раз либо в обучающем наборе, либо в тестовом наборе.
#  Обратите внимание, что у нас есть 6 функций в нашем наборе данных, поэтому у нас все еще есть 6 функций
#  как в нашем обучающем наборе, так и в тестовом наборе.
#  Мы можем изменить размер нашего обучающего набора, используя параметр train_size.
#  Например, train_test_split(X, y, train_size=0.6) поместит 60% данных в обучающий набор и 40% в тестовый набор.

# TODO: Building a Scikit-learn Model Using a Training Set
#  (Построение модели Scikit-learn с использованием обучающего набора)
#  Теперь, когда мы знаем, как разделить наши данные на обучающий набор и тестовый набор,
#  нам нужно изменить способ построения и оценки модели.
#  Все построение модели выполняется с помощью обучающего набора, а вся оценка выполняется с помощью тестового набора.
#  В последнем модуле мы построили модель и оценили ее на том же наборе данных.
#  Теперь мы строим модель, используя обучающую выборку.
# model = LogisticRegression()
# model.fit(X_train, y_train)
# TODO: И мы оцениваем модель с помощью набора тестов.
# print(model.score(X_test, y_test))
# TODO: На самом деле все метрики, которые мы рассчитывали в предыдущих частях,
#  должны быть рассчитаны на тестовом наборе.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# from sklearn.model_selection import train_test_split
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# # building the model
# model = LogisticRegression()
# model.fit(X_train, y_train)
#
# # evaluating the model
# print("accuracy:", model.score(X_test, y_test))
# y_pred = model.predict(X_test)
# print("accuracy:", accuracy_score(y_test, y_pred))
# print("precision:", precision_score(y_test, y_pred))
# print("recall:", recall_score(y_test, y_pred))
# print("f1 score:", f1_score(y_test, y_pred))
# TODO: Наши значения accuracy, precision (достоверность), recall и оценки F1 на самом деле очень похожи на значения,
#  когда мы использовали весь набор данных. Это признак того, что наша модель не переобучена!
#  Если вы запустите код, вы заметите, что каждый раз вы получаете разные оценки.
#  Это связано с тем, что разделение обучение/тестирование выполняется случайным образом,
#  и в зависимости от того, какие точки попадут в обучающий набор и тест, оценки будут разными.
#  Когда мы доберемся до урока перекрестной проверки, мы увидим,
#  что у нас есть более точные средства измерения этих оценок.

# TODO: Using a Random State (Использование случайного состояния)
#  Как мы заметили в предыдущей части, когда мы случайным образом разделяем данные на обучающий набор и тестовый набор,
#  мы получаем разные точки данных в каждом наборе каждый раз, когда запускаем код.
#  Это результат случайности, и нам нужно, чтобы он был случайным, чтобы он был эффективным,
#  но иногда это может затруднить тестирование кода.
#  Например, каждый раз, когда мы запускаем следующий код, мы получаем разные результаты.
# from sklearn.model_selection import train_test_split
#
# X = [[1, 1], [2, 2], [3, 3], [4, 4]]
# y = [0, 0, 1, 1]
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
# # X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)
# print('X_train', X_train)
# print('X_test', X_test)
# TODO: Чтобы каждый раз получать одно и то же разделение, мы можем использовать атрибут random_state.
#  Мы выбираем произвольное число, чтобы каждый раз использовать только его,
#  когда мы запускаем код, и мы будем получать одно и то же разделение.
# from sklearn.model_selection import train_test_split
#
# X = [[1, 1], [2, 2], [3, 3], [4, 4]]
# y = [0, 0, 1, 1]
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)
# print('X_train', X_train)
# print('X_test', X_test)
# TODO: Случайное состояние также называется семенем.

# TODO: Logistic Regression Threshold (Порог логистической регрессии)
#  Если вы помните, в Уроке 2 мы говорили о компромиссе между precision (достоверностью) и recall.
#  С помощью модели логистической регрессии у нас есть простой способ переключения
#  между упором на точность и упором на припоминание. Модель логистической регрессии не просто возвращает прогноз,
#  но возвращает значение вероятности от 0 до 1. Обычно мы говорим, что если значение >=0.5,
#  мы прогнозируем выжившего пассажира, а если значение <0.5, пассажир не выжил.
#  Однако мы можем выбрать любой порог от 0 до 1. Если мы увеличим порог, у нас будет меньше положительных прогнозов,
#  но наши положительные прогнозы с большей вероятностью окажутся правильными.
#  Это означает, что precision (достоверность) будет выше, а recall ниже.
#  С другой стороны, если мы снизим порог, у нас будет больше положительных прогнозов,
#  поэтому мы с большей вероятностью поймаем все положительные случаи.
#  Это означает, что recall будет выше, а precision (достоверность) ниже.
#  Каждый выбор порога представляет собой другую модель.
#  Кривая ROC (рабочая характеристика приемника) представляет собой график,
#  показывающий все возможные модели и их характеристики.

# TODO: Sensitivity & Specificity (Чувствительность и специфичность)
#  Кривая ROC представляет собой график зависимости чувствительности от специфичности.
#  Эти значения демонстрируют тот же компромисс, что и precision (достоверность) и recall.
#  Давайте вернемся к матрице путаницы, так как мы будем использовать ее для
#  определения чувствительности и специфичности.
#      См. Рис: SensitivitySpecificityPicture_1.png
#  Чувствительность - это еще один термин для recall, который представляет собой TP истинный положительный показатель.
#  Напомним, что он рассчитывается следующим образом:
#      sensitivity = recall = positives predicted correctly / positive cases = TP / (TP + FN)
#      См. Рис: SensitivitySpecificityPicture_2.png
#  Специфичностью является TN истинный отрицательный показатель. Он рассчитывается следующим образом.
#      specificity = negatives predicted correctly / negative cases = TN / (TN + FP)
#      См. Рис: SensitivitySpecificityPicture_3.png
#  Мы провели тестовое разделение нашего набора данных «Титаник» и получили следующую матрицу путаницы.
#      См. Рис: SensitivitySpecificityPicture_4.png
#  У нас есть 96 положительных случаев и 126 отрицательных случаев в нашем тестовом наборе.
#  Рассчитаем чувствительность и специфичность.
#      Sensitivity = 61 / 96 = 0.6354
#      Specificity = 105 / 126 = 0.8333
# TODO: Цель состоит в том, чтобы максимизировать эти два значения, хотя, как правило,
#  увеличение одного приводит к уменьшению другого. Это будет зависеть от ситуации,
#  будем ли мы уделять больше внимания чувствительности или специфичности.
#  В то время как мы обычно смотрим на значения positive и recall,
#  для построения графиков стандартом является использование чувствительности и специфичности.
#  Можно построить кривую precision (достоверность) и recall (полнота), но обычно это не делается.

# TODO: Sensitivity & Specificity in Scikit-learn (Чувствительность и специфичность в Scikit-learn)
#  В Scikit-learn не определены функции для чувствительности и специфичности, но мы можем сделать это сами.
#  Чувствительность аналогична recall, поэтому ее легко определить.
# from sklearn.metrics import recall_score
# sensitivity_score = recall_score
# print(sensitivity_score(y_test, y_pred))
# # 0.6829268292682927
# TODO: Теперь, чтобы определить специфичность, если мы понимаем, что это также recall отрицательного класса,
#  мы можем получить значение из функции sklearn precision_recall_fscore_support.
#  Давайте посмотрим на выходные данные Precision_recall_fscore_support.
# from sklearn.metrics import precision_recall_fscore_support
#
# print(precision_recall_fscore_support(y, y_pred))
# TODO: Второй массив — это recall, поэтому мы можем игнорировать остальные три массива.
#  Есть два значения. Первый — это recall отрицательного класса, а второй — recall положительного класса.
#  Второе значение — это стандартное значение recall или чувствительности, и вы можете видеть,
#  что это значение соответствует тому, что мы получили выше. Первая ценность – специфичность.
#  Итак, давайте напишем функцию для получения именно этого значения.
# def specificity_score(y_true, y_pred):
#     p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
#     return r[0]
#
#
# print(specificity_score(y_test, y_pred))
# # 0.9214285714285714
# TODO: Обратите внимание, что в примере кода мы используем random_state в разделении обучение/тестирование,
#  поэтому каждый раз, когда вы запускаете код, вы получаете одни и те же результаты.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import recall_score, precision_recall_fscore_support
#
# sensitivity_score = recall_score
#
#
# def specificity_score(y_true, y_pred):
#     p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
#     return r[0]
#
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)
#
# model = LogisticRegression()
# model.fit(X_train, y_train)
# y_pred = model.predict(X_test)
#
# print("sensitivity:", sensitivity_score(y_test, y_pred))
# print("specificity:", specificity_score(y_test, y_pred))
# TODO: Чувствительность такая же, как recall (или recall положительного класса),
#  а специфичность - это recall отрицательного класса.

# TODO: Adjusting the Logistic Regression Threshold in Sklearn (Настройка порога логистической регрессии в Sklearn)
#  Когда вы используете метод прогнозирования scikit-learn, вам даются значения прогноза 0 и 1.
#  Однако за кулисами модель логистической регрессии получает значение вероятности от 0 до 1 для каждой точки данных,
#  а затем округляет до 0 или 1.
#  Если мы хотим выбрать другой порог, отличный от 0.5, нам понадобятся эти значения вероятности.
#  Мы можем использовать функцию predict_proba, чтобы получить их.
#      (model.predict_proba(X_test)
# TODO: Результатом является пустой массив с двумя значениями для каждой точки данных (например, [0.78, 0.22]).
#  Вы заметите, что сумма двух значений равна 1. Первое значение — это вероятность того,
#  что точка данных находится в классе 0 (не сохранилась), а второе — это вероятность того,
#  что точка данных находится в классе 1 (сохранилась).
#  Нам нужен только второй столбец этого результата, который мы можем получить с помощью следующего синтаксиса numpy.
#      model.predict_proba(X_test)[:, 1]
# TODO: Теперь мы просто хотим сравнить эти значения вероятности с нашим порогом.
#  Скажем, нам нужен порог 0.75. Мы сравниваем приведенный выше массив с 0.75.
#  Это даст нам массив значений True/False, который будет нашим массивом прогнозируемых целевых значений.
#      y_pred = model.predict_proba(X_test)[:, 1] > 0.75
# TODO: Порог 0.75 означает, что нам нужно быть более уверенными, чтобы сделать положительный прогноз.
#  Это приводит к меньшему количеству положительных прогнозов и большему количеству отрицательных прогнозов.
#  Теперь мы можем использовать любые метрики scikit-learn до использования y_test
#  в качестве наших истинных значений и y_pred в качестве наших прогнозируемых значений.
#      print("precision:", precision_score(y_test, y_pred))
#      print("recall:", recall_score(y_test, y_pred))
# TODO: Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import precision_score, recall_score
# from sklearn.model_selection import train_test_split
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# model = LogisticRegression()
# model.fit(X_train, y_train)
#
# y_pred = model.predict_proba(X_test)[:, 1] > 0.75
#
# print("precision:", precision_score(y_test, y_pred))
# print("recall:", recall_score(y_test, y_pred))
# TODO: Установив пороговое значение 0.5, мы получим исходную модель логистической регрессии.
#  Любое другое пороговое значение дает альтернативную модель.

# TODO: How to Build an ROC Curve (Как построить кривую ROC)
#  Кривая ROC представляет собой график зависимости специфичности от чувствительности.
#  Мы строим модель логистической регрессии, а затем рассчитываем специфичность
#  и чувствительность для каждого возможного порога. Каждая предсказанная вероятность является порогом.
#  Если у нас есть 5 точек данных со следующими прогнозируемыми вероятностями: 0.3, 0.4, 0.6, 0.7, 0.8,
#  мы будем использовать каждое из этих 5 значений в качестве порога.
#  Обратите внимание, что мы на самом деле строим график зависимости чувствительности от (1-специфичности).
#  Нет веской причины делать это таким образом, кроме того, что это стандарт.
#  Давайте начнем с просмотра кода для построения кривой ROC.
#  В Scikit-learn есть функция roc_curve, которую мы можем использовать.
#  Функция берет истинные целевые значения и предсказанные вероятности из нашей модели.
#  Сначала мы используем метод predict_proba для модели, чтобы получить вероятности.
#  Затем мы вызываем функцию roc_curve. Функция roc_curve возвращает массив ложных срабатываний,
#  массив истинных срабатываний и пороговые значения.
#  Частота ложноположительных результатов соответствует 1-специфичности (ось X),
#  а частота истинных положительных результатов — это еще один термин для чувствительности (ось Y).
#  Пороговые значения не будут нужны на графике. Вот код для построения кривой ROC в matplotlib.
#  Обратите внимание, что у нас также есть код для построения диагональной линии.
#  Это может помочь нам визуально увидеть, насколько наша модель далека от модели,
#  которая предсказывает случайным образом. Мы предполагаем, что у нас уже есть набор данных,
#  разделенный на обучающий набор и тестовый набор.
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import roc_curve
# import matplotlib.pyplot as plt
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# model = LogisticRegression()
# model.fit(X_train, y_train)
# y_pred_proba = model.predict_proba(X_test)
# fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])
#
# plt.plot(fpr, tpr)
# plt.plot([0, 1], [0, 1], linestyle='--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.0])
# plt.xlabel('1 - specificity')
# plt.ylabel('sensitivity')
# plt.show()
# TODO: Поскольку мы не используем пороговые значения для построения графика,
#  график не сообщает нам, какое пороговое значение даст каждая из возможных моделей.

# TODO: ROC Curve Interpretation (Интерпретация кривой ROC)
#  Кривая ROC показывает производительность не одной модели, а многих моделей.
#  Каждый выбор порога представляет собой другую модель.
#  Давайте посмотрим на нашу кривую ROC с выделенными точками.
#  Каждая точка A, B и C относится к модели с другим порогом.
#  Модель А имеет чувствительность 0.6 и специфичность 0.9 (напомним, что на графике показана 1-специфичность).
#  Модель B имеет чувствительность 0.8 и специфичность 0.7. Модель C имеет чувствительность 0.9 и специфичность 0.5.
#  Как выбрать между этими моделями, будет зависеть от специфики нашей ситуации.
#      См. Рис: ROCCurveInterpretationPicture.png
#  Чем ближе кривая подходит к верхнему левому углу, тем выше производительность.
#  Линия никогда не должна опускаться ниже диагональной линии,
#  так как это будет означать, что она работает хуже, чем случайная модель.

# TODO: Picking a Model from the ROC Curve (Выбор модели из кривой ROC)
#  Когда мы будем готовы завершить нашу модель, мы должны выбрать один порог,
#  который мы будем использовать для наших прогнозов.
#  Кривая ROC помогает нам выбрать идеальный порог для нашей проблемы.
#  Давайте снова посмотрим на нашу кривую ROC с выделенными тремя точками:
#  если мы находимся в ситуации, когда более важно, чтобы все наши положительные прогнозы были правильными,
#  чем чтобы мы уловили все положительные случаи (это означает, что мы правильно предсказываем
#  большинство отрицательных случаев), мы должны выбрать модель с более высокой специфичностью (модель A).
#  Если мы находимся в ситуации, когда важно выявить как можно больше положительных случаев,
#  мы должны выбрать модель с более высокой чувствительностью (модель C).
#  Если нам нужен баланс между чувствительностью и специфичностью, мы должны выбрать модель B.
#      См. Рис: ROCCurveInterpretationPicture.png
#  Уследить за всеми этими терминами может быть сложно. Даже экспертам приходится просматривать их снова,
#  чтобы убедиться, что они правильно интерпретируют значения.

# TODO: Area Under the Curve (Площадь под кривой)
#  Иногда мы будем использовать кривую ROC для сравнения двух разных моделей. Вот сравнение кривых ROC двух моделей.
#      См. Рис: AreaUnderCurvePicture.png
#  Вы можете видеть, что синяя кривая превосходит оранжевую, поскольку синяя линия почти всегда выше оранжевой.
#  Чтобы получить эмпирическую меру этого, мы вычисляем площадь под кривой,
#  также называемую AUC. Это площадь под ROC-кривой. Это значение от 0 до 1, чем выше, тем лучше.
#  Поскольку ROC представляет собой график всех различных моделей логистической регрессии
#  с разными пороговыми значениями, AUC не измеряет производительность одной модели.
#  Это дает общее представление о том, насколько хорошо работает модель логистической регрессии.
#  Чтобы получить единую модель, вам все равно нужно найти оптимальный порог для вашей задачи.
#  Давайте воспользуемся scikit-learn, чтобы рассчитать площадь под кривой.
#  Мы можем использовать функцию roc_auc_score.
#      (roc_auc_score(y_test, y_pred_proba[:,1])
# TODO: Вот значения для двух строк:
#      Blue AUC: 0.8379
#      Orange AUC: 0.7385
# TODO: Эмпирически видно, что синий лучше. Мы можем использовать функцию roc_auc_score
#  для вычисления оценки AUC модели логистической регрессии в наборе данных Titanic.
#  Мы строим две модели логистической регрессии: model1 с 6 характеристиками
#  и model2 только с Pclass и мужскими характеристиками.
#  Мы видим, что показатель AUC модели 1 выше. Запустите этот код, чтобы увидеть результаты:
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import roc_auc_score
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# model1 = LogisticRegression()
# model1.fit(X_train, y_train)
# y_pred_proba1 = model1.predict_proba(X_test)
# print("model 1 AUC score:", roc_auc_score(y_test, y_pred_proba1[:, 1]))
#
# model2 = LogisticRegression()
# model2.fit(X_train[:, 0:2], y_train)
# y_pred_proba2 = model2.predict_proba(X_test[:, 0:2])
# print("model 2 AUC score:", roc_auc_score(y_test, y_pred_proba2[:, 1]))
# TODO: Важно отметить, что эта метрика говорит нам, насколько хорошо в целом модель
#  логистической регрессии работает с нашими данными. Поскольку кривая ROC показывает
#  производительность нескольких моделей, AUC не измеряет производительность одной модели.

# TODO: Concerns with Training and Test Set (Проблемы с обучающим и тестовым набором)
#  Мы проводим оценку, потому что хотим получить точную оценку того, насколько хорошо работает модель.
#  Если наш набор данных небольшой, наш набор тестов будет маленьким.
#  Таким образом, это может быть нехороший случайный набор точек данных,
#  и по случайному стечению обстоятельств в нашем оценочном наборе могут оказаться простые или сложные точки данных.
#  Поскольку наша цель состоит в том, чтобы получить наилучшее возможное измерение наших показателей
#  (accuracy (точность), precision (достоверность), recall (полнота) и F1 score (оценка F1),
#  мы можем сделать немного лучше, чем просто один обучающий и тестовый набор.
#  Напомним, что наше разделение обучающей и тестовой выборки выглядит следующим образом.
#  Как мы видим, все значения в обучающем наборе никогда не используются для оценки.
#  Было бы несправедливо строить модель с обучающим набором, а затем оценивать с помощью обучающего набора,
#  но мы не получаем максимально полную картину производительности модели.
#      См. Рис: ConcernsTrainingTestSetPicture_1.png
# TODO: Чтобы убедиться в этом эмпирически, давайте попробуем запустить код из урока 3,
#  который разделяет обучение и тестирование. Мы повторим это несколько раз и посмотрим на результаты.
#  Каждая строка является результатом разного случайного разделения обучение/тестирование.
#      См. Рис: ConcernsTrainingTestSetPicture_2.png
#  Вы можете видеть, что каждый раз, когда мы запускаем его, мы получаем разные значения показателей.
#  Точность колеблется от 0.79 до 0.84, достоверность от 0.75 до 0.81 и полнота от 0.63 до 0.75.
#  Это широкие диапазоны, которые зависят только от того, насколько нам повезло или не повезло,
#  какие точки данных оказались в тестовом наборе.
#  Вот код, если вы хотите попробовать запустить себя и увидеть различные значения показателей.
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# from sklearn.model_selection import train_test_split
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# X_train, X_test, y_train, y_test = train_test_split(X, y)
#
# # building the model (построение модели & Training set (Обучающий набор))
# model = LogisticRegression()
# model.fit(X_train, y_train)
#
# # evaluating the model (оценка модели & Test set (Тестовый набор))
# y_pred = model.predict(X_test)
# # print(" accuracy: {0:.5f}".format(accuracy_score(y_test, y_pred)))
# # print("precision: {0:.5f}".format(precision_score(y_test, y_pred)))
# # print("   recall: {0:.5f}".format(recall_score(y_test, y_pred)))
# # print(" f1 score: {0:.5f}".format(f1_score(y_test, y_pred)))
# print(f' accuracy: {(accuracy_score(y_test, y_pred)).round(5)}')
# print(f'precision: {(precision_score(y_test, y_pred)).round(5)}')
# print(f'   recall: {(recall_score(y_test, y_pred)).round(5)}')
# print(f' f1 score: {(f1_score(y_test, y_pred)).round(5)}')
# TODO: Вместо того, чтобы делать одно разделение обучение/тестирование,
#  мы разделим наши данные на обучающий набор и тестовый набор несколько раз.

# TODO: Multiple Training and Test Sets (Несколько обучающих и тестовых наборов)
#  В предыдущей части мы узнали, что в зависимости от нашего набора тестов
#  мы можем получить разные значения метрик оценки.
#  Мы хотим получить представление о том, насколько хорошо работает наша модель в целом,
#  а не только о том, насколько хорошо она работает на одном конкретном наборе тестов.
#  Вместо того, чтобы просто взять часть данных в качестве тестового набора,
#  давайте разобьем наш набор данных на 5 частей. Предположим, у нас есть 200 точек данных в нашем наборе данных.
#  Каждый из этих 5 фрагментов будет служить тестовым набором. Когда фрагмент 1 является тестовым набором,
#  мы используем оставшиеся 4 фрагмента в качестве обучающего набора.
#  Таким образом, у нас есть 5 обучающих и тестовых наборов следующим образом.
#  Каждый из 5 раз у нас есть тестовый набор 20% (40 точек данных) и обучающий набор 80% (160 точек данных).
#      См. Рис: MultipleTrainingTestSetsPicture_1.png
#      См. Рис: MultipleTrainingTestSetsPicture_2.png
#  Каждая точка данных находится ровно в 1 тестовом наборе.

# TODO: Building and Evaluating with Multiple Training and Test Sets
#  (Создание и оценка с помощью нескольких обучающих и тестовых наборов)
#  В предыдущей части мы увидели, как мы можем сделать 5 тестовых наборов,
#  каждый из которых имеет свой тренировочный набор.
#  Теперь для каждого обучающего набора мы строим модель и оцениваем ее с помощью связанного тестового набора.
#  Таким образом, мы строим 5 моделей и вычисляем 5 баллов.
#  Допустим, мы пытаемся рассчитать показатель Accuracy (точности) для нашей модели.
#  Мы сообщаем точность как среднее значение 5 значений:
#      См. Рис: BuildingEvaluatingMultipleTrainingTestSetsPicture_1.jpg
#  (0.83+0.79+0.78+0.80+0.75)/5 = 0.79
#  Если бы мы только что выполнили один обучающий и тестовый набор и случайно получили первый,
#  мы бы сообщили о точности 0.83. Если бы мы получили последний случайным образом, мы бы сообщили о точности 0.75.
#  Усреднение всех этих возможных значений помогает устранить влияние того, в какой набор тестов попадает точка данных.
#  Вы увидите такие разные значения только при наличии небольшого набора данных.
#  С большими наборами данных мы часто просто делаем обучающий и тестовый набор для простоты.
#  Этот процесс создания нескольких обучающих и тестовых наборов называется k-кратной перекрестной проверкой.
#  k — это количество фрагментов, на которые мы разделили наш набор данных.
#  Стандартное число — 5, как мы сделали в нашем примере выше.
#  Наша цель при перекрестной проверке — получить точные измерения для наших метрик
#  (accuracy (точность), precision(достоверность, воспроизводимость), recall (полнота)).
#  Мы строим дополнительные модели, чтобы быть уверенными в цифрах, которые мы рассчитываем и сообщаем.

# TODO: Final Model Choice in k-fold Cross Validation (Окончательный выбор модели в k-кратной перекрестной проверке)
#  Теперь мы построили 5 моделей вместо одной. Как мы выбираем единую модель для использования?
#  Эти 5 моделей были созданы только для целей оценки, чтобы мы могли сообщать о значениях метрик.
#  На самом деле нам не нужны эти модели, и мы хотим построить наилучшую возможную модель.
#  Наилучшей возможной моделью будет модель, которая использует все данные.
#  Поэтому мы отслеживаем наши расчетные значения для наших показателей оценки,
#  а затем строим модель, используя все данные. Это может показаться невероятно расточительным,
#  но компьютеры обладают большой вычислительной мощностью, поэтому стоит использовать немного больше,
#  чтобы убедиться, что мы сообщаем правильные значения для наших показателей оценки.
#  Мы будем использовать эти значения для принятия решений, поэтому их правильный расчет очень важен.
#  Вычислительная мощность для построения модели может быть проблемой, когда набор данных большой.
#  В этих случаях мы просто делаем тестовый сплит.

# TODO: KFold Class (Класс KFold)
#  Scikit-learn уже реализовал код для разбиения набора данных на k фрагментов и создания k обучающих и тестовых наборов
#  Для простоты давайте возьмем набор данных всего с 6 точками данных и 2 характеристиками
#  и трехкратной перекрестной проверкой набора данных.
#  Мы возьмем первые 6 строк из набора данных Titanic и будем использовать только столбцы Age и Fare.
# X = df[['Age', 'Fare']].values[:6]
# y = df['Survived'].values[:6]
# TODO: Начнем с создания экземпляра объекта класса KFold. Он принимает два параметра: n_splits
#  (это k, количество фрагментов для создания) и shuffle (независимо от того, рандомизировать порядок данных или нет).
#  Как правило, рекомендуется перемешивать данные, так как вы часто получаете набор данных в отсортированном порядке.
# kf = KFold(n_splits=3, shuffle=True)
# TODO: Класс KFold имеет метод разделения, который создает 3 разделения для наших данных.
#  Давайте посмотрим на вывод метода разделения.
#  Метод split возвращает генератор, поэтому мы используем функцию списка, чтобы превратить его в список.
# list(kf.split(X))
# TODO: Результат:
# from sklearn.model_selection import KFold
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# X = df[['Age', 'Fare']].values[:6]
# y = df['Survived'].values[:6]
#
# kf = KFold(n_splits=3, shuffle=True)
# print(list(kf.split(X)))
# TODO: Как мы видим, у нас есть 3 обучающих и тестовых набора, как и ожидалось.
#  Первый обучающий набор состоит из точек данных 0, 2, 3, 5, а тестовый набор состоит из точек данных 1, 4.
#  Разделение выполняется случайным образом, поэтому ожидайте увидеть разные точки данных
#  в наборах каждый раз, когда вы запускаете код.

# TODO: Creating Training and Test Sets with the Folds (Создание обучающих и тестовых наборов со сгибами)
#  Мы использовали класс KFold и метод разделения, чтобы получить индексы, которые находятся в каждом из разделений.
#  Теперь давайте используем этот результат, чтобы получить наше первое (из 3) разделение обучение/тест.
#  Сначала вытащим первый сплит.
# splits = list(kf.split(X))
# first_split = splits[0]
# print(first_split)
# # (array([0, 2, 3, 5]), array([1, 4]))
# TODO: Первый массив — это индексы обучающего набора, а второй — индексы тестового набора.
#  Давайте создадим эти переменные.
# train_indices, test_indices = first_split
# print("training set indices:", train_indices)
# print("test set indices:", test_indices)
# # training set indices: [0, 2, 3, 5]
# # test set indices: [1, 4]
# TODO: Теперь мы можем создать X_train, y_train, X_test и y_test на основе этих индексов.
# X_train = X[train_indices]
# X_test = X[test_indices]
# y_train = y[train_indices]
# y_test = y[test_indices]
# TODO: Если мы распечатаем каждую из них, мы увидим,
#  что у нас есть четыре точки данных в X_train и их целевые значения в y_train.
#  Остальные 2 точки данных находятся в X_test, а их целевые значения — в y_test.
# print("X_train")
# print(X_train)
# print("y_train", y_train)
# print("X_test")
# print(X_test)
# print("y_test", y_test)
# TODO: Запустите этот код, чтобы увидеть результаты:
# from sklearn.model_selection import KFold
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# X = df[['Age', 'Fare']].values[:6]
# y = df['Survived'].values[:6]
#
# kf = KFold(n_splits=3, shuffle=True)
#
# splits = list(kf.split(X))
# print(splits)
# first_split = splits[0]
# train_indices, test_indices = first_split
# print("training set indices:", train_indices)
# print("test set indices:", test_indices)
#
# X_train = X[train_indices]
# X_test = X[test_indices]
# y_train = y[train_indices]
# y_test = y[test_indices]
# print("X_train\n", X_train)
# print("y_train\n", y_train)
# print("X_test\n", X_test)
# print("y_test\n", y_test)
# TODO: На данный момент у нас есть обучающие и тестовые наборы в том же формате,
#  что и при использовании функции train_test_split.

# TODO: Build a Model (Построить модель)
#  Теперь мы можем использовать обучающие и тестовые наборы для построения модели и прогнозирования, как и раньше.
#  Вернемся к использованию всего набора данных (поскольку 4 точек данных недостаточно для построения приличной модели).
#  Вот весь код для построения и оценки модели при первом сгибе 5-кратной перекрестной проверки.
#  Обратите внимание, что код для подбора и оценки модели точно такой же,
#  как и при использовании функции train_test_split.
#  Попробуй это сейчас:
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# import pandas as pd
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# kf = KFold(n_splits=5, shuffle=True)
#
# splits = list(kf.split(X))
# train_indices, test_indices = splits[0]
# X_train = X[train_indices]
# X_test = X[test_indices]
# y_train = y[train_indices]
# y_test = y[test_indices]
#
# model = LogisticRegression()
# model.fit(X_train, y_train)
# print(model.score(X_test, y_test))
# TODO: До сих пор мы, по сути, разделяли один обучение/тест.
#  Чтобы выполнить k-кратную перекрестную проверку,
#  нам нужно использовать каждое из остальных 4 разделений для построения модели и оценки модели.

# TODO: Loop Over All the Folds (Цикл по всем сгибам)
#  Мы делали сгибы по одному, но на самом деле мы хотим перебрать все сгибы, чтобы получить все значения.
#  Мы поместим код из предыдущей части внутрь нашего цикла for.
# scores = []
# kf = KFold(n_splits=5, shuffle=True)
# for train_index, test_index in kf.split(X):
#     X_train, X_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]
#     model = LogisticRegression()
#     model.fit(X_train, y_train)
#     scores.append(model.score(X_test, y_test))
# print(scores)
# [0.75847, 0.83146, 0.85876, 0.76271, 0.74011]
# TODO: Поскольку у нас 5 кратностей, мы получаем 5 значений точности.
#  Напомним, чтобы получить единственное конечное значение, нам нужно взять среднее значение этих значений.
# print(np.mean(scores))
# 0.79029
# TODO: Теперь, когда мы рассчитали точность, нам больше не нужны 5 разных моделей, которые мы построили.
#  Для будущего использования нам нужна только одна модель.
#  Чтобы получить единственную наилучшую модель, мы строим модель на всем наборе данных.
#  Если нас спросят о достоверности этой модели, мы используем достоверность,
#  рассчитанную перекрестной проверкой (0.79029),
#  хотя на самом деле мы не проверяли эту конкретную модель с помощью тестового набора.
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# import pandas as pd
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# y = df['Survived'].values
#
# scores = []
# kf = KFold(n_splits=5, shuffle=True)
# for train_index, test_index in kf.split(X):
#     X_train, X_test = X[train_index], X[test_index]
#     y_train, y_test = y[train_index], y[test_index]
#     model = LogisticRegression()
#     model.fit(X_train, y_train)
#     scores.append(model.score(X_test, y_test))
# print(scores)
# print(np.mean(scores))
# final_model = LogisticRegression()
# final_model.fit(X, y)
# TODO: Ожидайте получать немного разные значения каждый раз, когда вы запускаете код.
#  Класс KFold каждый раз случайным образом разбивает данные, поэтому другое разбиение приведет к разным оценкам,
#  хотя вы должны ожидать, что среднее значение 5 оценок будет примерно одинаковым.

# TODO: Comparing Different Models (Сравнение различных моделей
#  До сих пор мы использовали наши методы оценки, чтобы получить оценки для одной модели.
#  Эти методы станут невероятно полезными, когда мы представим больше моделей и захотим определить,
#  какая из них лучше всего подходит для конкретной задачи.
#  Давайте используем наши методы для сравнения трех моделей:
#  • Модель логистической регрессии, использующая все характеристики в нашем наборе данных
#  • Модель логистической регрессии, использующая только столбцы Pclass, Age и Sex
#  • Модель логистической регрессии, использующая только столбцы Fare и Age.
#  Не ожидал бы, что вторая или третья модель будут работать лучше, поскольку в них меньше информации,
#  но мы можем определить, что использование только этих двух или трех столбцов дает производительность,
#  сравнимую с использованием всех столбцов.
#  Методы оценки необходимы для выбора между несколькими вариантами модели.

# TODO: Building the Models with Scikit-learn (Построение моделей с помощью Scikit-learn)
#  Давайте напишем код для построения двух моделей в scikit-learn.
#  Затем мы будем использовать k-кратную перекрестную проверку для расчета точности,
#  достоверности, полноты и оценки F1 для двух моделей, чтобы мы могли их сравнить.
#  Во-первых, мы импортируем необходимые модули и подготавливаем данные, как делали это раньше.
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# import pandas as pd
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
# TODO: Теперь мы можем построить объект KFold. Мы будем использовать 5 сплитов как стандарт.
#  Обратите внимание, что мы хотим создать один объект KFold, который будут использовать все модели.
#  Было бы несправедливо, если бы разные модели получали разное разделение данных.
# kf = KFold(n_splits=5, shuffle=True)
# TODO: Теперь мы создадим три разные матрицы признаков X1, X2 и X3. У всех будет одна и та же цель y.
# X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# X2 = df[['Pclass', 'male', 'Age']].values
# X3 = df[['Fare', 'Age']].values
# y = df['Survived'].values
# TODO: Поскольку мы будем делать это несколько раз, давайте напишем функцию для оценки модели.
#  Эта функция использует объект KFold для вычисления точности, достоверности, полноты и оценки F1
#  для модели логистической регрессии с заданной матрицей признаков X и целевым массивом y.
#  Затем мы вызываем нашу функцию три раза для каждой из наших трех матриц признаков и видим результаты.
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# import pandas as pd
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
#
# kf = KFold(n_splits=5, shuffle=True)
#
# X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# X2 = df[['Pclass', 'male', 'Age']].values
# X3 = df[['Fare', 'Age']].values
# y = df['Survived'].values
#
#
# def score_model(X, y, kf):
#     accuracy_scores = []
#     precision_scores = []
#     recall_scores = []
#     f1_scores = []
#     for train_index, test_index in kf.split(X):
#         X_train, X_test = X[train_index], X[test_index]
#         y_train, y_test = y[train_index], y[test_index]
#         model = LogisticRegression()
#         model.fit(X_train, y_train)
#         y_pred = model.predict(X_test)
#         accuracy_scores.append(accuracy_score(y_test, y_pred))
#         precision_scores.append(precision_score(y_test, y_pred))
#         recall_scores.append(recall_score(y_test, y_pred))
#         f1_scores.append(f1_score(y_test, y_pred))
#     print("accuracy:", np.mean(accuracy_scores))
#     print("precision:", np.mean(precision_scores))
#     print("recall:", np.mean(recall_scores))
#     print("f1 score:", np.mean(f1_scores))
#
#
# print("Logistic Regression with all features")
# score_model(X1, y, kf)
# print()
# print("Logistic Regression with Pclass, Sex & Age features")
# score_model(X2, y, kf)
# print()
# print("Logistic Regression with Fare & Age features")
# score_model(X3, y, kf)
# TODO: Мы интерпретируем эти результаты в следующей части.
#  Ожидайте получать немного разные результаты каждый раз, когда вы запускаете код.
#  Разделения в k-кратном порядке выбираются случайным образом, поэтому будут небольшие вариации в зависимости от того,
#  в каком разделении окажется каждая точка данных.

# TODO: Choosing a Best Model (Выбор лучшей модели)
#  Посмотрим на результаты предыдущей части.
#      Logistic Regression with all features
#      accuracy: 0.7959055418015616
#      precision: 0.764272127669388
#      recall: 0.6783206767486641
#      f1 score: 0.7163036778464393
#  .
#      Logistic Regression with Pclass, Sex & Age features
#      accuracy: 0.7981908207960389
#      precision: 0.7715749823848419
#      recall: 0.6830371999703425
#      f1 score: 0.7232930032930033
#  .
#      Logistic Regression with Fare & Age features
#      accuracy: 0.6538944962864216
#      precision: 0.6519918328980114
#      recall: 0.23722965720416847
#      f1 score: 0.34438594236494796
# TODO: Если сравнивать первые две модели, то у них почти одинаковые баллы.
#  Третья модель имеет более низкие оценки по всем четырем показателям.
#  Таким образом, первые два варианта намного лучше, чем третий.
#  Это соответствует интуиции, поскольку третья модель не имеет доступа к полу пассажира.
#  Мы ожидаем, что у женщин больше шансов выжить, поэтому пол будет очень ценным предиктором.
#  Поскольку первые две модели дают эквивалентные результаты, имеет смысл выбрать более простую модель,
#  которая использует функции Pclass, Sex & Age.
#  Теперь, когда мы выбрали лучшую модель, мы строим единую окончательную модель, используя все данные.
# model = LogisticRegression()
# model.fit(X1, y)
# TODO: Теперь мы можем сделать прогноз с помощью нашей модели.
# from sklearn.model_selection import KFold
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# import pandas as pd
# import numpy as np
#
# df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# df['male'] = df['Sex'] == 'male'
#
# kf = KFold(n_splits=5, shuffle=True)
#
# X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
# X2 = df[['Pclass', 'male', 'Age']].values
# X3 = df[['Fare', 'Age']].values
# y = df['Survived'].values
#
#
# def score_model(X, y, kf):
#     accuracy_scores = []
#     precision_scores = []
#     recall_scores = []
#     f1_scores = []
#     for train_index, test_index in kf.split(X):
#         X_train, X_test = X[train_index], X[test_index]
#         y_train, y_test = y[train_index], y[test_index]
#         model = LogisticRegression()
#         model.fit(X_train, y_train)
#         y_pred = model.predict(X_test)
#         accuracy_scores.append(accuracy_score(y_test, y_pred))
#         precision_scores.append(precision_score(y_test, y_pred))
#         recall_scores.append(recall_score(y_test, y_pred))
#         f1_scores.append(f1_score(y_test, y_pred))
#     print("accuracy:", np.mean(accuracy_scores))
#     print("precision:", np.mean(precision_scores))
#     print("recall:", np.mean(recall_scores))
#     print("f1 score:", np.mean(f1_scores))
#
#
# print("Logistic Regression with all features")
# score_model(X1, y, kf)
# print()
# print("Logistic Regression with Pclass, Sex & Age features")
# score_model(X2, y, kf)
# print()
# print("Logistic Regression with Fare & Age features")
# score_model(X3, y, kf)
#
# model = LogisticRegression()
# model.fit(X1, y)
# print(model.predict([[3, False, 25, 0, 1, 2]]))
#
# model = LogisticRegression()
# model.fit(X2, y)
# print(model.predict([[3, False, 25]]))
#
# model = LogisticRegression()
# model.fit(X3, y)
# print(model.predict([[3, 25]]))  # характеристика: sex (пол) - играет важную роль в прогнозировании модели!!!

# TODO: Мы пробовали только три различных комбинации характеристик.
#  Возможно, сработает и другая комбинация.

# TODO: Machine Learning - Welcome to the Matrix (Машинное обучение — добро пожаловать в матрицу)
#  Расчет показателей оценки с использованием матрицы путаницы.
#  Задание: Вам будут даны значения матрицы путаницы
#  (FP истинно положительные, FP ложноположительные, FN ложноотрицательные и TN истинно отрицательные).
#  Ваша задача состоит в том,
#  чтобы вычислить: Accuracy (точность), Precision (достоверность), Recall (полнота), F1 score (оценка f1),
#  а также распечатать значения, округленные до 4 знаков после запятой.
#  Чтобы округлить, вы можете использовать round(x, 4).
#  Формат ввода:  Значения tp, fp, fn, tn в указанном порядке, разделенные пробелами
#  Формат вывода: Каждое значение в отдельной строке, округленное до 4 знаков после запятой,
#  в следующем порядке: Accuracy (точность), Precision (достоверность), Recall (полнота), F1 score (оценка f1)
#  Sample Input:
#  233 65 109 480
#  Sample Output:
#  0.8038
#  0.7819
#  0.6813
#  0.7281
#  Объяснение:
#  Accuracy is (tp + tn) / total = (233+480)/(233+65+109+480)=0.8038
#  Precision is tp / (tp + fp) = 233/(233+65) = 0.7819
#  Recall is tp / (tp + fn) = 233/(233+109) = 0.6813
#  F1 score is 2 * precision * recall / (precision + recall) = 2*0.7819*0.6813/(0.7819+0.6813) = 0.7281
# tp, fp, fn, tn = [int(x) for x in input().split()]
#
# accuracy = (tp + tn) / (tp + fp + fn + tn)
# precision = tp / (tp + fp)
# recall = tp / (tp + fn)
# f1_score = 2 * precision * recall / (precision + recall)
#
# print(accuracy.__round__(4))
# print(precision.__round__(4))
# print(recall.__round__(4))
# print(f1_score.__round__(4))

# TODO: A Nonparametric Machine Learning Algorithm (Непараметрический алгоритм машинного обучения)
#  До сих пор мы имели дело с логистической регрессией.
#  В логистической регрессии мы смотрим на данные графически и рисуем линию, чтобы разделить данные.
#  Модель определяется коэффициентами, определяющими линию. Эти коэффициенты называются параметрами.
#  Поскольку модель определяется этими параметрами,
#  логистическая регрессия является параметрическим алгоритмом машинного обучения.
#  В этом модуле мы представим деревья решений,
#  которые являются примером непараметрического алгоритма машинного обучения.
#  Деревья решений не будут определяться списком параметров, как мы увидим в следующих уроках.
#  Каждый алгоритм машинного обучения является либо параметрическим, либо непараметрическим.

# TODO: Tree Terminology (Древовидная терминология)
#  Причина, по которой многие люди любят деревья решений, заключается в том, что их очень легко интерпретировать.
#  По сути, это блок-схема вопросов, на которые вы отвечаете о точке данных, пока не получите прогноз.
#  Вот пример дерева решений для набора данных Titanic. В следующем уроке мы увидим, как строится это дерево.
#  Каждый из прямоугольников называется узлом. Узлы, которые имеют характеристики разделения,
#  называются внутренними узлами. Самый первый внутренний узел вверху называется корневым узлом.
#  Конечные узлы, в которых мы делаем прогнозы выжившего/не выжившего, называются листовыми узлами.
#  У всех внутренних узлов есть два узла под ними, которые мы называем дочерними узлами.
#      См. Рис: TreeTerminologyPicture.png
#  Термины для деревьев (корень, лист) происходят от настоящего дерева, хотя оно перевернуто,
#  так как мы обычно рисуем корень наверху. Мы также используем термины,
#  которые рассматривают дерево как генеалогическое древо (дочерний узел и родительский узел).

# TODO: Interpreting a Decision Tree (Интерпретация дерева решений)
#  Чтобы интерпретировать это дерево решений, давайте рассмотрим пример.
#  Допустим, мы хотим узнать прогноз для 10-летнего пассажира мужского пола в классе P 2.
#  В первом узле, поскольку пол пассажира мужской, мы переходим к правому дочернему элементу.
#  Затем, начиная с их 10-летнего возраста, что составляет <= 13, мы переходим к левому дочернему элементу,
#  а в третьем узле мы переходим к правому дочернему элементу, поскольку P-класс равен 2.
#  На следующей диаграмме мы выделяем путь для этого пассажира.
#      См. Рис: InterpretingDecisionTreePicture.png
#  Обратите внимание, что нет правил, согласно которым мы используем каждую характеристику,
#  или в каком порядке мы используем характеристики, или для непрерывного значения (например, Возраст),
#  где мы делаем разделение. Стандартным в дереве решений является то, что каждое разделение имеет только 2 варианта.
#  Деревья решений часто предпочитают, если у вас есть нетехническая аудитория,
#  поскольку они могут легко интерпретировать модель.

# TODO: How did we get the Decision Tree? (Как мы получили дерево решений?)
#  При построении дерева решений мы не просто случайным образом выбираем, какую характеристику разделить первой.
#  Мы хотим начать с выбора характеристики с наибольшей прогностической силой.
#  Давайте снова посмотрим на наше то же самое Дерево решений.
#      См. Рис: HowDecisionTreePicture.png
#  Интуитивно понятно, что для нашего набора данных «Титаник»,
#  поскольку женщинам часто отдавалось предпочтение в спасательных шлюпках,
#  мы ожидаем, что пол будет очень важной характеристикой.
#  Поэтому использование этой характеристики в первую очередь имеет смысл.
#  На каждой стороне дерева решений мы независимо определим, какую характеристику разделить следующей.
#  В нашем примере выше второй сплит для женщин относится к классу P.
#  Второй сплит для мужчин – по возрасту.
#  Мы также отмечаем, что в некоторых случаях мы делаем три splits (сплита), а в некоторых — только два.
#  Для любого заданного набора данных существует множество различных возможных деревьев решений,
#  которые могут быть созданы в зависимости от порядка использования характеристик.
#  В следующих уроках мы увидим, как математически выбрать лучшее дерево решений.
