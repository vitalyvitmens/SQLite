# TODO: Data Science (Наука о данных)
#  Наука о данных — обязательная дисциплина в эпоху больших данных.
#  Сегодня компании и отрасли должны принимать решения на основе данных,
#  чтобы выжить. Узнайте, как работает наука о данных.

# TODO: Data Manipulation (Манипуляция данными)

# TODO: Welcome to Data Science (Добро пожаловать в науку о данных)
#  Поздравляем! Вы сделали большой шаг к тому, чтобы стать специалистом по данным!
#  В дополнение к прохождению этого курса не забудьте воспользоваться всей поддержкой обучения,
#  доступной вам на SoloLearn, включая ежедневные советы, практики «Попробуйте сами», задачи тренера по коду,
#  игровую площадку для кода и участие в нашем замечательном сообществе учащихся.
#  Мы рады услышать от вас, поэтому, пожалуйста, оставляйте комментарии и отзывы, когда вы учитесь с нами.
#  Давайте начнем!

# TODO: What is Data Science? (Что такое наука о данных?)
#  В бизнесе существует множество вариантов использования науки о данных,
#  включая поиск лучшего алгоритма прогнозирования цен на жилье для Zillow,
#  поиск ключевых атрибутов, связанных с качеством вина,
#  и создание системы рекомендаций для увеличения рейтинга кликов для Amazon.
#  Извлечение информации из, казалось бы, случайных данных, наука о данных обычно включает в себя сбор данных,
#  очистку данных, выполнение исследовательского анализа данных,
#  создание и оценку моделей машинного обучения и передачу информации заинтересованным сторонам.
#  Наука о данных — это междисциплинарная область, которая объединяет статистику,
#  анализ данных, машинное обучение и связанные с ними методы для извлечения знаний и идей.

# TODO: Why Python? (Почему питон?)
#  В этом курсе «Введение в науку о данных» мы изучаем науку о данных с помощью Python.
#  Будучи языком программирования общего назначения,
#  Python в настоящее время является самым популярным языком программирования в науке о данных.
#  Он прост в использовании, имеет отличную поддержку сообщества и хорошо интегрируется
#  с другими платформами (например, веб-приложениями) в инженерной среде.
#  Этот курс посвящен исследовательскому анализу данных с помощью
#  трех фундаментальных библиотек Python: numpy, pandas и matplotlib.
#  Также будет рассмотрена библиотека машинного обучения scikit-learn.
#  В более поздних модулях мы будем прогнозировать стоимость домов с помощью линейной регрессии,
#  определять классы ириса с помощью алгоритмов классификации и находить
#  кластеры в винах — всего лишь несколько примеров того, что мы можем делать в науке о данных.
#  В науке о данных есть и другие популярные языки программирования,
#  такие как R, который имеет преимущество в статистическом моделировании.

# TODO: Numerical Data (Числовые данные)
#  Наборы данных поступают из самых разных источников и форматов: это могут быть наборы числовых измерений,
#  текстовый корпус, изображения, аудиоклипы или вообще что угодно.
#  Независимо от формата, первым шагом в науке о данных является преобразование его в массивы чисел.
#  Мы собрали 45 показателей роста президента США в сантиметрах в хронологическом порядке
#  и сохранили их в списке — встроенном типе данных в python.
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
# TODO: В данном примере Джордж Вашингтон был первым президентом, а его рост составлял 189 см.
#  Если бы мы хотели узнать, сколько президентов выше 188 см, мы могли бы пройтись по списку,
#  сравнить каждый элемент со 188 и увеличить количество на 1 по мере выполнения критерия.
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# cnt = 0
# for height in heights:
#     if height > 188:
#         cnt += 1
# print(cnt)
# TODO: Это показывает, что есть пять президентов, которые выше 188 см.
#  Независимо от формата данных, первым шагом в науке о данных является преобразование их в массивы чисел.

# TODO: Introduction to Numpy (Введение в Numpy)
#  Numpy (сокращение от Numerical Python) позволяет нам с легкостью найти ответ на вопрос,
#  сколько президентов выше 188 см. Ниже мы покажем, как использовать библиотеку и начнем с базового объекта в numpy.
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# heights_arr = np.array(heights)
# print((heights_arr > 188).sum())
# TODO: Оператор import позволяет нам получить доступ к функциям и модулям внутри библиотеки numpy.
#  Библиотека будет использоваться часто, поэтому по соглашению numpy импортируется под более коротким именем, np.
#  Вторая строка предназначена для преобразования списка в объект массива numpy через np.array(),
#  с которым могут работать инструменты, предоставляемые в numpy.
#  Последняя строка предоставляет простое и естественное решение исходного вопроса с помощью numpy.
#  По мере того, как наши наборы данных становятся больше и сложнее,
#  numpy позволяет нам использовать более эффективный метод без циклов for для обработки и анализа наших данных.
#  Наш пример набора данных в этом модуле будет включать рост, возраст и партийную принадлежность президентов США.
#  Модули Python могут получить доступ к коду из другого модуля, импортировав файл/функцию с помощью оператора import.

# TODO: Size and Shape (Размер и форма)
#  Класс массива в Numpy называется ndarray или n-мерным массивом.
#  Мы можем использовать это для подсчета количества президентов в heights_arr,
#  используя атрибут numpy.ndarray.size.
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# heights_arr = np.array(heights)
# print(heights_arr.size)
# TODO: Обратите внимание, что после создания массива в numpy его размер нельзя изменить.
#  Размер говорит нам, насколько велик массив, форма говорит нам о размере.
#  Чтобы получить текущую форму массива, используйте форму атрибута:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# heights_arr = np.array(heights)
# print(heights_arr.shape)
# TODO: На выходе получается кортеж, вспомним, что кортеж встроенного типа данных является неизменяемым,
#  в то время как список является изменяемым и содержит одно значение, указывающее,
#  что существует только одно измерение, т.е. ось 0.
#  Вдоль оси 0 имеется 45 элементов (по одному для каждого президента) Здесь heights_arr — это массив из 1d.
#  Размер атрибута в numpy аналогичен встроенному методу len в python, который используется
#  для вычисления длины итерируемых объектов python, таких как str, list, dict и т.д.

# TODO: Reshape (Изменить форму)
#  Другие данные, которые мы собрали, включают возраст президентов:
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
# TODO: Поскольку и рост, и возраст относятся к одним и тем же президентам, мы можем их объединить:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# # convert a list to a numpy array (преобразовать список в массив numpy)
# heights_and_ages_arr = np.array(heights_and_ages)
# print(heights_and_ages_arr.shape)
# TODO: Это создает один длинный массив. Было бы понятнее, если бы мы могли выровнять рост и возраст
#  для каждого президента и реорганизовать данные в матрицу 2 на 45, где первая строка содержит все росты,
#  а вторая строка — возраст. Для этого можно создать новый массив,
#  вызвав numpy.ndarray.reshape с новыми размерами, указанными в кортеже:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# # convert a list to a numpy array (преобразовать список в массив numpy)
# heights_and_ages_arr = np.array(heights_and_ages)
# print(heights_and_ages_arr.reshape((2, 45)))
# TODO: Измененный массив теперь представляет собой 2darray, но обратите внимание, что исходный массив не изменился.
#  Мы можем изменить форму массива несколькими способами,
#  если размер измененного массива соответствует размеру исходного.
#  Numpy может рассчитать для нас форму (размер), если мы укажем неизвестное измерение как -1.
#  Например, для заданного 2darray `arr` формы (3,4), arr.reshape(-1)
#  выведет 1darray формы (12,), а arr.reshape((-1,2)) создаст 2dray формы (6,2).

# TODO: Data Type (Тип данных)
#  Еще одна характеристика массива numpy заключается в том, что он однородный,
#  то есть каждый элемент должен иметь один и тот же тип данных.
#  Например, в heights_arr мы записали все высоты целыми числами;
#  таким образом, каждый элемент хранится как целое число в массиве.
#  Чтобы проверить тип данных, используйте numpy.ndarray.dtype
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# heights_arr = np.array(heights)
# print(heights_arr.dtype)
# TODO: Если мы смешаем число с плавающей запятой, скажем, первый элемент будет 189,0 вместо 189:
# heights_float = [189.0, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#                  193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#                  182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
# TODO: Затем, после преобразования списка в массив, мы увидим,
#  что все остальные числа преобразуются в числа с плавающей запятой:
# import numpy as np
# heights_float = [189.0, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#                  193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#                  182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# heights_float_arr = np.array(heights_float)
# print(heights_float_arr)
# print("\n")
# print(heights_float_arr.dtype)
# TODO: Numpy поддерживает несколько типов данных, таких как:
#  - int (целое число),
#  - float (числовое число с плавающей запятой)
#  - bool (логические значения, True и False).
#  Число после типа данных, например. int64 представляет разрядность типа данных.

# TODO: Indexing (Индексация)
#  Мы можем использовать индексацию массива для выбора отдельных элементов из массивов.
#  Как и списки Python, индекс numpy начинается с 0.
#  Чтобы получить доступ к высоте 3-го президента Томаса Джефферсона в 1darray 'heights_arr':
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# heights_arr = np.array(heights)
# print(heights_arr)
# TODO: В 2darray есть две оси, ось 0 и 1.
#  Ось 0 проходит вниз по строкам, тогда как ось 1 проходит горизонтально по столбцам.
#  Напомним, что в 2dary heights_and_ages_arr его размеры равны (2, 45).
#  Чтобы найти возраст Томаса Джефферсона в начале его президентства,
#  вам нужно получить доступ ко второй строке, где хранится возраст:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
# print(heights_and_ages_arr[1, 2])
# TODO: В 2darray строка — это ось 0, а столбец — ось 1,
#  поэтому для доступа к 2darray numpy сначала ищет позицию в строках, а затем в столбцах.
#  Итак, в нашем примере heights_and_ages_arr[1,2] мы обращаемся к строке 2 (возраст),
#  столбцу 3 (третий президент), чтобы найти возраст Томаса Джефферсона.

# TODO: Slicing (Нарезка)
#  Что, если мы хотим проверить первые три элемента из первой строки в 2darray?
#  Мы используем «:», чтобы выбрать все элементы из индекса до конечного индекса, но не включая его.
#  Это называется нарезка
# import numpy as np
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
# print(heights_and_ages_arr[0, 0:3])
# TODO: Когда начальный индекс равен 0, мы можем его опустить, как показано ниже:
# import numpy as np
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
# print(heights_and_ages_arr[0, :3])
# TODO: Что, если мы хотим увидеть весь четвертый столбец? Укажите это, используя «:», как показано ниже.
# import numpy as np
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
# print(heights_and_ages_arr[:, 3])
# TODO: Синтаксис срезов Numpy соответствует синтаксису списка Python: arr[start:stop:step].
#  Если какой-либо из них не указан, по умолчанию используются значения start=0, stop=размер измерения, step=1.

# TODO: Assigning Single Values (Присвоение отдельных значений)
#  Иногда вам нужно изменить значения определенных элементов в массиве.
#  Например, мы заметили, что четвертая запись в heights_arr была неверной,
#  она должна быть 165 вместо 163, мы можем переназначить правильный номер:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# heights_arr = np.array(heights)
# heights_arr[3] = 165
# print(heights_arr)
# TODO: В 2darray можно легко назначить отдельные значения.
#  Вы можете использовать индексацию для одного элемента.
#  Например, измените четвертую запись в heights_arr на 165:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
# heights_and_ages_arr[0, 3] = 165
# print(heights_and_ages_arr)
# TODO: Или мы можем использовать нарезку для нескольких элементов.
#  Например, чтобы заменить первую строку ее средним значением 180 в heights_and_ages_arr:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
#
# heights_and_ages_arr[0, :] = 180
# print(heights_and_ages_arr)
# TODO: Мы также можем комбинировать нарезку, чтобы изменить любое подмножество массива.
#  Например, чтобы переназначить 0 в левый верхний угол:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
#
# heights_and_ages_arr[:, :2] = 0
# print(heights_and_ages_arr)
# TODO: Легко обновлять значения в подмассиве, когда вы объединяете массивы с нарезкой.
#  Чтобы узнать больше о базовой нарезке и расширенной индексации в numpy, перейдите по этой ссылке:
#  https://numpy.org/doc/stable/reference/arrays.indexing.html

# TODO: ЗАДАЧА: Assigning Single Values (Присвоение отдельных значений)
#  Замените значение во второй строке и третьем столбце массива heights_and_ages_arr на 2.
# heights_and_ages_arr[1, 2] = 2

# TODO: Assigning an Array to an Array (Назначение массива массиву)
#  Кроме того, 1darray или 2darry могут быть назначены подмножеству другого 2darray, если их формы совпадают.
#  Вспомним 2darray heights_and_ages_arr:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
# print(heights_and_ages_arr)
# TODO: Если мы хотим обновить рост и возраст первого президента новыми данными,
#  мы можем предоставить данные в виде списка:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
#
# heights_and_ages_arr[:, 0] = [190, 58]
# print(heights_and_ages_arr)
# TODO: Мы также можем обновить данные в подмассиве с помощью массива numpy как такового:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_and_ages = heights + ages
# heights_and_ages_arr = np.array(heights_and_ages)
# heights_and_ages_arr = heights_and_ages_arr.reshape((2, 45))
#
# new_record = np.array([[180, 183, 190], [54, 50, 69]])
# heights_and_ages_arr[:, 42:] = new_record
# print(heights_and_ages_arr)
# TODO: Обратите внимание, что значения последних трех столбцов изменились.
#  Обновление многомерного массива новой записью в numpy выполняется просто, если их формы совпадают.

# TODO: Combining Two Arrays (Объединение двух массивов)
#  Часто мы получаем данные, хранящиеся в разных массивах,
#  и нам нужно объединить их в один, чтобы хранить в одном месте.
#  Например, вместо того, чтобы хранить возраст в списке, его можно сохранить в 2darray:
# import numpy as np
#
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# ages_arr = np.array(ages)
#
# print(ages_arr.shape)
# print(ages_arr[:3, ])
# TODO: Если мы изменим heights_arr на (45,1), то же самое, что и ages_arr,
#  мы можем сложить их горизонтально (по столбцам), чтобы получить 2darray с помощью 'hstack':
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_arr = np.array(heights)
# ages_arr = np.array(ages)
#
# heights_arr = heights_arr.reshape((45, 1))
# ages_arr = ages_arr.reshape((45, 1))
#
# height_age_arr = np.hstack((heights_arr, ages_arr))
# print(height_age_arr.shape)
# print(height_age_arr[:3, ])
# TODO: Теперь в height_age_arr есть и рост, и возраст президентов,
#  каждый столбец соответствует росту и возрасту одного президента.
#  Точно так же, если мы хотим объединить массивы по вертикали (по строкам), мы можем использовать «vstack».
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_arr = np.array(heights)
# ages_arr = np.array(ages)
#
# heights_arr = heights_arr.reshape((1, 45))
# ages_arr = ages_arr.reshape((1, 45))
#
# height_age_arr = np.vstack((heights_arr, ages_arr))
# print(height_age_arr.shape)
# print(height_age_arr[:, :3])
# TODO: Чтобы объединить более двух массивов по горизонтали, просто добавьте дополнительные массивы в кортеж.

# TODO: Concatenate (Объединить)
#  В более общем смысле мы можем использовать функцию numpy.concatenate.
#  Если мы хотим объединить, связать вместе два массива по строкам, то передайте «axis = 1»,
#  чтобы получить тот же результат, что и при использовании numpy.hstack; и передайте «axis = 0»,
#  если вы хотите объединить массивы по вертикали.
#  В примере из предыдущей части вместо этого мы использовали hstack для объединения двух массивов по горизонтали:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_arr = np.array(heights)
# ages_arr = np.array(ages)
#
# heights_arr = heights_arr.reshape((45, 1))
# ages_arr = ages_arr.reshape((45, 1))
#
# # height_age_arr = np.hstack((heights_arr, ages_arr))
# height_age_arr = np.concatenate((heights_arr, ages_arr), axis=1)
#
# print(height_age_arr.shape)
# print(height_age_arr[:3, :])
# TODO: Также вы можете получить тот же результат, что и при использовании vstack:
# import numpy as np
#
# heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#            193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#            182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
# ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#         55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
#
# heights_arr = np.array(heights)
# ages_arr = np.array(ages)
#
# heights_arr = heights_arr.reshape((1, 45))
# ages_arr = ages_arr.reshape((1, 45))
#
# # height_age_arr = np.vstack((heights_arr, ages_arr))
# height_age_arr = np.concatenate((heights_arr, ages_arr), axis=0)
#
# print(height_age_arr.shape)
# print(height_age_arr[:, :3])
# TODO: Вы можете использовать np.hstack для объединения массивов, ТОЛЬКО если они имеют одинаковое количество строк.

# TODO: Mathematical Operations on Arrays (Математические операции над массивами)
#  Выполнение математических операций над массивами очень просто.
#  Например, чтобы преобразовать высоту из сантиметров в футы, зная,
#  что 1 сантиметр равен 0,0328084 фута, мы можем использовать умножение:
# import numpy as np
#
# heights_arr = np.array(
#     [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#      193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#      182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
# ages_arr = np.array(
#     [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#      55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1, 1))
#
# heights_arr = heights_arr.reshape((45, 1))
# height_age_arr = np.hstack((heights_arr, ages_arr))
#
# print(height_age_arr[:, 0] * 0.0328084)
# TODO: Теперь у нас есть все высоты в футах. Обратите внимание, что эта операция не изменит исходный массив,
#  она вернет новый 1darray, где 0,0328084 было умножено на каждый элемент в первом столбце «heights_age_arr».
#  Другие математические операции сложения, вычитания,
#  деления и возведения в степень (+, -, /, **) работают с массивами точно так же.

# TODO: Numpy Array Method (Метод массива Numpy)
#  Кроме того, в numpy есть несколько методов для выполнения более сложных вычислений с массивами.
#  Например, метод sum() находит сумму всех элементов массива:
# import numpy as np
#
# heights_arr = np.array(
#     [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#      193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#      182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
# ages_arr = np.array(
#     [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#      55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1, 1))
#
# heights_arr = heights_arr.reshape((45, 1))
# height_age_arr = np.hstack((heights_arr, ages_arr))
#
# print(height_age_arr.sum())
# TODO: Сумма всех ростов и возрастов равна 10 575.
#  Чтобы суммировать все высоты и суммировать все возрасты по отдельности, мы можем указать ось = 0,
#  чтобы вычислить сумму по строкам, то есть вычислить сумму для каждого столбца или сумму столбца.
#  С другой стороны, чтобы получить суммы строк, укажите axis=1.
#  В этом примере мы хотим вычислить общую сумму роста и возраста соответственно:
# import numpy as np
#
# heights_arr = np.array(
#     [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#      193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#      182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
# ages_arr = np.array(
#     [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#      55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1, 1))
#
# heights_arr = heights_arr.reshape((45, 1))
# height_age_arr = np.hstack((heights_arr, ages_arr))
#
# print(height_age_arr.sum(axis=0))
# print(height_age_arr.min(axis=0))
# print(height_age_arr.max(axis=0))
# print(height_age_arr.mean(axis=0))
# TODO: Результатом являются суммы строк: рост всех президентов (т. е. первая строка) составляет в сумме 8100,
#  а сумма возрастов (т. е. вторая строка) равна 2475.
#  Другие операции, такие как .min(), .max(), .mean(), работают аналогично .sum().

# TODO: Comparisons (Сравнения)
#  Практикуя науку о данных, мы часто сталкиваемся со сравнениями для определения строк,
#  которые соответствуют определенным значениям.
#  Для этого мы можем использовать такие операции, как «<», «>», «>=», «<=» и «==».
#  Например, в наборе данных height_age_arr нас могут интересовать только те президенты,
#  которые начали свое президентство моложе 55 лет.
# TODO:
# import numpy as np
#
# heights_arr = np.array(
#     [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#      193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#      182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
# ages_arr = np.array(
#     [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#      55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1, 1))
#
# heights_arr = heights_arr.reshape((45, 1))
# height_age_arr = np.hstack((heights_arr, ages_arr))
#
# print(height_age_arr[:, 1] < 55)
# TODO: Результатом является 1darray с логическими значениями, указывающими, какие президенты соответствуют критериям.
#  Если нас интересует только то, какие президенты начали свое президентство в возрасте 51 года,
#  мы можем вместо этого использовать «==».
# import numpy as np
#
# heights_arr = np.array(
#     [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#      193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#      182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
# ages_arr = np.array(
#     [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#      55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1, 1))
#
# heights_arr = heights_arr.reshape((45, 1))
# height_age_arr = np.hstack((heights_arr, ages_arr))
#
# print(height_age_arr[:, 1] == 51)
# print(f'Удовлетворяет условию: {(height_age_arr[:, 1] == 51).sum()} строк')
# TODO: Чтобы узнать, сколько строк удовлетворяет условию, используйте .sum() в результирующем булевом массиве 1d,
#  например, (height_age_arr[:, 1] == 51).sum(), чтобы увидеть, что было ровно пять президентов,
#  которые начали президентство в возрасте 51 года. Истина рассматривается как 1, а Ложь - как 0 в сумме.

# TODO: Mask & Subsetting (Маска и подмножество)
#  Теперь, когда можно идентифицировать строки, соответствующие определенным критериям, можно найти подмножество данных.
#  Например, вместо всего набора данных нам нужны только высокие президенты, то есть те президенты,
#  чей рост больше или равен 182 см. Сначала мы создаем маску 1darray с логическими значениями:
# import numpy as np
#
# heights_arr = np.array(
#     [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#      193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#      182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
# ages_arr = np.array(
#     [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#      55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1, 1))
#
# heights_arr = heights_arr.reshape((45, 1))
# height_age_arr = np.hstack((heights_arr, ages_arr))
#
# mask = height_age_arr[:, 0] >= 182
# print(mask.sum())
# TODO: Затем передайте его на первую ось `height_age_arr`,
#  чтобы отфильтровать президентов, которые не соответствуют критериям:
# import numpy as np
#
# heights_arr = np.array(
#     [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#      193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#      182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
# ages_arr = np.array(
#     [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#      55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1, 1))
#
# heights_arr = heights_arr.reshape((45, 1))
# height_age_arr = np.hstack((heights_arr, ages_arr))
#
# mask = height_age_arr[:, 0] >= 182
# tall_presidents = height_age_arr[mask]
# print(tall_presidents.shape)
# # print(tall_presidents)
# TODO: Это подмассив height_age_arr, и все президенты в high_presidents были ростом не менее 182 см.
#  Маскирование используется для извлечения, изменения, подсчета или иного манипулирования значениями
#  в массиве на основе некоторого критерия. В нашем примере критерием был рост 182 см и выше.

# TODO: Multiple Criteria (Несколько критериев)
#  Мы можем создать маску, удовлетворяющую более чем одному критерию.
#  Например, помимо роста, мы хотим найти тех президентов,
#  которым на момент начала президентства было 50 лет или меньше.
#  Для этого мы используем & для разделения условий,
#  и каждое условие заключено в круглые скобки «()», как показано ниже:
# import numpy as np
#
# heights_arr = np.array(
#     [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183,
#      193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178,
#      182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
# ages_arr = np.array(
#     [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55,
#      55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1, 1))
#
# heights_arr = heights_arr.reshape((45, 1))
# height_age_arr = np.hstack((heights_arr, ages_arr))
#
# mask = (height_age_arr[:, 0] >= 182) & (height_age_arr[:, 1] <= 50)
#
# print(height_age_arr[mask])
# TODO: Результаты показывают нам, что есть четыре президента, которые удовлетворяют обоим условиям.
#  Манипуляции с данными в Python почти синонимичны манипулированию массивами Numpy.
#  Операции, показанные здесь, являются строительными блоками многих других примеров,
#  используемых в этом курсе. Их важно освоить!

# TODO: ЗАДАЧА: Data Science - Average of Rows (Наука о данных — среднее количество строк)
#  В матрице или двумерном массиве X средние значения (or means)
#  элементов строк называются средними значениями строк.
#  Задача:
#  Для заданного двумерного массива вернуть средние значения строк.
#  Формат ввода Первая строка: два целых числа, разделенные пробелами,
#  первое указывает на строки матрицы X (n), а второе указывает на столбцы X (p)
#  Следующие n строк: значения строки в X
#  Output Format:
#  Пустой массив 1d из значения округляются до второго десятичного знака.
#  2 2
#  1.5 1
#  2 2.9
#  Sample Output:
#  [1.25 2.45]
#  Output Format:
#  3 3
#  1.5 3.8 2
#  1.5 2.2 7.7
#  2.2 3.3 4.4
#  Sample Output:
#  [2.43 3.8  3.3 ]
#  Пояснение:
#  В первой строке есть два числа 1.5 и 1, поэтому сумма равна 1.5 + 1 = 2.5,
#  а среднее значение равно 2.5/2 = 1.25.
#  Затем для второй строки вычисляется среднее значение как (2 + 2.9)/2 = 4.9/2 = 2.45.
# import numpy as np
#
# row, col = [int(x) for x in input().split()]
#
# list = [float(j) for i in range(row) for j in input().split()]
# array = np.array(list).reshape((row, col))
#
# print(array.mean(axis=1).round(2))

# import numpy as np
#
# n, p = [int(x) for x in input().split()]  # он принимает первые два ввода n и p
# x = []  # массив для списка
#
# for i in range(n):  # ввод данных для каждой строки
#     x.append(input().split())  # ввод и разбиение данных на ввод по столбцам
#
# arr = np.array(x)  # создание этого массива numpy
# arr = arr.astype(np.float16)  # преобразование типа данных в float
#
# print(arr.mean(axis=1).round(2))   # имеющее среднее значение по строкам с осью = 1

# TODO: Pandas vs. Numpy (Панды против Нампи)
#  Что, если мы хотим проверить данные об Аврааме Линкольне в 'height_age_arr',
#  но не можем вспомнить его целочисленное положение.
#  Есть ли удобный способ получить доступ к данным, проиндексировав имя президента,
#  например:
# print(height_age_arr['Abraham Lincoln'])
# TODO: К сожалению, мы получим сообщение об ошибке. Однако это можно сделать в пандах.
#  Библиотека pandas построена на основе numpy, что означает, что многие features (характеристики),
#  методы и функции являются общими. По соглашению импортируйте библиотеку под коротким именем «pd»:
# import pandas as pd
# TODO: В следующих уроках мы увидим, что pandas позволяет нам получать доступ к данным, напрямую индексируя имя.
#  Поскольку numpy ndarrays однородны, pandas ослабляет это требование
#  и допускает различные dtypes в своих структурах данных.

# TODO: Series Серии
#  Серия — это один из строительных блоков в пандах.
#  Pandas Series — это одномерный помеченный массив, который может содержать данные любого типа
#  (целое число, строка, число с плавающей запятой, объекты Python и т.д.), подобно столбцу в электронной таблице Excel.
#  Метки осей вместе называются индексом. Если нам дадут мешок с буквами a, b и c, и мы посчитаем,
#  сколько букв каждой из них у нас есть, мы обнаружим, что есть 1 a, 2 b и 3 c.
#  Мы могли бы создать серию, указав список счетчиков и соответствующие им метки:
# import pandas as pd
#
# print(pd.Series([1, 2, 3], index=['a', 'b', 'c']))  # with index
# # TODO: В качестве альтернативы значения могут быть массивом numpy:
# import numpy as np
# import pandas as pd
#
# print(pd.Series(np.array([1, 2, 3]), index=['a', 'b', 'c']))  # from a 1darray
# TODO: Или мы могли бы использовать словарь для указания индекса с ключами:
# import pandas as pd
#
# print(pd.Series({'a': 1, 'b': 2, 'c': 3}))  # from a dict
# TODO: Если мы не укажем индекс, по умолчанию индексом будут целые позиции, начинающиеся с 0.
#  В серии мы можем получить доступ к значению напрямую по его индексу:
# import pandas as pd
#
# series = pd.Series({'a': 1, 'b': 2, 'c': 3})
# print(series['a'])
# print(series[:])
# TODO: Доступ к значению по его индексу, а не по целочисленной позиции, удобен,
#  когда набор данных состоит из тысяч, если не миллионов строк.
#  Series — это строительный блок для DataFrame, который мы представим далее.
#  Думайте о Series как о пустом 1darray с именами индексов или строк.

# TODO: DataFrames (кадры данных)
#  В науке о данных данные обычно более чем одномерны и относятся к разным типам данных;
#  таким образом, Серии недостаточно. DataFrames — это 2darrays с метками строк и столбцов.
#  Один из способов создать DataFrame с нуля — передать dict.
#  Например, на этой неделе мы продали 3 бутылки красного вина Адаму, 6 — Бобу и 5 — Чарльзу.
#  Мы продали 5 бутылок белого вина Адаму, 0 бутылок Бобу и 10 бутылок Чарльзу.
#  Мы можем организовать данные в DataFrame, создав dict «wine_dict» с количеством бутылок каждого типа вина, которое
#  мы продали, а затем передать его вместе с именами клиентов в качестве индекса для создания DataFrame «продажи».
#  См. фото: wine_dict.jpg
# import pandas as pd
#
# wine_dict = {
#     'red_wine': [3, 6, 5],
#     'white_wine': [5, 0, 10]
# }
# sales = pd.DataFrame(wine_dict, index=["adam", "bob", "charles"])
# print(sales)
# TODO: Думайте о DataFrame как о коллекции Series.
#  Здесь продажи состоят из двух серий, одна из которых называется «red_wine»,
#  а другая — «white_wine», поэтому мы можем получить доступ к каждой серии, вызвав ее имя:
# import pandas as pd
#
# wine_dict = {
#     'red_wine': [3, 6, 5],
#     'white_wine': [5, 0, 10]
# }
# sales = pd.DataFrame(wine_dict, index=["adam", "bob", "charles"])
# print(sales['white_wine'])
# TODO: Мы увидим другие способы индексации в DataFrames в последующих частях.
#  Если мы не укажем индекс, DataFrame сгенерирует целочисленный индекс, начиная с 0.

# TODO: Inspect a DataFrame - Shape and Size (Проверка DataFrame — форма и размер)
#  Давайте взглянем на новый DataFrame, там помимо роста и возраста президентов есть информация о порядке,
#  именах и партиях. DataFrame Presidents_df считывается из CSV-файла следующим образом.
#  Обратите внимание, что index установлен для имен президентов.
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
# TODO: Подобно numpy, чтобы получить размеры DataFrame, используйте .shape
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.shape)
# TODO: В этом DataFrame 45 строк и 4 столбца.
#  Чтобы получить количество строк, мы можем получить доступ к первому элементу в кортеже.
#  Чтобы получить количество столбцов, мы можем получить доступ ко второму элементу в кортеже.
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.shape[0])
# print(presidents_df.shape[1])
# # print(presidents_df)
# TODO: Size также работает с DataFrame, возвращая целое число, представляющее количество элементов в этом объекте.
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.size)
# TODO: Здесь оба метода. .shape и .size работают так же, как и с numpy ndarrays.

# TODO: Inspect a DataFrame - Head and Tail (Осмотрите DataFrame — голова и хвост)
#  Вместо того, чтобы смотреть на весь набор данных, мы можем просто взглянуть.
#  Чтобы увидеть первые несколько строк в DataFrame, используйте .head() ;
#  если мы не указываем n (количество строк), по умолчанию отображаются первые пять строк.
#  Здесь мы хотим увидеть верхние 3 строки.
#  См. фото: presidents_df_head.jpg
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.head(n=3))
# TODO: В Presidents_df индексом является имя президента, всего четыре столбца: порядок, возраст, рост и партия.
#  Точно так же, если мы хотим увидеть последние несколько строк,
#  мы можем использовать .tail(), по умолчанию также пять строк.
#  См. фото: presidents_df_tail.jpg
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.tail())
# TODO: Функция .head полезна для быстрой проверки наличия в объекте данных правильного типа.

# TODO: Inspect a DataFrame - Info (Проверка DataFrame — информация)
#  Используйте .info(), чтобы получить обзор DataFrame.
#  Его выходные данные включают индекс, имена столбцов, количество ненулевых значений, dtypes и использование памяти.
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# presidents_df.info()
# TODO: Тип dtype для порядка, возраста и роста — целые числа, а партия — объект.
#  Количество ненулевых значений в каждом столбце совпадает с количеством строк,
#  что указывает на отсутствие пропущенных значений.
#  В дополнение к форме и размеру, как показано в numpy,
#  функции pandas предоставляют дополнительные функции для изучения данных.

# TODO: Rows with .loc (Строки с .loc)
#  Вместо того, чтобы запоминать целочисленные позиции для определения
#  порядка, возраста, роста и партии Авраама Линкольна,
#  с помощью DataFrame мы можем получить к нему доступ по имени, используя .loc
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.loc['Abraham Lincoln'])
# # TODO: Результатом является серия панд формы (4,).
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(type(presidents_df.loc['Abraham Lincoln']))
# print(presidents_df.loc['Abraham Lincoln'].shape)
# TODO: Мы также можем разрезать по индексу.
#  Скажем, мы заинтересованы в сборе информации обо всех президентах от Авраама Линкольна до Улисса С. Гранта:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.loc['Abraham Lincoln':'Ulysses S. Grant'])
# TODO: Результатом является новый DataFrame, подмножество 'presidents_df'.
#  .loc[] позволяет нам выбирать данные по метке или по условному оператору.

# TODO: Rows with .loc (Строки с .loc)
#  В качестве альтернативы, если мы знаем целочисленную позицию (позиции),
#  мы можем использовать .iloc для доступа к строке (строкам).
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.iloc[44])
# TODO: Чтобы собрать информацию от 16-го по 18-й президентов, мы можем:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.iloc[15:18])
# TODO: И .loc[], и .iloc[] могут использоваться с булевым массивом для подмножества данных.

# TODO: Columns (Столбцы)
#  Мы можем получить весь столбец из Presidents_df по имени.
#  Сначала мы получаем доступ ко всем именам столбцов:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.columns)
# TODO: Который возвращает объект индекса, содержащий все имена столбцов.
#  Затем мы можем получить доступ к столбцу 'height':
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df['height'])
# print(presidents_df['height'].shape)
# TODO: Который возвращает серию, содержащую рост всех президентов США.
#  Чтобы выбрать несколько столбцов, мы передаем имена в списке, в результате чего создается DataFrame.
#  Помните, мы можем использовать .head() для доступа к первым трем строкам, как показано ниже:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df[['height', 'age']].head(n=3))
# TODO: Таким образом, мы фокусируемся только на интересующих столбцах.
#  При доступе к одному столбцу одна скобка приводит к ряду (одномерному),
#  а двойные скобки приводят к DataFrame (многомерному).

# TODO: More with .loc (Больше о .loc)
#  Если мы хотим получить доступ к столбцу 'order', 'age' и 'height',
#  мы можем сделать это с помощью .loc . .loc позволяет нам получить доступ к любому из столбцов.
#  Например, если мы хотим получить доступ к столбцам от порядка до роста для первых трех президентов:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.loc[:, 'order':'height'].head(n=3))
# TODO: Индекс в pandas делает извлечение информации из строк или столбцов удобным и простым,
#  особенно когда набор данных большой или столбцов много.
#  Поэтому нам не нужно запоминать целые позиции каждой строки или столбца.

# TODO: Min / Max / Mean (Мин./макс./среднее)
#  Нецелесообразно распечатывать весь набор данных с большим размером выборки.
#  Вместо этого мы хотим обобщить и охарактеризовать выборочные данные, используя только несколько значений.
#  Сводная статистика включает меры местоположения и меры распространения.
#  Меры местоположения — это величины, которые представляют собой среднее значение переменной,
#  а меры разброса показывают, насколько схожи или различны значения переменной.
#  Меры местоположения — минимум, максимум, среднее. Меры разброса — диапазон, дисперсия, стандартное отклонение.
#  Простейшие сводные статистические данные, которые являются мерами местоположения,
#  включают минимальное, наименьшее число:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.min())
# TODO: максимум, наибольшее число:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.max())
# # TODO: и mean, среднее:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.mean())
# print(presidents_df['age'].mean())
# print(presidents_df['height'].mean())
# TODO: Напомним, что среднее арифметическое — это сумма элементов, деленная на количество элементов,
#  в python 3.x деление целых чисел приводит к числу с плавающей запятой. Когда минимум и максимум известны,
#  мы можем определить диапазон, меру распространения. Например, рост всех президентов США колеблется от 163 до 193 см.
#  Среднее значение говорит нам, где сосредоточены данные.
#  Например, средний возраст в начале президентского срока составляет 54,71 года.
#  Обратите внимание, что метод mean() может работать только с числовыми значениями, поэтому столбец «party» был опущен.
#  Эти методы работают и с сериями.
#  Например, 'presidents_df['age'].mean()' также дает результат 54,71.

# TODO: Quantiles (квантили)
#  Квантиль - это точка отсечения, которая делит диапазон данных
#  на непрерывные интервалы с равным количеством наблюдений.
#  Медиана — это единственная точка отсечения в 2-квантили,
#  так что 50% данных ниже медианы, а другая половина выше нее.
#  Квартили позволяют быстро разделить набор данных на четыре группы,
#  что позволяет легко увидеть, к какой из четырех групп относится конкретная точка данных.
#  первый квартиль (первые 25% данных),
#  следующий квартиль - 25% данных между первым квартилем и медианой,
#  следующий квартиль - 25% данных между медианой и третьим квартилем,
#  последний квартиль - 25% данных между третьим квартилем и максимумом.
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df['height'].quantile([0.25, 0.5, 0.75, 1]))
# print(presidents_df[['order', 'age', 'height']].quantile([0.25, 0.5, 0.75, 1]))
# TODO: Здесь 25% президентов начали свое президентство в возрасте 51 года или моложе,
#  а половина — в возрасте 55 лет или моложе.
#  Среднее значение и медиана обычно не совпадают,
#  если только данные не являются идеально симметричными.
#  Среднее значение — это среднее значение всех чисел, сложенных вместе и разделенное на количество добавленных чисел.
#  Медиана — это значение, отделяющее верхнюю половину от нижней половины выборки данных.
#  В возрастных данных среднее значение близко к медиане, это означает, что данные могут быть симметричными.
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df[['age', 'height']].mean())
# print(presidents_df[['age', 'height']].median())
# print(presidents_df[['age', 'height']].quantile(0.5))
# TODO: И .quantile(0.5), и .median() приводят к одному и тому же результату.

# import pandas as pd
#
# a = pd.Series([1, 2, 4, 2, 4, 1])
# print(a.median())
# print(a.quantile(0.5))

# TODO: Variance and Standard Deviation (Дисперсия и стандартное отклонение)
#  В теории вероятностей и статистике дисперсия - это среднеквадратичное отклонение каждой точки данных
#  от среднего значения всего набора данных.
#  Вы можете думать об этом как о том, насколько далеко набор чисел разбросан от их среднего значения.
#  Стандартное отклонение (std) представляет собой квадратный корень из дисперсии.
#  Высокое стандартное значение указывает на большой спред,
#  а низкое стандартное значение указывает на малый спред или на то,
#  что большинство точек близко к среднему значению.
#  В примере ниже, данные состоят из всех констант 2, вариации нет,
#  поэтому Дисперсия равна 0,0, так же как и ее стандартное отклонение:
# import pandas as pd
#
# const = pd.Series([2, 2, 2])
#
# print(dat.mean())
# print(const.var())
# print(const.std())
# TODO: Рассмотрим другой пример:
# [2, 3, 4]
# TODO: Среднее значение [2,3,4] равно (2+3+4)/3 = 3.0,
#  а его изменение равно (2-3.0)**2 + (3-3.0)**2 + (4-3.0)**2 = 1+0+1 = 2.
#  Обратите внимание, что в Python .var() вернет дисперсию, деленную на N-1,
#  где N — длина данных, тогда результат будет 2/(3.0-1) = 1.
# import pandas as pd
#
# dat = pd.Series([2, 3, 4])
#
# print(dat.mean())
# print(dat.var())
# print(dat.std())
# TODO: И std - это просто квадратный корень из дисперсии:
# import pandas as pd
#
# dat = pd.Series([-4, 3, 10])
# # dat = pd.Series([-50, 0, 50])
#
# print(dat.mean())
# print(dat.var())
# print(dat.std())
# TODO: Для возрастов президентов:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df['age'].mean())
# print(presidents_df['age'].var())
# print(presidents_df['age'].std())
# TODO: Мы можем применить std ко всему DataFrame, чтобы получить стандартное отклонение по столбцам.
#  Для того, что бы убрать ошибку Python при выводе в консоль: numeric_only=True
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.mean(numeric_only=True))
# print(presidents_df.var(numeric_only=True))
# print(presidents_df.std(numeric_only=True))
#
# print(presidents_df.min(numeric_only=True))
# print(presidents_df.max(numeric_only=True))
# print(presidents_df.quantile([0.25, 0.5, 0.75, 1.0], numeric_only=True))
# TODO: Таким же образом мы можем применить min, max, quantile и var ко всему DataFrame.

# TODO: describe() (описывать())
#  describe() выводит почти всю сводную статистику, упомянутую ранее, за исключением дисперсии.
#  Кроме того, он подсчитывает все ненулевые значения каждого столбца.
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df['age'].describe())
# print(presidents_df.describe())
# TODO: Из вывода мы видим, что существует 45 ненулевых точек данных
#  о возрасте со средним значением 55 и стандартным значением 6,60.
#  Возраст варьируется от 42 до 70 лет со средним значением 55 лет.
#  Его первый и третий квартили составляют 51 и 58 лет соответственно.
#  Теперь у нас есть общее описание всех возрастных данных.
#  В дополнение к применению к ряду, describe() можно применить к DataFrame с несколькими столбцами.
#  Как показывает количество (= 45), ни в одном из трех столбцов нет нулевых значений.
#  order — это просто индекс от 1 до 45. Интересно, что и возраст, и рост
#  лежат в интервале примерно одинаковой длины: 70–42 = 28 для возраста, а 193–163 = 30 для роста.
#  Кроме того, обе функции имеют аналогичные стандартные отклонения, что указывает на аналогичный разброс данных.
#  .describe() игнорирует нулевые значения, такие как `NaN` (не число), и генерирует описательную статистику,
#  которая обобщает центральную тенденцию (т.е. среднее значение),
#  дисперсию (т.е. стандартное отклонение) и форму (т.е. мин., max и квантили) распределения набора данных.

# TODO: Categorical Variable (Категориальная переменная)
#  Четвертый столбец party был опущен в выводе .describe(), потому что это категориальная переменная.
#  Категориальная переменная — это та, которая принимает одно значение из ограниченного набора категорий.
#  Не имеет смысла рассчитывать среднее значение демократических, республиканских, федералистских и других партий.
#  Мы можем проверить уникальные значения и соответствующую частоту, используя .value_counts():
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df['party'].value_counts())
# TODO: Мы также можем вызвать .describe(), чтобы увидеть,
#  что существует 45 ненулевых значений, 7 уникальных партий,
#  наиболее часто встречающаяся партия — республиканская, всего 19 президентов принадлежат к этой партии.
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df['party'].describe())
# TODO: Сводная статистика предоставляет нам большой объем информации, изложенной максимально просто.
#  Мера местоположения, медиана, является более надежной, чем среднее, для непрерывных переменных,
#  поскольку последнее чувствительно к выбросам, например, чрезвычайно большим значениям.

# TODO: Groupby (Сгруппировать по)
#  Сводная статистика по всему набору данных обеспечивает хорошее общее представление,
#  но часто нас интересуют некоторые вычисления, зависящие от данной метки или категории.
#  Например, каков средний рост условной партии президентов? Чтобы найти значение на основе условия,
#  мы можем использовать операцию groupby. Подумайте о группе, выполнив три шага: разделить, применить и объединить.
#  1 шаг: разделение - разбивает DataFrame на несколько DataFrame на основе значения указанного ключа;
#  2 шаг: применение - заключается в выполнении операции внутри каждого меньшего DataFrame;
#  3 шаг: объединяет части обратно в более крупный DataFrame.
# presidents_df.groupby('party')
# TODO: .groupby("party") возвращает объект DataFrameGroupBy, а не набор DataFrames.
#  Чтобы получить результат, примените агрегат .mean()) к этому объекту DataFrameGroupBy:
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.groupby('party').median())
# print(presidents_df.groupby('party').quantile([0.5], numeric_only=True))
# print(presidents_df.groupby('party').mean())
# TODO: Метод mean() является одной из многих возможностей, вы можете применить любую функцию агрегации pandas
#  или numpy или любую операцию DataFrame, как мы демонстрируем в этом курсе.

# TODO: Aggregation (Агрегация)
#  Мы также можем выполнять несколько операций над объектом groupby, используя метод .agg().
#  Он принимает строку, характеристику или их список. Например, мы хотели бы получить минимальное,
#  медианное и максимальное значения высоты, сгруппированные по партиям:
# import pandas as pd
# import numpy as np
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.groupby('party')['height'].agg([min, np.median, 'max']))
# # print(presidents_df.groupby('party')['height'].quantile([0, 0.5, 1.0], numeric_only=True))
# TODO: Из вывода мы видим, что рост демократических президентов колеблется от 168 см до 193 см, при медиане 180 см.
#  Часто нас интересуют разные сводные статистические данные для нескольких столбцов.
#  Например, мы хотели бы проверить медиану и среднее значение роста,
#  а также минимальное и максимальное значение для возрастов, сгруппированных по партиям.
#  В этом случае мы можем передать dict с ключом - указывающим имя столбца, и значением - указывающим характеристики:
# import pandas as pd
# import numpy as np
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df.groupby('party').agg({'height': [np.median, np.mean], 'age': [min, max]}).round())
# TODO: Использование groupby и agg дает нам гибкость и, следовательно,
#  возможность рассматривать различные точки зрения на переменную или столбец, обусловленные категориями.

# TODO: ЗАДАЧА: Data Science - Reshape (Наука о данных — изменение формы)
#  Имея список чисел и количество строк (r), преобразовать список в двумерный массив.
#  Обратите внимание, что r делит длину списка поровну.
#  Формат ввода Первая строка: целое число (r), указывающее количество строк двумерного массива
#  Следующая строка: числа, разделенные пробелом
#  Формат вывода Пустой двумерный массив значений, округленных до второго десятичного знака.
#  Sample Input:
#  2
#  1.2 0 0.5 -1
#  Sample Output:
#  [[ 1.2 0. ]
#  [ 0.5 -1. ]]
#  Объяснение:
#  Требуемое количество строк равно 2, и нам дан список из 4 чисел; в результате массив 2d должен быть 2 x 2.
#  Таким образом, первая строка - это первые два числа, а вторая строка содержит следующие два числа в данном списке.
# import numpy as np
#
# r = int(input())
# lst = [float(x) for x in input().split()]
# arr = np.array(lst).reshape(r, int(len(lst) / r))
# print(arr.round(2))

# TODO: Matplotlib (Матплотлиб)
#  "Одна картинка стоит тысячи слов" - звучит правдоподобно в науке о данных.
#  Визуализация данных может выявлять неочевидные закономерности и более эффективно передавать информацию.
#  В этой части мы рассмотрим библиотеку Matplotlib, один из самых популярных инструментов визуализации данных,
#  работающий с массивами numpy, а также с сериями pandas и DataFrames.
#  По соглашению мы импортируем библиотеку под более коротким именем:
# import matplotlib as mpl
# TODO: В частности, для остальной части курса будет использоваться модуль matplotlib.pyplot,
#  набор командных функций, которые заставляют matplotlib работать как MATLAB:
# import matplotlib.pyplot as plt
# TODO: Стиль можно изменить с классического на ggplot, имитируя эстетический стиль, используемый в пакете R ggplot2.
# plt.style.use('ggplot')
# TODO: matplotlib.pyplot — это набор функций, которые позволяют строить графики в Python так же, как в MATLAB.
#  Каждая функция вносит некоторые изменения в фигуру, например, создает фигуру, создает область построения на фигуре,
#  рисует линии, аннотирует графики метками и т. д., как мы увидим в следующих уроках.

# TODO: Basics (Основы)
#  Для всех графиков matplotlib сначала создайте фигуру и объект осей; чтобы показать график, вызовите «plt.show()».
#  Фигура содержит все объекты, включая оси, графику, тексты и метки.
#  Оси представляют собой ограничивающую рамку с галочками и метками. Думайте об осях как об отдельном сюжете.
# import matplotlib.pyplot as plt
#
# plt.style.use('ggplot')
# fig = plt.figure()
# ax = plt.axes()
# plt.savefig('fig.png')
# plt.show()
# TODO: matplotlib.pyplot предоставляет множество функций настройки, как мы увидим в следующих уроках.

# TODO: Line Plot (Линейный график)
#  Давайте начнем с красивой волновой функции, синусоидальной функции, sin(x), где x находится в диапазоне от 0 до 10.
#  Нам нужно сгенерировать последовательность вдоль оси x, равномерно распределенный массив, через linspace()
# import numpy as np
#
# x = np.linspace(0, 10, 1000)  # x - это 1000 равномерно расположенных чисел от 0 до 10
# TODO: Вторая строка генерирует равномерно расположенную последовательность из 1000 чисел от 0 до 10. Вы можете видеть,
#  как приливы поднимаются и опускаются с течением времени, а высота приливов определяется функцией sin.
# y = np.sin(x)
# TODO: Для построения, как и прежде, мы сначала создаем фигуру и объекты осей.
# import matplotlib.pyplot as plt
#
# fig = plt.figure()
# ax = plt.axes()
# TODO: Теперь мы делаем график непосредственно из осей, "ax.plot()"; по умолчанию он генерирует объект Line2D.
#  Чтобы показать график, нам нужно вызвать show().
# import numpy as np
# import matplotlib.pyplot as plt
#
# plt.style.use('ggplot')
# x = np.linspace(0, 10, 1000)  # x - это 1000 равномерно расположенных чисел от 0 до 10
# y = np.sin(x)
# fig = plt.figure()
# ax = plt.axes()
# ax.plot(x, y)
# plt.savefig("plot.png")
# plt.show()
# TODO: В качестве альтернативы мы можем использовать интерфейс pylab
#  и позволить фигуре и осям создаваться для нас в фоновом режиме.
# import numpy as np
# import matplotlib.pyplot as plt
#
# plt.style.use('ggplot')
# x = np.linspace(0, 10, 1000)  # x - это 1000 равномерно расположенных чисел от 0 до 10
# y = np.sin(x)
# fig = plt.figure()  # fig - можно тоже убрать как и оси (фигура и оси создадутся в фоновом режиме)
# plt.plot(x, y)
# plt.savefig("plot.png")
# plt.show()
# TODO: Линейный график отображает данные вдоль числовой линии, что является полезным инструментом
#  для отслеживания изменений за короткие и длительные периоды времени.

# TODO: Labels and Titles (Ярлыки и названия)
#  Одним из важнейших компонентов каждой фигуры является название фигуры.
#  Задача заголовка — точно сообщить, о чем идет речь.
#  Кроме того, для осей нужны заголовки или, что чаще называют, метки осей.
#  Метки осей поясняют, что представляют собой нанесенные на график значения данных.
#  Мы можем указать метки осей x, y и заголовок, используя:
#  - plt.xlabel()
#  - plt.ylabel()
#  - plt.title()
# import numpy as np
# import matplotlib.pyplot as plt
#
# plt.style.use('ggplot')
# x = np.linspace(0, 10, 1000)  # 1d массив длиной 1000
# y = np.sin(x)
#
# plt.plot(x, y)
# # plt.xlim(0, 7)
# # plt.ylim(-2, 2)
# plt.xlabel('Ось X')
# plt.ylabel('Ось Y')
# plt.title('Функция sin(x)')
# plt.savefig("plot.png")
# plt.show()
# TODO: Также можно установить пределы осей x, y - используя:
#  - plt.xlim()
#  - plt.ylim()

# TODO: Multiple Lines (Несколько линий)
#  Обычно существуют различные наборы данных схожего характера, и мы хотели бы сравнить их и увидеть различия.
#  Мы можем нанести несколько линий на одну и ту же фигуру.
#  Скажем, функция sin фиксирует приливы на восточном побережье,
#  а функция cos одновременно фиксирует приливы на западном побережье,
#  мы можем изобразить их обе на одном рисунке, вызвав функцию .plot() несколько раз.
# import numpy as np
# import matplotlib.pyplot as plt
#
# plt.style.use('ggplot')
# x = np.linspace(0, 10, 1000)  # 1d массив длиной 1000
# # y = np.sin(x)
# plt.plot(x, np.sin(x))
# plt.plot(x, np.cos(x))
# plt.savefig("plot.png")
# plt.show()
# TODO: Для различения линий можно указать цвета и стили линий:
# import numpy as np
# import matplotlib.pyplot as plt
#
# plt.style.use('ggplot')
# x = np.linspace(0, 10, 1000)  # 1d массив длиной 1000
# plt.plot(x, np.sin(x), color='k')
# plt.plot(x, np.cos(x), color='r', linestyle='--')
# plt.savefig("plot.png")
# plt.show()
# TODO: Обратите внимание, что мы указали основные цвета, используя одну букву, то есть k для черного и r для красного.
#  Еще примеры включают b для синего, g для зеленого, c для голубого и т. д.
#  Подробнее об использовании цветов в matplotlib см. в его документации .
#  Иногда бывает полезно визуально сравнить различные наборы данных на одном и том же рисунке,
#  и matplotlib упрощает эту задачу!

# TODO: Legend (Легенда)
#  При наличии нескольких линий на одной оси часто бывает полезно создать легенду графика, обозначающую каждую линию.
#  Мы можем использовать метод plt.legend() в сочетании с указанием меток в plt.plot().
# import numpy as np
# import matplotlib.pyplot as plt
#
# plt.style.use('ggplot')
# x = np.linspace(0, 10, 1000)  # 1d массив длиной 1000
# y = np.sin(x)
# plt.plot(x, np.sin(x), 'k:', label='sin(x)')
# plt.plot(x, np.cos(x), 'r--', label='cos(x)')
# plt.legend()
# plt.savefig("plot.png")
# plt.show()
# TODO: Обратите внимание, здесь мы используем «k:», чтобы указать,
#  что линия функции sin должна быть черной (обозначается k) и пунктирной (обозначается :).
#  Стиль линии и цветовые коды можно объединить в один аргумент, не являющийся ключевым словом, в функции plt.plot().
#  Цвет, стиль линии (например, сплошная, пунктирная и т.д.),
#  а также положение, размер и стиль меток можно изменить с помощью необязательных аргументов функции.
#  Для получения более подробной информации проверьте строку документации каждой функции и документацию matplotlib.
#  https://matplotlib.org/3.1.3/tutorials/index.html#colors

# TODO: Scatter Plot (Точечная диаграмма)
#  Другим простым типом графика является точечный график, вместо того, чтобы соединять точки отрезками,
#  точки представлены по отдельности точкой или другой формой.
#  Передайте 'o' в plt.plot() или используйте plt.scatter(), чтобы показать соотношение роста и возраста:
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
# plt.style.use('ggplot')
# plt.scatter(presidents_df['height'], presidents_df['age'])
# plt.savefig("plot.png")
# plt.show()
# TODO: Поэтому мы создали точечный график, демонстрирующий корреляцию между ростом и возрастом президентов.
#  Обратите внимание, что Matplotlib выводит для нас соответствующие диапазоны из данных как по оси x, так и по оси y.
#  По умолчанию каждая точка данных представляет собой полный круг, и имеется хороший набор других фигур.
#  Например: мы можем передать '<', чтобы нарисовать треугольник,
#  указывающий влево, кроме того, мы указываем синий цвет:
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
# plt.style.use('ggplot')
# plt.scatter(presidents_df['height'], presidents_df['age'], marker='<', color='b')
# plt.xlabel('height')
# plt.ylabel('age')
# plt.title('U.S. presidents')
# plt.savefig("plot.png")
# plt.show()
# TODO: Полный список символов см. в документации по matplotlib:
#  https://matplotlib.org/stable/api/markers_api.html
#  Точечная диаграмма - полезный инструмент для отображения взаимосвязи между двумя функциями;
#  тогда как линейный график уделяет больше внимания изменению между точками,
#  поскольку его наклон отображает скорость изменения.

# TODO: Plotting with Pandas (Графика с пандами)
#  Отличительной особенностью pandas является то, что он хорошо интегрируется с matplotlib,
#  поэтому мы можем строить графики непосредственно из DataFrames и Series.
#  Мы указываем тип графика как «scatter», «height» по оси x и «age» по оси y, а затем даем ему название:
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
# plt.style.use('ggplot')
# presidents_df.plot(kind='scatter',
#                    x='height',
#                    y='age',
#                    title='U.S. presidents')
# plt.savefig("plot.png")
# plt.show()
# TODO: Затем мы создали точечную диаграмму из DataFrame с метками осей и заголовком.
#  Поскольку мы указали оси x, y - с именами столбцов из DataFrame, метки также были аннотированы на осях.

# TODO: Histogram (Гистограмма)
#  Гистограмма — это диаграмма, состоящая из прямоугольников, ширина которых равна интервалу,
#  а площадь пропорциональна частоте изменения переменной.
#  Например, на приведенной ниже гистограмме имеется пять бинов одинаковой длины
#  ([163, 169), [169, 175), [175, 181), [181, 187) и [187, 193)), то есть: 3 президента ростом от 163 до 169 см и т.д.
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
# plt.style.use('ggplot')
# presidents_df['height'].plot(kind='hist',
#                              title='height',
#                              bins=5)
# plt.savefig("plot.png")
# plt.show()
# TODO: plt.hist(presidents_df['height'], bins=5) создаст тот же график, что и выше.
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
# plt.style.use('ggplot')
# plt.hist(presidents_df['height'], bins=5, color='b')
# plt.savefig("plot.png")
# plt.show()
# TODO: Кроме того, plt.hist() выводит 1darray с частотой и конечными точками бина.
#  Здесь мы видим, что это распределение с асимметрией влево
#  (хвост распределения с левой стороны длиннее, чем с правой),
#  что указывает на то, что среднее значение (180,0 см) немного ниже медианы (182,0 см).
#  Распределение не совсем симметрично.
#  Гистограмма показывает базовое частотное распределение набора непрерывных данных,
#  позволяя проверять данные на предмет их формы, выбросов, асимметрии и т.д.

# TODO: Boxplot (Блочная диаграмма)
#  Помните вывод .describe()?
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
#
# print(presidents_df['height'].describe())
# TODO: Соответствующая диаграмма показана ниже.
#  Синее поле указывает межквартильный диапазон (IQR, между первым квартилем и третьим),
#  другими словами, 50% данных попадают в этот диапазон. Красная полоса показывает медиану,
#  а нижние и верхние черные усы — минимум и максимум.
# import matplotlib.pyplot as plt
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
# plt.style.use('classic')
# presidents_df.boxplot(column='height')
# plt.savefig("plot.png")
# plt.show()
# TODO: См. рисунок: Boxplot.png (и разберись почему в PyCharm не отражаются данные графики)
#  Поскольку красная полоса, медиана, разрезает прямоугольник на неравные части,
#  это означает, что данные о высоте искажены.
#  График «коробка с усами» не показывает частоту и не отображает каждую индивидуальную статистику,
#  но четко показывает, где находится середина данных и не искажены ли данные.

# TODO: Bar Plot (Бар Участок)
#  Гистограммы показывают распределение данных по нескольким группам.
#  Например, гистограмма может отображать распределение президентов по партиям.
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
#
# presidents_df = pd.read_csv('https://sololearn.com/uploads/files/president_heights_party.csv', index_col='name')
# party_cnt = presidents_df['party'].value_counts()
#
# plt.style.use('ggplot')
# party_cnt.plot(kind='bar')
# plt.savefig("plot.png")
# plt.show()
# TODO: Bar plots обычно путают с гистограммами. Гистограмма представляет числовые данные,
#  тогда как bar plots показывает категориальные данные.
#  Гистограмма рисуется таким образом, что между столбцами нет промежутка, в отличие от bar plots.
#  Создание графиков непосредственно из серии pandas или DataFrame — это мощно.
#  Это удобно, когда мы изучаем анализ данных в науке о данных.

# TODO: ЗАДАЧА: Data Science - Missing Numbers (Наука о данных — пропущенные числа)
#  Вменение пропущенных значений. В реальном мире вам часто придется обрабатывать пропущенные значения.
#  Один из способов вменения (т.е. заполнения) числового столбца состоит в том,
#  чтобы заменить нулевые значения его средним значением.
#  Учитывая список чисел, включая некоторые пропущенные значения, превратить его в серию pandas,
#  заменить пропущенные значения средним значением и, наконец, вернуть ряд.
#  Формат ввода: Список чисел, включающий одну или несколько строк «nan», указывающих на отсутствующее значение.
#  Формат вывода: Список вмененных значений, в котором все значения округлены до первого десятичного знака.
#  Sample Input:
#  3 4 5 3 4 4 nan
#  Sample Output:
#  0 3.0
#  1 4.0
#  2 5.0
#  3 3.0
#  4 4.0
#  5 4.0
#  6 3.8
#  dtype: float64
#  Объяснение: Среднее значение 3, 4, 5, 3, 4 и 4 равно 3,8, поэтому мы заменяем отсутствующее значение средним.
# import numpy as np
# import pandas as pd
#
# lst = [float(x) if x != 'nan' else np.NaN for x in input().split()]
# series = pd.Series(np.array(lst))
# series_not_nan = series.fillna(series.mean())
# print(series_not_nan.round(1))
# # series_with_nan = series.fillna(series)
# # print(series_with_nan.round(1))

# TODO: What is Machine Learning? (Что такое машинное обучение?)
#  Машинное обучение, подмножество науки о данных, представляет собой научное исследование вычислительных алгоритмов
#  и статистических моделей для выполнения конкретных задач с помощью шаблонов и выводов вместо явных инструкций.
#  Машинное обучение можно описать как набор инструментов для построения моделей на основе данных.
#  Специалисты по данным исследуют данные, выбирают и строят модели (машина),
#  настраивают параметры таким образом, чтобы модель соответствовала наблюдениям (обучение),
#  а затем используют модель для прогнозирования и понимания аспектов новых невидимых данных.
#  Машинное обучение — это набор инструментов, используемых для построения моделей на основе данных.
#  Построение моделей для понимания данных и прогнозирования —
#  важная часть работы специалиста по обработке и анализу данных.

# TODO: Supervised and Unsupervised Learning (Контролируемое и неконтролируемое обучение)
#  В машинном обучении мы говорим о контролируемом и неконтролируемом обучении.
#  Обучение с учителем — это когда у нас есть известная цель (также называемая меткой) на основе
#  прошлых данных (например, предсказание цены, по которой будет продаваться дом),
#  а обучение без учителя — это когда нет известного прошлого ответа (например,
#  определение темы, обсуждаемые в обзорах ресторанов).
#  В этом модуле мы рассмотрим линейную регрессию, алгоритм машинного обучения с учителем.
#  В следующих модулях мы также рассмотрим другой алгоритм машинного обучения с учителем, классификацию,
#  а также алгоритм машинного обучения без учителя, кластеризацию.
#  И проблемы регрессии, и проблемы классификации являются проблемами обучения с учителем.

# TODO: Scikit-learn
#  Scikit-learn, одна из самых известных библиотек машинного обучения на Python для машинного обучения,
#  реализует большое количество часто используемых алгоритмов.
#  Независимо от типа алгоритма синтаксис следует одному и тому же рабочему процессу:
#  импорт > создание экземпляра > подгонка > прогнозирование.
#  Как только основное использование и синтаксис Scikit-learn понятны для одной модели,
#  переход на новый алгоритм становится простым.
#  Таким образом, до конца курса мы будем работать с scikit-learn
#  для создания моделей машинного обучения в различных вариантах использования.
#  В этом модуле мы узнаем, как прогнозировать цены на жилье в Бостоне, США, с помощью линейной регрессии.
#  В дополнение к набору алгоритмов scikit-learn также предоставляет несколько небольших наборов данных,
#  используемых сообществом машинного обучения для сравнения алгоритмов с данными, поступающими из реального мира,
#  такими как набор данных о ценах на жилье в Бостоне, который мы будем использовать в этом модуле,
#  набор данных радужной оболочки для задачи классификации в следующем и т.д.

# TODO: Linear Regression (Линейная регрессия)
#  Начнем с линейной регрессии, простой модели обучения с учителем.
#  Линейная регрессия математически соответствует прямой линии данных:
#      y = b + m * x
#  где:
#  b — точка пересечения
#  m — наклон
#  x — признак или вход
#  y — метка или выход.
#  Наша задача — найти такие m и b, чтобы ошибки были минимальными.
#  См. Рис: 1.png
#  См. Рис: 2.png
#  Чтобы визуализировать концепцию, давайте начнем с пяти точек (1.5, 2.8), (2, 5.3), (2.5, 5.5), (3, 7), (3.5, 8.8):
#  См. Рис: 3.png
#  Мы хотели бы провести линию через эти данные точек, однако, даже на глаз, не существует линии,
#  проходящей через все пять точек, поэтому мы сделаем все, что в наших силах.
#  Что это значит? Какая из трех линий, показанных ниже, по вашему мнению, лучше всего соответствует данным?
#  Зеленая линия — это y = 10 + (-2) * X, синяя линия — это y = 5,5 + 0 * X, а красная линия — это y = 1 + 2 * X:
#  См. Рис: 4.png
#  Красная линия! Почему? Потому что он лучше всего отражает линейную зависимость между X и Y и ближе всего к точкам.
#  Математически расстояние между подобранной линией и точками данных вычисляется по остаткам,
#  обозначенным пунктирной черной вертикальной линией на графике ниже:
#  См. Рис: 5.png
#  Таким образом, линейная регрессия, по сути, находит линию,
#  где она минимизирует сумму квадратов остатков, которые мы обсудим позже.
#  Модели линейной регрессии популярны, потому что они могут быстро выполнять подгонку и легко интерпретируются.
#  Прогнозирование непрерывного значения с помощью линейной регрессии — хорошая отправная точка.

# TODO: Boston Housing Dataset (Набор данных о жилье в Бостоне)
#  Набор данных о жилье в Бостоне — это наш образец набора данных,
#  в котором представлены медианные значения домов в разных районах Бостона.
#  Наряду с медианными значениями дома в 1000 долларов США (MEDV), преступностью (CRIM),
#  концентрацией оксидов азота (NOX), средним количеством комнат (RM),
#  процентом населения с более низким статусом (LSTAT) и другими признаками.
#  Наша цель — спрогнозировать медианную цену дома (MEDV), цель в этом сценарии,
#  используя некоторые предоставленные функции.
#  Данные создаются в scikit-learn, и мы будем использовать load_boston
#  для загрузки объекта, содержащего всю информацию.
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# TODO: Для более простых манипуляций позже мы создаем pandas DataFrame из numpy ndarrays,
#  хранящихся в boston_dataset.data, следующим образом:
# import pandas as pd
#
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# TODO: Как следует из названия, boston_dataset.feature_names содержит имена для всех характеристик.
#  Затем мы добавляем цель в DataFrame:
# boston['MEDV'] = boston_dataset.target
# TODO: Давайте проверим набор данных в «бостонском» DataFrame.
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# print(boston.shape)
# TODO: Есть 506 записей и 14 столбцов, включая 13 характеристик и цель.
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# print(boston.columns)
# TODO: Дополнительные сведения о наборе данных о ценах на жилье в Бостоне
#  см. в руководстве пользователя в документации scikit-learn.

# TODO: Head (Глава)
#  Это полезно для быстрого тестирования, содержит ли DataFrame данные правильного типа.
#  Чтобы увидеть первые несколько строк DataFrame, используйте .head(n),
#  где вы можете указать n для количества выбираемых строк.
#  Если n опущено, по умолчанию выбираются первые 5 строк.
#  Для проверки первых 5 строк используем boston.head(),
#  для простоты отображения выбираем столбцы CHAS, RM, AGE, RAD и MEDV:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# print(boston[['CHAS', 'RM', 'AGE', 'RAD', 'MEDV']].head())
# # print(boston[['CHAS', 'RM', 'AGE', 'RAD', 'MEDV']].head(n=5))
# TODO: После сканирования значений CHAS и RAD кажутся целыми числами, а не числами с плавающей запятой.
#  Согласно описанию данных, CHAS определяет, граничит ли участок собственности с рекой (=1) или нет (=0);
#  RAD — индекс доступности радиальных магистралей.
#  Часто наборы данных загружаются из файлов других форматов (например, csv, text),
#  рекомендуется проверить несколько первых и последних строк фрейма данных и убедиться,
#  что данные имеют согласованный формат, используя head и tail соответственно.

# TODO: Summary Statistics (Сводные статистические данные)
#  Напомним, что распечатывать весь набор данных с большим объемом выборки нецелесообразно.
#  Вместо этого мы хотим обобщить и охарактеризовать выборочные данные, используя только несколько значений.
#  Чтобы проверить сводную статистику набора данных (округлите до второго десятичного знака для лучшего отображения):
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# print(boston.describe().round(2))
# print(boston[['RM', 'AGE']].describe().round(2))
# print(boston.describe(include='all').round(2))
# TODO: Двоичный CHAS имеет среднее значение 0,07, а его 3-й квартиль равен 0.
#  Это указывает на то, что большинство значений в CHAS равны 0.
#  Среднее количество комнат на жилище колеблется от 3.56 до 8.78, при среднем значении 6,28 и медиане 6,21.
#  Распределение RM кажется симметричным.
#  Если DataFrame содержит больше, чем просто числовые значения,
#  по умолчанию describe() выводит описательную статистику для числовых столбцов.
#  Чтобы отобразить сводную статистику по всем столбцам, укажите в методе include = 'all'.

# TODO: Visualization (Визуализация)
#  Сводная статистика дает общее представление о каждой характеристике и цели,
#  но визуализация раскрывает информацию более четко.
#  Хорошей практикой является визуализация и проверка столбца распределения за столбцом.
#  Здесь мы смотрим на CHAS и RM, чтобы проверить наши выводы из предыдущей части.
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
#
# boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# boston.hist(column='CHAS')
# plt.savefig("plot1.png")
# plt.show()
# TODO: CHAS принимает только два значения, 0 и 1, причем большинство из них нулевые.
#  Это согласуется с тем, что сообщает describe(); в частности, третий квартиль CHAS равен 0.
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
#
# boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# boston.hist(column='RM', bins=20)
# plt.savefig("plot1.png")
# plt.show()
# TODO: Распределение RM кажется нормальным и симметричным.
#  Симметрия согласуется с тем, что мы наблюдали из выходных данных describe(),
#  поскольку среднее значение RM 6,28 близко к его медиане 6,21.
#  Информативная визуализация данных не только раскрывает идеи,
#  но и бесценна для передачи результатов заинтересованным сторонам.

# TODO: Correlation Matrix (Корреляционная матрица)
#  Чтобы понять взаимосвязь между характеристиками (столбцами),
#  корреляционная матрица очень полезна при исследовательском анализе данных.
#  Корреляция измеряет линейные отношения между переменными.
#  Мы можем построить матрицу корреляции, чтобы показать коэффициенты корреляции между переменными.
#  Он симметричен, где каждый элемент представляет собой коэффициент корреляции в диапазоне от -1 до 1.
#  Значение около 1 (соответственно -1) указывает на сильную положительную
#  (соответственно отрицательную) корреляцию между переменными.
#  Мы можем создать корреляционную матрицу, используя функцию «corr»:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# corr_matrix = boston.corr().round(2)
# print(corr_matrix)
# TODO: Последняя строка или столбец используются для определения характеристик, наиболее тесно связанных
#  с целевым показателем MEDV (средняя стоимость домов, занимаемых владельцами, в 1000 долларов).
#  LSTAT (процент населения с более низким статусом) наиболее отрицательно коррелирует с целевым значением (-0,74),
#  что означает, что по мере снижения процента более низкого статуса средняя стоимость дома увеличивается;
#  в то время как RM (среднее количество комнат в жилище) наиболее положительно коррелирует с MEDV (0,70),
#  что означает, что стоимость дома увеличивается с увеличением количества комнат.
#  Понимание данных с помощью исследовательского анализа данных является важным шагом перед построением модели.
#  От размера выборки и распределения до корреляции между характеристиками и целью — мы получаем больше информации
#  на каждом этапе, помогая в выборе функций и алгоритмов.

# TODO: Data Preparation - Feature Selection (Подготовка данных — выбор признаков (характеристик))
#  На предыдущем уроке мы заметили, что RM и MEDV положительно коррелируют.
#  Вспомните, что точечная диаграмма — полезный инструмент для отображения взаимосвязи между двумя характеристиками;
#  давайте посмотрим на график рассеяния:
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
#
# boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# boston.plot(kind='scatter',
#             x='RM',
#             y='MEDV',
#             figsize=(8, 6))
# plt.savefig("plot1.png")
# plt.show()
# TODO: Мы указываем тип графика, передавая строку 'scatter' в аргумент type,
#  определяем метки для x и y соответственно и устанавливаем размер фигуры через кортеж (ширину, высоту) в дюймах.
#  Цена увеличивается по мере линейного увеличения стоимости RM.
#  Есть несколько выбросов, которые кажутся выходящими за рамки общей картины.
#  Например, одна точка в центре справа соответствует дому почти с 9 комнатами,
#  но медианная стоимость чуть выше 20 тысяч долларов.
#  Дома с аналогичными значениями обычно имеют около 6 комнат.
#  Кроме того, кажется, что у данных есть потолок; то есть максимальное медианное значение ограничено 50.
#  С другой стороны, цены имеют тенденцию снижаться с увеличением LSTAT; и тренд не такой линейный.
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
#
# boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# boston.plot(kind='scatter',
#             x='LSTAT',
#             y='MEDV',
#             figsize=(8, 6))
# plt.savefig("plot1.png")
# plt.show()
# TODO: Из этих двух характеристик RM кажется лучшим выбором для прогнозирования MEDV.
#  Таким образом, мы начинаем с одномерной линейной регрессии: MEDV = b + m * RM.
#  В scikit-learn для моделей требуется двумерная матрица признаков (X, 2darray или pandas DataFrame)
#  и одномерный целевой массив (Y). Здесь мы определяем матрицу признаков как столбец RM в Бостоне и назначаем его X.
#  Обратите внимание на двойные квадратные скобки вокруг «RM» в приведенном ниже коде, чтобы убедиться,
#  что результат остается DataFrame, двумерной структурой данных:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# print(X.shape)
# TODO: Точно так же мы определяем нашу цель как столбец MEDV в Бостоне и назначаем его в переменной с именем Y:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# Y = boston['MEDV']
# print(Y.shape)
# TODO: Напомним, что одинарная скобка выводит Pandas Series, а двойная скобка выводит Pandas DataFrame,
#  и модель ожидает, что матрица признаков X будет 2darray.
#  Выбор признаков используется по нескольким причинам, включая упрощение моделей,
#  чтобы их было легче интерпретировать, сокращение времени обучения, уменьшение переобучения и т.д.

# TODO: Instantiating the Model (Создание экземпляра модели)
#  В scikit-learn каждый класс модели представлен классом в python.
#  Класс модели — это не то же самое, что экземпляр модели.
#  Напомним, что экземпляр — это отдельный объект определенного класса.
#  Таким образом, мы сначала импортируем класс линейной регрессии, затем создаем экземпляр модели,
#  то есть создаем экземпляр класса LinearRegression:
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# print(model)
# TODO: Теперь модель создана, но еще не применена к данным.
#  Scikit-learn делает различие между выбором модели и применением модели к данным очень четко.

# TODO: Train-Test Split (Поезд-тестовый сплит)
#  Затем мы разделяем данные на наборы для обучения и тестирования.
#  Почему? Чтобы оценить производительность модели на новых невидимых данных.
#  Мы обучаем модель, используя обучающий набор, и сохраняем тестовый набор для оценки.
#  Хорошее эмпирическое правило — разбивать данные 70/30,
#  то есть 70% данных используются для обучения, а 30% — для тестирования.
#  Мы используем функцию train_test_split внутри модуля scikit-learn model_selection,
#  чтобы разделить данные на два случайных подмножества.
#  Установите random_state, чтобы результаты были воспроизводимыми.
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
#                                                     test_size=0.3,
#                                                     random_state=1)
# TODO: Проверяем размеры, чтобы обеспечить одинаковое количество рядов.
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# print(X_train.shape)
# print(Y_train.shape)
# print(X_test.shape)
# print(Y_test.shape)
# TODO: Чтобы получить объективную оценку прогностической способности модели,
#  важно, чтобы данные тестирования не отображались в построенной модели.

# TODO: Fitting the Model (Подгонка модели)
#  Короче говоря, примерка равна тренировке.
#  Он подгоняет модель к обучающим данным и находит коэффициенты, указанные в модели линейной регрессии,
#  т.е. точку пересечения и наклон. После обучения модель можно использовать для прогнозирования.
#  Теперь давайте применим модель к данным. Помните, что мы сохраняем данные тестирования,
#  чтобы сообщать о производительности модели, и используем только обучающий набор для построения модели.
#  Синтаксис:
#      model.fit(X_train, Y_train)
# TODO: Команда fit() запускает вычисления, и результаты сохраняются в объекте модели.
#  Подгонка — это то, насколько хорошо модель машинного обучения соотносится с данными, на которых она была обучена.

# TODO: Parameter Estimates (Оценки параметров)
#  Модель линейной регрессии была подобрана, что означает, что оба параметра,
#  точка пересечения и наклон, были изучены. Кто они такие?
#  В Scikit-learn по соглашению все параметры модели имеют завершающие символы подчеркивания, например,
#  для доступа к оценочному пересечению из модели, округленному до 2-го десятичного знака для лучшего отображения:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
# print(model.intercept_.round(2))
# TODO: Точно так же расчетный коэффициент признака RM равен:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
# print(model.coef_.round(2))
# TODO: Два параметра представляют точку пересечения и наклон линии, подходящей к данным.
#  Наша подогнанная модель MEDV = -30,57 + 8,46 * RM.
#  При увеличении RM на одну единицу средняя цена дома вырастет на 8460 долларов.
#  Полный код, соответствующий модели:
# from sklearn.linear_model import LinearRegression
# from sklearn.model_selection import train_test_split
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
# print(model.intercept_.round(2))
# print(model.coef_.round(2))
# TODO: Ты сделал это! Вы только что построили первую модель линейной регрессии в scikit-learn:
#  от импорта класса до создания экземпляра модели, чтобы подогнать модель к данным, и готово!

# TODO: Prediction (Прогноз)
#  После обучения модели контролируемое машинное обучение будет оценивать тестовые данные
#  на основе предыдущих прогнозов для невидимых данных. Мы можем сделать прогноз, используя метод predict().
#  Когда среднее количество комнат в доме составляет 6.5,
#  модель прогнозирует стоимость дома в размере 24 426,06 долларов США.
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# import numpy as np
#
# new_RM = np.array([6.5]).reshape(-1, 1)  # убедитесь, что это 2D
# print(model.predict(new_RM))
# TODO: Обратите внимание, что ввод должен быть двумерным, в этом случае будет работать либо 2darray, либо DataFrame.
#  Это значение такое же, как если бы мы подставили линию b + m*x,
#  где b — предполагаемое пересечение с моделью, а m — предполагаемый наклон.
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# import numpy as np
#
# new_RM = np.array([6.5]).reshape(-1, 1)  # убедитесь, что это 2D
# print(model.intercept_ + model.coef_ * 6.5)
# TODO: Кроме того, мы можем передать тестовый набор и получить прогнозы для всех домов.
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
# print(y_test_predicted.shape)
# print(type(y_test_predicted))
# TODO: Результатом является 1darray, такой же формы, как Y_test.
# import pandas as pd
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# print(Y_test.shape)
# TODO: Метод predict() оценивает медианную стоимость дома, вычисляя model.intercept_ + model.coef_*RM.

# TODO: Residuals (Остатки)
#  Насколько хорош наш прогноз? Мы можем проверить производительность модели,
#  визуально сравнив подобранную линию и истинные наблюдения в тестовом наборе.
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.datasets import load_boston
# from sklearn.linear_model import LinearRegression
#
# from sklearn.model_selection import train_test_split
#
# model = LinearRegression()
# boston_dataset = load_boston()
#
# boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# X = boston[['RM']]
# Y = boston['MEDV']
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# plt.scatter(X_test, Y_test,
#             label='testing data')
# plt.plot(X_test, y_test_predicted,
#          label='prediction', linewidth=3)
# plt.xlabel('RM')
# plt.ylabel('MEDV')
# plt.legend(loc='upper left')
# plt.savefig("plot.png")
# plt.show()
# TODO: Некоторые точки лежат на линии, а некоторые - в стороне от нее.
#  Мы можем измерить расстояние между точкой и линией вдоль вертикальной линии,
#  и это расстояние называется остатком или ошибкой.
#  Остаток - это разница между наблюдаемым значением цели и прогнозируемым значением.
#  Чем ближе невязка к 0, тем лучше работает наша модель.
#  Мы можем рассчитать остаток и представить его в виде диаграммы рассеяния.
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.datasets import load_boston
# from sklearn.linear_model import LinearRegression
#
# from sklearn.model_selection import train_test_split
#
# model = LinearRegression()
# boston_dataset = load_boston()
#
# boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# X = boston[['RM']]
# Y = boston['MEDV']
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# residuals = Y_test - y_test_predicted
#
# # plot the residuals
# plt.scatter(X_test, residuals)
# # plot a horizontal line at y = 0
# plt.hlines(y=0,
#            xmin=X_test.min(), xmax=X_test.max(),
#            linestyle='--')
# # set xlim
# plt.xlim((4, 9))
# plt.xlabel('RM')
# plt.ylabel('residuals')
# plt.savefig("plot.png")
# plt.show()
# TODO: Остатки разбросаны по горизонтальной линии, y = 0, без определенного рисунка.
#  Это кажущееся случайным распределение является признаком того, что модель работает.
#  В идеале остатки должны быть расположены симметрично и случайным образом вокруг горизонтальной оси;
#  если остаточный график показывает некоторую закономерность, линейную или нелинейную,
#  это указывает на то, что наша модель может быть улучшена.
#  Остаточные графики могут выявить систематическую ошибку модели,
#  а статистические показатели указывают на соответствие.

# TODO: Mean Squared Error (Среднеквадратическая ошибка)
#  Ранее мы узнали, что когда каждый остаток близок к 0, это предполагает хорошее соответствие.
#  Например, первые пять остатков в нашей модели:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# residuals = Y_test - y_test_predicted
# print(residuals[:5])
# TODO: Это отдельные точки данных, а как насчет производительности модели для всех точек данных?
#  Нам нужен способ агрегировать остатки и просто указать одно число в качестве метрики.
#  Естественно взять среднее значение всех остатков:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# residuals = Y_test - y_test_predicted
# print(residuals.mean())
# TODO: -0,24 довольно близко к 0, но есть проблема: остатки могут быть как положительными,
#  так и отрицательными, поэтому получение среднего значения отменяет их. Это не точная метрика.
#  Чтобы решить эту проблему, мы возьмем квадрат каждого остатка, а затем возьмем среднее значение квадратов.
#  Это называется среднеквадратичной ошибкой (MSE):
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# residuals = Y_test - y_test_predicted
# print((residuals ** 2).mean())
# TODO: Мы также можем использовать метод mean_squared_error()
#  в модуле метрик scikit-learn для вывода того же результата:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# residuals = Y_test - y_test_predicted
#
# from sklearn.metrics import mean_squared_error
#
# print(mean_squared_error(Y_test, y_test_predicted))
# TODO: В целом, чем меньше MSE, тем лучше, но абсолютного порога «хорошо» или «плохо» не существует.
#  Мы можем определить его на основе зависимой переменной, т.е. MEDV в наборе тестов.
#  Y_test колеблется от 6.3 до 50 с дисперсией 92.26. По сравнению с общей дисперсией, MSE 36.52 — это неплохо.
#  Чтобы шкала ошибок совпадала со шкалой целей, часто используется среднеквадратическая ошибка (RMSE).
#  Это квадратный корень из MSE.

# TODO: R-squared (R-квадрат)
#  Другой распространенный показатель для оценки производительности модели называется R-квадрат;
#  можно рассчитать через model.score():
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# residuals = Y_test - y_test_predicted
#
# from sklearn.metrics import mean_squared_error
#
# print(model.score(X_test, Y_test))
# TODO: Это доля общей вариации, объясняемая моделью.
#  Здесь около 60% изменчивости данных тестирования объясняется нашей моделью.
#  Общая вариация рассчитывается как сумма квадратов разницы между ответом
#  и средним значением ответа, в примере данных тестирования:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# residuals = Y_test - y_test_predicted
#
# from sklearn.metrics import mean_squared_error
#
# print(((Y_test - Y_test.mean()) ** 2).sum())
# TODO: Принимая во внимание, что вариация, которую модель не может зафиксировать,
#  вычисляется как сумма квадратов остатков:
# import pandas as pd
#
# from sklearn.datasets import load_boston
#
# boston_dataset = load_boston()
# ## build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X = boston[['RM']]
# Y = boston['MEDV']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
#
# from sklearn.linear_model import LinearRegression
#
# model = LinearRegression()
# model.fit(X_train, Y_train)
#
# y_test_predicted = model.predict(X_test)
#
# residuals = Y_test - y_test_predicted
#
# from sklearn.metrics import mean_squared_error
#
# print((residuals ** 2).sum())
# TODO: Тогда доля общего отклонения от данных равна:
# print(1 - 5550.6166390874705 / 13931.482039473683)
# TODO: Идеальная модель объясняет все различия в данных.
#  Примечание R-квадрат находится в диапазоне от 0 до 100 %:
#  0 % указывает, что модель не объясняет ни одной из изменчивости данных отклика вокруг своего среднего значения,
#  а 100 % указывает, что модель объясняет все это.
#  Оценка значений R-квадрата в сочетании с остаточными графиками дает количественную оценку производительности модели.

# TODO: Overview (Обзор)
#  Напомним, LSTAT (% более низкого статуса среди населения) наиболее отрицательно коррелирует с ценой на жилье.
#  Мы можем добавить эту характеристику и построить модель многомерной линейной регрессии,
#  в которой цена дома линейно зависит как от RM, так и от LSTAT: MEDV = b0 + b1 * RM + b2 * LSTAT - то же самое,
#  за исключением части подготовки данных, теперь мы имеем дело с двумя характеристиками:
# import pandas as pd
# from sklearn.datasets import load_boston
# import matplotlib.pyplot as plt
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# X = boston[['RM']]
# Y = boston['MEDV']
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
#                                                     test_size=0.3,
#                                                     random_state=1)
# model = LinearRegression()
# model.fit(X_train, Y_train)
# y_test_predicted = model.predict(X_test)
# # data preparation
# X2 = boston[['RM', 'LSTAT']]
# Y = boston['MEDV']
# # train test split
# # same random_state to ensure the same splits
# X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y,
#                                                       test_size=0.3,
#                                                       random_state=1)
# model2 = LinearRegression()
# print(model2.fit(X2_train, Y_train))
# TODO: Мы можем получить доступ к параметрам после установки модели2.
# import pandas as pd
# from sklearn.datasets import load_boston
# import matplotlib.pyplot as plt
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# X = boston[['RM']]
# Y = boston['MEDV']
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
#                                                     test_size=0.3,
#                                                     random_state=1)
# model = LinearRegression()
# model.fit(X_train, Y_train)
# y_test_predicted = model.predict(X_test)
# # data preparation
# X2 = boston[['RM', 'LSTAT']]
# Y = boston['MEDV']
# # train test split
# # same random_state to ensure the same splits
# X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y,
#                                                       test_size=0.3,
#                                                       random_state=1)
# model2 = LinearRegression()
# model2.fit(X2_train, Y_train)
# print(model2.intercept_)
# print(model2.coef_)
# TODO: Обратите внимание, что коэффициенты хранятся в 1darray формы (2,).
#  Тогда вторая модель MEDV = 5,32 + 4,13 * RM + (-0,68) * LSTAT.
#  Разрешение на прогнозы:
# import pandas as pd
# from sklearn.datasets import load_boston
# import matplotlib.pyplot as plt
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# X = boston[['RM']]
# Y = boston['MEDV']
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
#                                                     test_size=0.3,
#                                                     random_state=1)
# model = LinearRegression()
# model.fit(X_train, Y_train)
# y_test_predicted = model.predict(X_test)
# # data preparation
# X2 = boston[['RM', 'LSTAT']]
# Y = boston['MEDV']
# # train test split
# # same random_state to ensure the same splits
# X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y,
#                                                       test_size=0.3,
#                                                       random_state=1)
# model2 = LinearRegression()
# model2.fit(X2_train, Y_train)
# print(model2.intercept_)
# print(model2.coef_)
# TODO: Расширение от одномерной до многомерной линейной регрессии в scikit-learn не вызывает затруднений.
#  Реализация модели, подгонка и прогнозы идентичны, единственное отличие заключается в подготовке данных.

# TODO: ЗАДАЧА: Overview (Обзор)
#  Завершите код для обучения модели многомерной линейной регрессии для прогнозирования MEDV на основе характеристик:
#  - RM    (количество комнат в доме)
#  - LSTAT (% более низкого статуса среди населения)
#  - CRIM  (уровень преступности на душу населения по городам)
# import pandas as pd
# from sklearn.datasets import load_boston
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
#
# boston_dataset = load_boston()
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
#
# X3 = boston[['RM', 'LSTAT', 'CRIM']]
# Y = boston['MEDV']
# X3_train, X3_test, Y_train, Y_test = train_test_split(X3, Y, test_size=0.3, random_state=1)
# model3 = LinearRegression()
# model3.fit(X3_train, Y_train)
# print(model3.intercept_)
# print(model3.coef_)

# TODO: Comparing Models (Сравнение моделей)
#  Какая модель лучше? Простым показателем для линейной регрессии является
#  среднеквадратическая ошибка (MSE) для данных тестирования.
#  Лучшие модели имеют более низкие MSE.
#  Напомним, что MSE первой модели на тестовых данных:
# import pandas as pd
# from sklearn.datasets import load_boston
# import matplotlib.pyplot as plt
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import mean_squared_error
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# X = boston[['RM']]
# Y = boston['MEDV']
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
#                                                     test_size=0.3,
#                                                     random_state=1)
# model = LinearRegression()
# model.fit(X_train, Y_train)
# y_test_predicted = model.predict(X_test)
# # data preparation
# X2 = boston[['RM', 'LSTAT']]
# Y = boston['MEDV']
# # train test split
# # same random_state to ensure the same splits
# X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y,
#                                                       test_size=0.3,
#                                                       random_state=1)
# model2 = LinearRegression()
# model2.fit(X2_train, Y_train)
# y_test_predicted2 = model2.predict(X2_test)
# print(mean_squared_error(Y_test, y_test_predicted).round(2))
# TODO: MSE второй модели:
# import pandas as pd
# from sklearn.datasets import load_boston
# import matplotlib.pyplot as plt
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import mean_squared_error
#
# boston_dataset = load_boston()
# # build a DataFrame
# boston = pd.DataFrame(boston_dataset.data,
#                       columns=boston_dataset.feature_names)
# boston['MEDV'] = boston_dataset.target
# X = boston[['RM']]
# Y = boston['MEDV']
#
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
#                                                     test_size=0.3,
#                                                     random_state=1)
# model = LinearRegression()
# model.fit(X_train, Y_train)
# y_test_predicted = model.predict(X_test)
# # data preparation
# X2 = boston[['RM', 'LSTAT']]
# Y = boston['MEDV']
# # train test split
# # same random_state to ensure the same splits
# X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y,
#                                                       test_size=0.3,
#                                                       random_state=1)
# model2 = LinearRegression()
# model2.fit(X2_train, Y_train)
# y_test_predicted2 = model2.predict(X2_test)
# print(mean_squared_error(Y_test, y_test_predicted2).round(2))
# TODO: Вторая модель имеет более низкий MSE, а именно снижение на 21% (36,52-28,93)/36,52 = 21%);
#  таким образом, она лучше прогнозирует среднюю стоимость домов, чем одномерная модель.
#  Как правило, чем больше характеристик включает модель, тем ниже будет MSE.
#  Тем не менее, будьте осторожны с включением слишком большого количества характеристик.
#  Некоторые характеристики могут быть случайным шумом, что ухудшает интерпретируемость модели.

# TODO: ЗАДАЧА: Data Science - Ordinary Squares (Наука о данных — обычные квадраты)
#  Обычные наименьшие квадраты для линейной регрессии.
#  Обычный метод наименьших квадратов (OLS) — это метод оценки параметров β в простой линейной регрессии,
#  Xβ = y, где X — матрица признаков, а y — зависимая переменная (или цель),
#  путем минимизации суммы квадратов различия между наблюдаемой зависимой переменной
#  в данном наборе данных и предсказанной линейной функцией.
#  Математически решение дается формулой на изображении, где верхний индекс T означает транспонирование матрицы,
#  а верхний индекс -1 означает, что это обратная матрица.
#  Задача Учитывая матрицу признаков двумерного массива X и вектор y, вернуть вектор коэффициентов;
#  см. Рис: formula_1.png.
#  Формат ввода Первая строка: два целых числа, разделенных пробелами,
#  первая указывает строки матрицы признаков X (n), а вторая указывает столбцы X (p)
#  Следующие n строк: значения строки в матрице признаков
#  Последняя строка: значения p of target y
#  Output Format (Формат вывода): Пустой массив 1d значений, округленных до второго десятичного знака.
#  Sample Input:
#  2 2
#  1 0
#  0 2
#  2 3
#  Sample Output:
#  [2. , 1.5]
#  Объяснение: Матрица признаков X имеет n = 2 строки и p = 2 признака.
#  Следуя решению OLS, замените X на np.array([[1.0.], [0.2.]] и y на np.array([2.3.]) соответственно,
#  выполнив умножение матриц с помощью инструментов в numpy приводит к np.array([2. 1.5]).
# import numpy as np
#
# n, p = [int(x) for x in input().split()]
# X = []
# for i in range(n):
#     X.append([float(x) for x in input().split()])
#
# y = [float(x) for x in input().split()]
#
# X = np.array(X).reshape(n, p)
# y = np.array(y)
# b = np.linalg.pinv(X) @ y.transpose()
# print(np.around(b, decimals=2))

# TODO: Discrete Values (Дискретные значения)
#  В последнем модуле мы построили модель линейной регрессии для прогнозирования непрерывного значения,
#  медианной стоимости дома в Бостоне. В этом модуле мы будем работать над задачами классификации,
#  задачей которых является предсказание дискретного значения.
#  Дискретные данные могут иметь только определенные значения,
#  в то время как непрерывные данные могут принимать любое значение.
#  Примеры задач классификации с использованием дискретных значений данных:
#  • предсказать, является ли рак молочной железы доброкачественным или злокачественным, по набору признаков
#  • классифицировать изображение как изображение кошек, собак или лошадей
#  • предсказать, является ли электронное письмо спамом или нет от указанного адреса электронной почты.
#  В каждом из примеров метки имеют категориальное образования, представляют конечное число классов.
#  Дискретные значения данных могут быть числовыми, например количество учеников в классе,
#  или категориальными, например красным, синим или желтым.

# TODO: Binary and Multi-class Classification (Бинарная и многоклассовая классификация)
#  Существует два типа классификации: бинарная и мультиклассовая.
#  Если есть два класса для прогнозирования, это проблема бинарной классификации,
#  например, доброкачественная или злокачественная опухоль.
#  Когда классов больше двух, задача представляет собой проблему множественной классификации.
#  Например, классификация видов ириса, которые могут быть разноцветными, virqinica или setosa,
#  на основе характеристик их чашелистиков и лепестков. Общие алгоритмы классификации включают логистическую регрессию,
#  k ближайших соседей, деревья решений, наивный байесовский алгоритм, машины опорных векторов, нейронные сети и т.д.
#  Здесь мы узнаем, как использовать k ближайших соседей для классификации видов ирисов.
#  Проблемы контролируемого обучения сгруппированы в задачи регрессии и классификации.
#  Обе задачи имеют целью построение функции отображения входных переменных (X) в выходную переменную (y).
#  Разница в том, что выходная переменная непрерывна в регрессии и категорична для классификации.

# TODO: Iris Dataset (Набор данных Ирис)
#  Знаменитая база данных радужной оболочки, впервые использованная сэром Р.А. Фишером,
#  является, пожалуй, самым известным набором данных, который можно найти в литературе по распознаванию образов.
#  Существует 150 растений ириса, каждое из которых имеет 4 числовых атрибута:
#  - длина чашелистика в см
#  - ширина чашелистика в см
#  - длина лепестка в см
#  - ширина лепестка в см.
#  Задача состоит в том, чтобы предсказать каждое растение как: ирис-сетозу или ирис-разноцветный или
#  ирис-виргиника на основе этих 4 признаков.
#  См. Рис: iris.png
# TODO: Набор данных хранится в файле csv,
#  мы можем загрузить его как DataFrame, используя read_csv() в библиотеке pandas:
# import pandas as pd
#
# iris = pd.read_csv('./data/iris.csv')
# TODO: Теперь проверьте размеры и первые несколько строк:
# import pandas as pd
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# print(iris.shape)
# TODO: Мы используем функцию .head() для просмотра первых 5 строк:
# print(iris.head())
# TODO: Идентификатор столбца — это индекс строки, не очень информативный,
#  поэтому мы можем удалить его из набора данных с помощью функции drop():
# iris.drop('id', axis=1, inplace=True)
# print(iris.head())
# TODO: Когда мы изучаем алгоритмы машинного обучения, использование простых данных с хорошим поведением,
#  таких как набор данных цветка ириса, сокращает кривую обучения и упрощает понимание и отладку.

# TODO: Summary Statistics (Суммарная Статистика)
#  Посмотрите сводную статистику:
# import pandas as pd
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# print(iris.describe())
# # print(iris.describe(include='all'))
# # print(iris[['petal_len', 'petal_wd']].describe())

# TODO: Все четыре характеристики являются числовыми, каждая из которых имеет разные диапазоны.
#  Ни в одном из столбцов нет пропущенных значений. Следовательно, это чистый набор данных.
#  Диапазоны атрибутов по-прежнему имеют одинаковую величину, поэтому мы пропустим стандартизацию.
#  Однако стандартизация атрибутов таким образом, чтобы каждый из них имел среднее значение, равное нулю,
#  и стандартное отклонение, равное единице, может быть важным этапом предварительной обработки
#  для многих алгоритмов машинного обучения. Это также называется масштабированием характеристик;
#  см. важность масштабирования характеристик для получения более подробной информации:
#  https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html

# TODO: Class Distribution (Распределение классов)
#  Набор данных содержит 3 класса по 50 экземпляров в каждом.
#  Мы можем проверить это:
# import pandas as pd
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# print(iris.groupby('species').size())
# TODO: Или просто используйте value_counts():
# import pandas as pd
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# print(iris['species'].value_counts())
# TODO: Метод value_counts() — отличная утилита для быстрого понимания распределения данных.
#  При использовании с категориальными данными он подсчитывает количество уникальных значений в интересующем столбце.
#  Iris — это сбалансированный набор данных, поскольку точки данных для каждого класса равномерно распределены.
#  Примером несбалансированного набора данных является мошенничество.
#  Как правило, лишь небольшой процент от общего числа транзакций является фактическим мошенничеством,
#  примерно 1 из 1000. И когда набор данных несбалансирован, будет использоваться немного другой анализ.
#  Поэтому важно понимать, являются ли данные сбалансированными или несбалансированными.
#  Несбалансированный набор данных — это набор, в котором классы в данных представлены неравномерно.
#  Чтобы узнать больше о несбалансированных данных, перейдите по этой ссылке:
#  https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/

# TODO: Univariate Plot (Одномерный сюжет)
#  Чтобы лучше понять каждый атрибут, начните с одномерных графиков, то есть графиков каждой отдельной переменной.
# import pandas as pd
# from matplotlib import pyplot as plt
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris[['sepal_len', 'sepal_wd', 'petal_len', 'petal_wd']].hist()
# plt.show()
# TODO: Это дает нам гораздо более четкое представление о распределении входной переменной, показывая,
#  что и длина чашелистика, и ширина чашелистика имеют нормальное (гауссово) распределение.
#  То есть распределение имеет красивый симметричный колоколообразный вид. Однако длина лепестков не нормальная.
#  Его график показывает две моды, один пик происходит около 0, а другой около 5.
#  Меньше закономерностей наблюдалось для ширины лепестка.
#  Гистограммы представляют собой гистограммы, отображающие количество или относительную частоту значений,
#  попадающих в разные интервалы или диапазоны классов.
#  Есть больше одномерных сводных графиков, включая графики плотности и ящичные диаграммы.

# TODO: Multivariate Plot (Многомерный сюжет)
#  Чтобы увидеть взаимодействие между атрибутами, мы используем точечные диаграммы.
#  Однако трудно увидеть, есть ли какая-либо группировка без каких-либо указаний на истинный вид цветка,
#  который представляет точка данных.
#  Поэтому мы определяем цветовой код для каждого вида, чтобы различать виды визуально:
# import numpy as np
# import matplotlib.pyplot as plt
# import pandas as pd
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# # build a dict mapping species to an integer code
# inv_name_dict = {'iris-setosa': 0,
#                  'iris-versicolor': 1,
#                  'iris-virginica': 2}
#
# # build integer color code 0/1/2
# colors = [inv_name_dict[item] for item in iris['species']]
# # scatter plot
# scatter = plt.scatter(iris['sepal_len'], iris['sepal_wd'], c=colors)
# plt.xlabel('sepal length (cm)')
# plt.ylabel('sepal width (cm)')
# # add legend
# plt.legend(handles=scatter.legend_elements()[0],
#            labels=inv_name_dict.keys())
# plt.savefig("plot.png")
# plt.show()
# TODO: Используя функции sepal_length и sepal_width, мы можем отличить iris-setosa от других;
#  отделить радужную оболочку от радужной оболочки виргинской сложнее из-за перекрытия,
#  как видно по зеленым и желтым точкам данных.
#  Точно так же между длиной и шириной лепестка:
# import matplotlib.pyplot as plt
# import pandas as pd
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# # build a dict mapping species to an integer code
# inv_name_dict = {'iris-setosa': 0,
#                  'iris-versicolor': 1,
#                  'iris-virginica': 2}
#
# # build integer color code 0/1/2
# colors = [inv_name_dict[item] for item in iris['species']]
# # scatter plot
# scatter = plt.scatter(iris['petal_len'], iris['petal_wd'], c=colors)
# plt.xlabel('petal length (cm)')
# plt.ylabel('petal width (cm)')
# # add legend
# plt.legend(handles=scatter.legend_elements()[0],
#            labels=inv_name_dict.keys())
# plt.savefig("plot.png")
# plt.show()
# TODO: Интересно, что длина и ширина лепестка сильно коррелированы,
#  и эти два признака очень полезны для идентификации различных видов ирисов.
#  Примечательно, что граница между iris-versicolor и iris-virginica остается немного размытой,
#  что указывает на трудности для некоторых классификаторов.
#  При обучении стоит помнить, какие функции мы должны использовать.
#  Чтобы увидеть диаграммы разброса всех пар характеристик, используйте pandas.plotting.scatter_matrix() .
#  Помимо гистограмм отдельных переменных по диагонали, он покажет графики рассеяния всех пар атрибутов,
#  чтобы помочь определить структурированные отношения между характеристиками.

# TODO: K nearest neighbors (K ближайших соседей)
#  K ближайших соседей (knn) — это контролируемая модель машинного обучения, которая берет точку данных,
#  просматривает k ближайших помеченных точек данных и присваивает метку большинством голосов.
#  Здесь мы видим, что изменение k может повлиять на результат модели. В knn k является гиперпараметром.
#  Гиперпараметр в машинном обучении — это параметр, значение которого задается до начала процесса обучения.
#  Позже мы узнаем, как настроить гиперпараметр.
#  Например, на Рис: picture_1.png есть два класса: синие квадраты и красные треугольники.
#  Какую метку мы должны присвоить зеленой точке с неизвестной меткой на основе алгоритма 3nn,
#  т.е. когда k равно 3? Из 3 ближайших точек данных от зеленой точки (сплошной круг)
#  две представляют собой красные треугольники, а одна — синий квадрат, поэтому прогнозируется,
#  что зеленая точка будет красным треугольником. Если k равно 5 (круг со штрихпунктирной линией),
#  то он классифицируется как синий квадрат (3 синих квадрата против 2 красных треугольников, синих квадратов больше).
#  В scikit-learn алгоритм k ближайших соседей реализован в модуле sklearn.neighbors:
from sklearn.neighbors import KNeighborsClassifier

# TODO: См. Рис: picture_2.png: Рассмотрим в нашем наборе данных по радужной оболочке три ближайших соседа данных,
#  отмеченных красной стрелкой, как показано ниже: Все ближайшие соседи — iris-setosa (т.е. фиолетовые точки данных);
#  таким образом, к 3-nn заостренный элемент данных также должен быть помечен как iris-setosa.
# TODO: K ближайших соседей также можно использовать для задач регрессии. Разница заключается в предсказании.
#  Вместо большинства голосов knn для регрессии делает прогноз, используя средние метки k ближайших точек данных.

# TODO: Data Preparation (Подготовка данных)
#  Ранее мы определили, что длина и ширина лепестков являются наиболее полезными признаками
#  для разделения видов; затем мы определяем характеристики и метки следующим образом:
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
# TODO: Напомним, что для оценки производительности модели мы делаем это на данных,
#  которые невидимы при построении модели. В результате мы отложили некоторую часть данных
#  в качестве тестового набора для имитации неизвестных данных, которые будут представлены модели в будущем.
#  Как и в предыдущем модуле, мы используем train_test_split в sklearn.model_selection.
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
# TODO: Мы используем разделение 70-30, т. е. 70% данных предназначены для обучения, а 30% — для тестирования.
#  Обратите внимание, что мы указали, что разделение было стратифицировано по метке (y).
#  Это сделано для того, чтобы распределение меток оставалось одинаковым как в обучающем, так и в тестовом наборах:
# import pandas as pd
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)
#
# print(y_train.value_counts())
# print(y_test.value_counts())
# TODO: В классификациях часто выбирается стратифицированная выборка, чтобы гарантировать,
#  что обучающие и тестовые наборы имеют примерно такой же процент выборок каждого целевого класса, как и полный набор.

# TODO: Modeling (Моделирование)
#  Теперь мы готовы построить и обучить модель knn. Сначала мы импортируем класс модели:
from sklearn.neighbors import KNeighborsClassifier

# TODO: Теперь создайте экземпляр knn из класса KNeighborsClassifier.
# knn = KNeighborsClassifier(n_neighbors=5)
# TODO: Обратите внимание, что единственный параметр, который нам нужно установить в этой задаче,
#  — это n_neighbors или k, как в knn. Мы устанавливаем k равным 5 случайным выбором.
#  Используйте данные X_train и y_train для обучения модели:
# import pandas as pd
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# from sklearn.model_selection import train_test_split
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.neighbors import KNeighborsClassifier
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # fit
# print(knn.fit(X_train, y_train))
# TODO: Он выводит обученную модель. Мы используем большинство значений по умолчанию для параметров,
#  например, metric = 'minkowski' и p = 2 вместе определяют, что расстояние является евклидовым расстоянием.
#  Подробнее об использовании других параметров см. в документации scikit-learn:
#  https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

# TODO: Label Prediction (Предсказание метки
#  Чтобы сделать прогноз в scikitlearn, мы можем вызвать метод predict().
#  Мы пытаемся предсказать виды радужной оболочки, используя заданные характеристики в матрице признаков X.
#  Давайте сделаем прогнозы на тестовом наборе данных и сохраним результат в pred для последующего просмотра:
#      pred = knn.predict(X_test)
# TODO:
# import numpy as np
# import pandas as pd
#
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # fit
# knn.fit(X_train, y_train)
#
# pred = knn.predict(X_test)
# print(pred[:5])
# # print(*pred[:5], sep='\n')
# TODO: Каждое предсказание представляет собой вид радужной оболочки и хранится в 1darray.
#  predict() возвращает массив предсказанных меток класса для данных предиктора.

# TODO: Probability Prediction (Прогнозирование вероятности)
#  Из всех алгоритмов классификации, реализованных в scikitlearn, есть дополнительный метод predict_proba.
#  Вместо разделения метки он выводит вероятность цели в виде массива.
#  Давайте посмотрим, каковы предсказанные вероятности для 11-го и 12-го цветов:
# import numpy as np
# import pandas as pd
#
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # knn = KNeighborsClassifier()
# # fit
# knn.fit(X_train, y_train)
#
# pred = knn.predict(X_test)
#
# y_pred_prob = knn.predict_proba(X_test)
# print(y_pred_prob[10:12])
# TODO: Например, вероятность того, что 11-й цветок будет предсказан как:
#  ирис-сетоза = 1,
#  ирис-разноцветный = 0,
#  ирис-виргиника = 0.
#  Для следующего цветка существует 20-процентная вероятность того,
#  что он будет классифицирован как ирис-сетоза. versicolor, но с вероятностью 80% это iris-virginica.
#  Это говорит нам о том, что из пяти ближайших соседей 12-го цветка в тестовом наборе 1 — это ирис-разноцветный,
#  остальные 4 — ирис-виргиника. Чтобы увидеть соответствующие прогнозы:
# import numpy as np
# import pandas as pd
#
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # fit
# knn.fit(X_train, y_train)
#
# pred = knn.predict(X_test)
#
# y_pred_prob = knn.predict_proba(X_test)
# print(pred[10:12])
# TODO: В задачах классификации мягкое предсказание возвращает предсказанные вероятности точек данных,
#  принадлежащих каждому из классов, в то время как жесткое предсказание выводит только метки.

# TODO: Accuracy (Точность)
#  В классификации наиболее простой метрикой является точность.
#  Он вычисляет долю точек данных, предсказанные метки которых точно соответствуют наблюдаемым меткам.
# import numpy as np
# import pandas as pd
#
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # fit
# knn.fit(X_train, y_train)
#
# y_pred = knn.predict(X_test)
#
# print((y_pred == y_test.values).sum())
# print(y_test.size)
# TODO: Классификатор допустил одну ошибку. Таким образом, точность составляет 44/45:
# import numpy as np
# import pandas as pd
#
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # fit
# knn.fit(X_train, y_train)
#
# y_pred = knn.predict(X_test)
#
# print((y_pred == y_test.values).sum() / y_test.size)
# print(y_pred)
# print(y_test.values)
# print(y_test.size)
# TODO: Такой же как:
# import numpy as np
# import pandas as pd
#
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # fit
# knn.fit(X_train, y_train)
#
# y_pred = knn.predict(X_test)
#
# print(knn.score(X_test, y_test))
# TODO: В модуле sklearn.metrics функция accuracy_score(y_true, y_pred) выполняет тот же расчет.

# TODO: Confusion Matrix (Матрица путаницы)
#  Одна только точность классификации может ввести в заблуждение,
#  если в каждом классе имеется неравное количество наблюдений или если в наборе данных более двух классов.
#  Вычисление матрицы путаницы даст лучшее представление о том,
#  что классификация работает правильно и какие типы ошибок она допускает.
#  Что такое матрица путаницы? Это сводка количества правильных и неправильных прогнозов с разбивкой по каждому классу.
#  При классификации радужной оболочки мы можем использовать confusion_matrix() в модуле sklearn.metrics:
# import numpy as np
# import pandas as pd
#
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # fit
# knn.fit(X_train, y_train)
#
# y_pred = knn.predict(X_test)
#
# from sklearn.metrics import confusion_matrix
#
# print(confusion_matrix(y_test, y_pred))
# TODO: Мы можем визуализировать матрицу путаницы:
# import matplotlib.pyplot as plt
# import pandas as pd
# import numpy as np
#
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# # instantiate
# knn = KNeighborsClassifier(n_neighbors=5)
# # fit
# knn.fit(X_train, y_train)
#
# y_pred = knn.predict(X_test)
#
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import plot_confusion_matrix
#
# plot_confusion_matrix(knn, X_test, y_test, cmap=plt.cm.Blues)
# plt.savefig("plot.png")
# TODO: Здесь мы указали метки по порядку. Каждый столбец матрицы соответствует предсказанному классу,
#  а каждая строка соответствует реальному классу.
#  Таким образом, строка суммируется с общим количеством экземпляров класса.
#  Первый ряд соответствует собственно радужной оболочке; [15, 0, 0] указывает на то,
#  что 15 видов радужной оболочки были предсказаны правильно, и ни один из них не был помечен неправильно;
#  в то время как последняя строка [0, 1, 14] предполагает, что из 15 реальных ирисовиргинских,
#  0 были предсказаны как ирисовики-сетоза, 1 предсказаны как ирисовики-разноцветные,
#  а остальные 14 были правильно идентифицированы как ирисовиргиники.
#  Это согласуется с нашим наблюдением во время исследовательского анализа данных,
#  то есть между двумя видами было некоторое совпадение на диаграмме рассеяния,
#  и отличить ирис-разноцветный от ирис-виргиника труднее, чем идентифицировать ирис-сетоза.
#  Матрица путаницы — это таблица, которая часто используется для описания производительности модели классификации
#  (или «классификатора») на наборе тестовых данных, для которых известны истинные значения.

# TODO: ЗАДАЧА:
#  Учитывая y_true и y_pred ниже, заполните пробелы в выводе.
#  Обратите внимание, что метки предназначены для индексации матрицы.
# import numpy as np
#
# y_true = np.array(['dog', 'cat', 'cat', 'dog', 'dog'])
#
# y_pred = np.array(['dog', 'cat', 'cat', 'cat', 'dog'])
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import plot_confusion_matrix
#
# confusion_matrix(y_true, y_pred, labels=['cat', 'dog'])
# print(confusion_matrix(y_true, y_pred))

# TODO: K-fold Cross Validation (K-кратная перекрестная проверка)
#  Ранее перед подгонкой модели мы выполняли разбиение обучающих тестов,
#  чтобы мы могли сообщать о производительности модели на тестовых данных.
#  Это простой метод перекрестной проверки, также известный как метод удержания.
#  Однако разбиение является случайным, поэтому производительность модели может зависеть от того,
#  как разбиваются данные. Чтобы преодолеть это, мы вводим k-кратную перекрестную проверку.
#  При k-кратной перекрестной проверке данные делятся на k подмножеств.
#  Затем метод задержки повторяется k раз, так что каждый раз одно из k подмножеств используется
#  в качестве тестового набора, а другие k-1 подмножества объединяются для обучения модели.
#  Затем точность усредняется по k испытаниям, чтобы обеспечить общую эффективность модели.
#  Таким образом, используются все точки данных; и есть больше показателей,
#  поэтому мы не полагаемся на одни тестовые данные для оценки производительности модели.
#  Самый простой способ использовать перекрестную проверку k-fold в scikit-learn — вызвать
#  функцию cross_val_score для модели и набора данных:
# from sklearn.model_selection import cross_val_score
#
# # create a new KNN model
# knn_cv = KNeighborsClassifier(n_neighbors=3)
# TODO: Обратите внимание, что сейчас мы устанавливаем модель 3nn.
# # train model with 5-fold cv
# cv_scores = cross_val_score(knn_cv, X, y, cv=5)
# TODO: Каждый набор задержек содержит 20% исходных данных.
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import cross_val_score
#
# # create a new KNN model
# knn_cv = KNeighborsClassifier(n_neighbors=3)
# # train model with 5-fold cv
# cv_scores = cross_val_score(knn_cv, X, y, cv=5)
# # print each cv score (accuracy)
# print(cv_scores)
# TODO: Как показано, из-за случайных назначений точность на наборах задержек колеблется от 0,9 до 1.
import numpy as np

# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import cross_val_score
#
# # create a new KNN model
# knn_cv = KNeighborsClassifier(n_neighbors=3)
# # train model with 5-fold cv
# cv_scores = cross_val_score(knn_cv, X, y, cv=5)
# # then average them
# print(cv_scores.mean())
# TODO: Мы не можем полагаться на одно отдельное разделение поезда и теста, мы сообщаем,
#  что модель 3nn имеет точность 95,33% на основе 5-кратной перекрестной проверки.
#  Как правило, предпочтение отдается 5-кратной или 10-кратной перекрестной проверке;
#  но формального правила нет. По мере того, как k становится больше,
#  разница в размере между обучающим набором и подмножествами повторной выборки становится меньше.
#  По мере уменьшения этой разницы смещение метода становится меньше.

# TODO: Grid Search (Поиск по сетке)
#  Когда мы построили нашу первую модель knn, мы установили гиперпараметр k равным 5,
#  а затем равным 3 в k-кратной перекрестной проверке; действительно случайный выбор.
#  Какой лучший к? Поиск оптимального k называется настройкой гиперпараметра.
#  Удобным инструментом является поиск по сетке. В scikit-learn мы используем GridSearchCV,
#  который несколько раз обучает нашу модель диапазону значений, указанных в параметре param_grid,
#  и вычисляет показатель перекрестной проверки, чтобы мы могли проверить,
#  какие из наших значений для протестированного гиперпараметра показали наилучшие результаты.
# from sklearn.model_selection import GridSearchCV
#
# # create new a knn model
# knn2 = KNeighborsClassifier()
# # create a dict of all values we want to test for n_neighbors
# param_grid = {'n_neighbors': np.arange(2, 10)}
# # use gridsearch to test all values for n_neighbors
# knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
# # fit model to data
# knn_gscv.fit(X, y)
# TODO: Чтобы проверить самое эффективное значение n_neighbors:
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import GridSearchCV
#
# # create new a knn model
# knn2 = KNeighborsClassifier()
# # create a dict of all values we want to test for n_neighbors
# param_grid = {'n_neighbors': np.arange(2, 10)}
# # use gridsearch to test all values for n_neighbors
# knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
# # fit model to data
# knn_gscv.fit(X, y)
# print(knn_gscv.best_params_)
# TODO: Мы видим, что 4 — лучшее значение для n_neighbors. Какова точность модели, когда k равно 4?
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import GridSearchCV
#
# # create new a knn model
# knn2 = KNeighborsClassifier()
# # create a dict of all values we want to test for n_neighbors
# param_grid = {'n_neighbors': np.arange(2, 10)}
# # use gridsearch to test all values for n_neighbors
# knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
# # fit model to data
# knn_gscv.fit(X, y)
# print(knn_gscv.best_score_)
# TODO: Использование поиска по сетке для поиска оптимального гиперпараметра
#  для нашей модели повышает точность модели более чем на 1%.
#  Теперь мы готовы построить окончательную модель:
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import GridSearchCV
#
# # create new a knn model
# knn2 = KNeighborsClassifier()
# # create a dict of all values we want to test for n_neighbors
# param_grid = {'n_neighbors': np.arange(2, 10)}
# # use gridsearch to test all values for n_neighbors
# knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
# # fit model to data
# knn_gscv.fit(X, y)
#
# knn_final = KNeighborsClassifier(n_neighbors=knn_gscv.best_params_['n_neighbors'])
# knn_final.fit(X, y)
#
# y_pred = knn_final.predict(X)
# print(knn_final.score(X, y))
# TODO: Мы можем сообщить, что наша окончательная модель, 4nn, имеет точность 97,3% в предсказании видов ириса!
#  Методы k-кратной перекрестной проверки и настройки параметров с поиском по сетке применимы
#  как к задачам классификации, так и к задачам регрессии.

# TODO: Label Prediction with New Data (Предсказание метки с новыми данными)
#  Теперь мы готовы развернуть модель knn_final.
#  Возьмем некоторые измерения радужной оболочки и запишем,
#  что длина и ширина чашелистика 5.84 см и 3.06 см соответственно,
#  а длина и ширина лепестка 3.76 см и 1.20 см соответственно.
#  Как мы делаем прогноз, используя построенную модель?
#  Используйте model.predict
#  Поскольку модель была обучена длине и ширине лепестков, эти данные нам понадобятся для прогноза.
#  Давайте поместим длину и ширину лепестка в массив numpy:
# new_data = np.array([3.76, 1.20])
# TODO: Если мы скормим его модели:
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import GridSearchCV
#
# # create new a knn model
# knn2 = KNeighborsClassifier()
# # create a dict of all values we want to test for n_neighbors
# param_grid = {'n_neighbors': np.arange(2, 10)}
# # use gridsearch to test all values for n_neighbors
# knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
# # fit model to data
# knn_gscv.fit(X, y)
#
# knn_final = KNeighborsClassifier(n_neighbors=knn_gscv.best_params_['n_neighbors'])
# knn_final.fit(X, y)
#
# new_data = np.array([3.76, 1.20])
# knn_final.predict(np.array(new_data))
# TODO: Подожди, что только что произошло? Когда мы обучали модель, данные представляли собой 2D DataFrame,
#  поэтому модель ожидала 2D-массива, который может быть массивом numpy или pandas DataFrame.
#  Теперь new_data — это одномерный массив, нам нужно сделать его двухмерным, как предлагалось в сообщении об ошибке:
# new_data = new_data.reshape(1, -1)
# TODO: Теперь мы готовы сделать прогноз метки:
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import GridSearchCV
#
# # create new a knn model
# knn2 = KNeighborsClassifier()
# # create a dict of all values we want to test for n_neighbors
# param_grid = {'n_neighbors': np.arange(2, 10)}
# # use gridsearch to test all values for n_neighbors
# knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
# # fit model to data
# knn_gscv.fit(X, y)
#
# knn_final = KNeighborsClassifier(n_neighbors=knn_gscv.best_params_['n_neighbors'])
# knn_final.fit(X, y)
#
# new_data = np.array([[3.76, 1.20]])
# # new_data = np.array([3.76, 1.20])
# # new_data = new_data.reshape(1, -1)
# print(knn_final.predict(new_data))
# TODO: Наша модель предсказывает, что это 'iris-versicolor'.
#  Model.predict также может принимать 2D-список.
#  Например, knn_final.predict([[3.76, 1.2]]) выведет тот же результат, что и в уроке.

# TODO: Probability Prediction with New Data (Прогноз вероятности с новыми данными)
#  Давайте соберем больше данных: три растения ириса имеют одинаковую ширину лепестка 2.25 см,
#  но различаются длиной лепестка: 5.03 см, 3.85 см и 1.77 см соответственно.
#  Мы сохраняем новые данные в двумерный массив следующим образом:
# new_data = np.array([[3.76, 1.2], [5.25, 1.2], [1.58, 1.2]])
# TODO: Из предыдущей части мы узнали, что можем делать прогнозы, используя knn_final.predict():
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import GridSearchCV
#
# # create new a knn model
# knn2 = KNeighborsClassifier()
# # create a dict of all values we want to test for n_neighbors
# param_grid = {'n_neighbors': np.arange(2, 10)}
# # use gridsearch to test all values for n_neighbors
# knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
# # fit model to data
# knn_gscv.fit(X, y)
#
# knn_final = KNeighborsClassifier(n_neighbors=knn_gscv.best_params_['n_neighbors'])
# knn_final.fit(X, y)
#
# new_data = np.array([[3.76, 1.2],
#                      [5.25, 1.2],
#                      [1.58, 1.2]])
# print(knn_final.predict(new_data))
# TODO: Напомним, что в классификациях чаще предсказывают вероятность того,
#  что каждая точка данных будет присвоена каждой метке:
# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.neighbors import KNeighborsClassifier
#
# iris = pd.read_csv('https://sololearn.com/uploads/files/iris.csv')
#
# iris.drop('id', axis=1, inplace=True)
#
# X = iris[['petal_len', 'petal_wd']]
# y = iris['species']
#
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
#
# from sklearn.model_selection import GridSearchCV
#
# # create new a knn model
# knn2 = KNeighborsClassifier()
# # create a dict of all values we want to test for n_neighbors
# param_grid = {'n_neighbors': np.arange(2, 10)}
# # use gridsearch to test all values for n_neighbors
# knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
# # fit model to data
# knn_gscv.fit(X, y)
#
# knn_final = KNeighborsClassifier(n_neighbors=knn_gscv.best_params_['n_neighbors'])
# knn_final.fit(X, y)
#
# new_data = np.array([[3.76, 1.2],
#                      [5.25, 1.2],
#                      [1.58, 1.2]])
# knn_final.predict(new_data)
# print(knn_final.predict_proba(new_data))
# TODO: Сумма каждой строки равна 1. Возьмите вторую радужную оболочку, наша модель предсказывает,
#  что вероятность того, что радужная оболочка будет разноцветной, составляет 25%, а 75% - виргинская.
#  Это согласуется с предсказанием на этикетке: virginica.
#  Для алгоритмов классификации в scikit Learn функция Predict_proba берет новую точку данных
#  и выводит вероятность для каждого класса в виде значения от 0 до 1.

# TODO: ЗАДАЧА:
#  Учитывая y_true и y_pred ниже, заполните пробелы в выводе.
#  Обратите внимание, что метки предназначены для индексации матрицы.
# import numpy as np
#
# from sklearn.metrics import confusion_matrix
#
# y_true = np.array(['cat', 'dog', 'dog', 'cat', 'fish', 'dog', 'fish'])
# y_pred = np.array(['cat', 'cat', 'cat', 'cat', 'fish', 'dog', 'fish'])
# confusion_matrix(y_true, y_pred, labels=['cat', 'dog', 'fish'])
# print(confusion_matrix(y_true, y_pred))

# TODO: ЗАДАЧА: Data Science - Binary Disorder (Наука о данных — бинарный беспорядок)
#  Матрица путаницы бинарной классификации.
#  Для бинарных классификаций матрица путаницы представляет собой матрицу два на два
#  для визуализации производительности алгоритма. Каждая строка матрицы представляет экземпляры
#  в прогнозируемом классе, а каждый столбец представляет экземпляры в реальном классе.
#  Задача Имея два списка из 1 и 0 (1 представляет истинную метку, а 0 представляет собой ложную ложную)
#  одинаковой длины, выведите 2 набора значений, каждая ячейка определяется следующим образом:
#  Верхний правый: Прогнозируемый истинный, но фактически ложный (Ложноположительный)
#  Внизу слева: Прогнозируемый ложный, но фактически верный (Ложноотрицательный)
#  Нижний правый: Прогнозируемый ложный и фактически ложный (Истинно отрицательный)
#  Формат ввода:
#  Первая строка: список из 1 и 0, разделенных пробелом. Это настоящие двоичные метки.
#  Вторая строка: список из 1 и 0, длина такая же, как и в первой строке.
#  Они представляли предсказанные метки.
#  Формат вывода:
#  2darray numpy из двух строк и двух столбцов, первая строка содержит количество истинных срабатываний
#  и ложных срабатываний, а вторая строка содержит количество ложных отрицаний и истинных отрицаний.
#  Sample Input:
#  1 1 0 0
#  1 0 0 0
#  Sample Output:
#  [[1., 0.],
#  [1., 2.]]
#  Объяснение
#  Среди реальных меток есть 2 истинных и 2 ложных.
#  Одна истинная метка была правильно предсказана как истинная,
#  а другая неправильно предсказана как ложная;
#  то есть один истинно положительный и один ложноотрицательный.
#  Из двух ложных ярлыков оба были предсказаны правильно;
#  то есть ноль ложных положительных результатов и два истинных отрицательных результата.
# import numpy as np
#
# y_true = np.array([int(x) for x in input().split()])
# y_pred = np.array([int(x) for x in input().split()])
#
# tp = sum(y_true & y_pred)
# fp = sum(~y_true & y_pred)
# fn = sum(y_true & ~y_pred)
# tn = len(y_true) - tp - fp - fn
#
# print(np.array([[tp, fp], [fn, tn]], dtype='float'))

# TODO: Overview (Обзор)
#  Кластеризация — это тип обучения без учителя, который позволяет нам находить группы похожих объектов,
#  объекты, которые больше связаны друг с другом, чем с объектами в других группах.
#  Это часто используется, когда у нас нет доступа к истине, другими словами, отсутствуют ярлыки.
#  Примеры вариантов использования в бизнесе включают группировку документов,
#  музыки и фильмов на основе их содержимого или поиск сегментов клиентов
#  на основе покупательского поведения в качестве основы для механизмов рекомендаций.
#  Цель кластеризации состоит в том, чтобы разделить данные на группы или кластеры
#  с более схожими чертами друг с другом, чем с данными в других кластерах.

# TODO: Different Types of Clustering Algorithms (Различные типы алгоритмов кластеризации)
#  Известно более 100 алгоритмов кластеризации, 12 из них реализованы в scikit-learn, но немногие завоевали популярность
#  В общем, существует четыре типа: модели на основе центроида — каждый кластер представлен одним средним вектором
#  (например, k-means), модели на основе связности — построены на основе удаленной связности
#  (например, иерархическая кластеризация) модели на основе распределения — построены с использованием
#  статистических распределений (например, гауссовых смесей) Модели на основе плотности — кластеры определяются как
#  плотные области (например, DBSCAN) сообщает химический анализ.
#  Дополнительные сведения об алгоритмах кластеризации см. в документации.
#  https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods

# TODO: K-means (К-средних)
#  Одним из самых популярных алгоритмов кластеризации является k-means.
#  Предполагая, что имеется n точек данных, алгоритм работает следующим образом:
#  Шаг 1: инициализация — выбор k случайных точек в качестве центров кластеров, называемых центроидами.
#  Шаг 2: присвоение кластеров — назначение каждой точки данных ее ближайшему центроиду
#  на основе его расстояния до каждого центроида и это формирует k кластеров
#  Шаг 3: обновление центроида - для каждого нового кластера вычислить его центроид,
#  взяв среднее значение всех точек, присвоенных кластеру
#  Шаг 4: повторять шаги 2 и 3 до тех пор, пока ни одно из назначений кластера не изменится
#  или максимальное количество итераций
#  Алгоритм k-средних реализован в модуле sklearn.cluster, для доступа к нему:
# from sklearn.cluster import KMeans
# TODO: Алгоритм приобрел большую популярность, потому что его легко реализовать
#  и он хорошо масштабируется для больших наборов данных. Однако трудно предсказать количество кластеров,
#  он может застрять в локальных оптимумах и может плохо работать, когда кластеры имеют разные размеры и плотность.

# TODO: Distance Metric (Метрика расстояния)
#  Как мы рассчитываем расстояние в алгоритме k-средних?
#  Одним из способов является евклидово расстояние, прямая линия между двумя точками данных,
#  См. Рис: DistanceMetricPicture_1.jpg.
# TODO: Например, евклидово расстояние между точками x1 = (0, 1) и x2 = (2, 0) определяется как:
#  См. Рис: DistanceMetricPicture_2.png
# TODO: Или в numpy мы можем рассчитать расстояние следующим образом:
# import numpy as np
#
# x1 = np.array([0, 1])
# x2 = np.array([2, 0])
# print(np.sqrt(((x1 - x2) ** 2).sum()))
# # 2.23606797749979
# print(np.sqrt(5))
# # 2.23606797749979
# TODO: Его можно расширить до более высоких измерений. В n-мерном пространстве есть две точки:
#  См. Рис: DistanceMetricPicture_3.png
# TODO: Тогда евклидово расстояние от p до q определяется формулой Пифагора:
#  См. Рис: DistanceMetricPicture_4.png
# TODO: Существуют и другие метрики расстояния, такие как манхэттенское расстояние, косинусное расстояние и т.д.
#  Выбор метрики расстояния зависит от данных.

# TODO: Wine Data (Винные данные)
#  В этом модуле мы анализируем результат химического анализа вин, выращенных в конкретном регионе Италии.
#  И цель состоит в том, чтобы попытаться сгруппировать похожие наблюдения вместе
#  и определить количество возможных кластеров. Это помогло бы нам делать прогнозы и уменьшать размерность.
#  Как мы увидим, у каждого вина есть 13 характеристик, и если бы мы могли сгруппировать все вина,
#  скажем, в 3 группы, то это свело бы 13-мерное пространство к 3-мерному пространству.
#  Более конкретно, мы можем представить каждую из наших исходных точек данных с точки зрения того,
#  насколько далеко она находится от каждого из этих трех кластерных центров.
#  В анализе сообщалось о количестве 13 компонентов из 178 вин: спирт, яблочная кислота,
#  зольность, щелочность золы, магний, общее количество фенолов, флавоноиды, нефлаваноидные фенолы,
#  проантоцианы, интенсивность цвета, оттенок, OD280/OD315 разбавленных вин и пролин.
#  Данные загружаются в фрейм данных «вино».
# import numpy as numpy
# import pandas as pd
# from sklearn.datasets import load_wine
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
# print(wine.shape)
# print(wine.columns)
# TODO: Для простоты отображения мы показываем базовую статистику первых 3-х признаков:
# import numpy as numpy
# import pandas as pd
# from sklearn.datasets import load_wine
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
# print(wine.iloc[:, :3].describe())
# # print(wine.info())
# TODO: Пропущенных значений нет. Стоит отметить, что атрибуты не находятся в одном масштабе.
#  Нам нужно будет масштабировать данные позже.
#  Другой способ проверить имена столбцов и тип данных каждого столбца — использовать .info().

# TODO: Plotting the Data (График данных)
#  Сводная статистика предоставляет некоторую информацию,
#  в то время как визуализация предлагает более прямое представление,
#  показывающее распределение и взаимосвязь между характеристиками.
#  Здесь мы вводим характеристику построения графика для отображения гистограмм по диагонали
#  и точечных диаграмм для каждой пары атрибутов вне диагонали, 'scatter_matrix',
#  для простоты отображения покажем только две характеристики:
# import matplotlib.pyplot as plt
# import numpy as numpy
# import pandas as pd
# from pandas.plotting import scatter_matrix
# from sklearn.datasets import load_wine
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
# scatter_matrix(wine.iloc[:, [0, 5]])
# plt.savefig("plot.png")
# plt.show()
# TODO: Поскольку мы не знаем достоверной информации, мы изучаем графики рассеяния,
#  чтобы найти разумного кандидата на k, количество кластеров. Кажется, есть примерно три подгруппы.
#  Помните, что для количества подгрупп нет правильных или неправильных ответов.
#  В реальных данных мы редко находим четкие кластеры; но мы приходим к нашему лучшему обоснованному предположению.
#  Например, на приведенной выше диаграмме рассеивания кажутся три подгруппы.
#  Независимо от того, является ли это проблемой обучения с учителем или без учителя,
#  исследовательский анализ данных (EDA) необходим и настоятельно рекомендуется перед тем,
#  как погрузиться в моделирование.

# TODO: Pre-processing: Standardization (Предварительная обработка: стандартизация)
#  Изучив все пары графиков рассеяния, мы выбрали два признака, чтобы лучше проиллюстрировать алгоритм:
#  алкоголь и общие_фенолы, график рассеивания которых также предлагает три подкластера.
# X = wine[['alcohol', 'total_phenols']]
# TODO: В отличие от любых моделей обучения с учителем, модели машинного обучения без учителя, как правило, не требуют
#  разделения данных на обучающие и тестовые наборы, поскольку для проверки модели нет достоверной информации.
#  Однако алгоритмы на основе центроидов требуют одного шага предварительной обработки,
#  потому что k-средние лучше работают с данными, где каждый атрибут имеет одинаковые масштабы.
#  Один из способов добиться этого — стандартизировать данные;
#  математически: z = (x - среднее) / стандартное отклонение, где x — необработанные данные,
#  среднее значение и стандартное отклонение — среднее значение и стандартное отклонение x,
#  а z — масштабированное значение x, центрированное на 0 и имеющее единичное стандартное отклонение.
#  StandardScaler под sklearn.preprocessing упрощает задачу:
# from sklearn.preprocessing import StandardScaler
#
# # instantiate the scaler
# scale = StandardScaler()
# # compute the mean and std to be used later for scaling
# scale.fit(X)
# # StandardScaler(copy=True, with_mean=True, with_std=True)
# TODO: Мы можем посмотреть на масштаб объекта, извлечь рассчитанное среднее и стандартное значение:
# import pandas as pd
# from sklearn.datasets import load_wine
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# from sklearn.preprocessing import StandardScaler
#
# scale = StandardScaler()
# scale.fit(X)
#
# print(scale.mean_)
# print(scale.scale_)
# TODO: Измерения спирта-сырца имеют среднее значение 13,00 и стандартное значение 0,81,
#  в то время как общее количество фенолов находится на уровне 2,29 при стандартном значении 0,62.
#  Затем мы можем подобрать данные для обучения и преобразовать их.
# X_scaled = scale.transform(X)
# TODO: Мы показываем исходные (красный) и масштабированные (синий) данные на графике,
#  чтобы визуализировать эффект масштабирования. См. Рис: PreProcessingPicture.jpg
#  После масштабирования данные центрируются вокруг (0, 0), а диапазоны по осям x и y примерно одинаковы, от -2.5 до 2.5
# import pandas as pd
# from sklearn.datasets import load_wine
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# from sklearn.preprocessing import StandardScaler
#
# scale = StandardScaler()
# scale.fit(X)
#
# X_scaled = scale.transform(X)
# print(X_scaled.mean(axis=0))
# print(X_scaled.std(axis=0))
# TODO: Давайте проверим работоспособность, чтобы увидеть,
#  центрирована ли каждая характеристика на 0 и имеет ли стандартное значение 1:
#  Хорошей практикой является масштабирование характеристик перед обучением модели,
#  если алгоритмы основаны на расстоянии.
#  Дополнительные сведения см. в разделе «Важность масштабирования характеристик»:
#  https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html

# TODO: K-means Modeling (K-среднее моделирование)
#  Точно так же, как линейная регрессия и k ближайших соседей или любые алгоритмы машинного обучения в scikit-learn,
#  для моделирования мы следуем рабочему процессу создания/подгонки/предсказания. В KMeans есть и другие аргументы,
#  такие как метод инициализации центроидов, критерии остановки и т.д., но мы фокусируемся на количестве кластеров,
#  n_clusters, и позволяем другим параметрам принимать значения по умолчанию. Здесь мы указываем 3 кластера:
# import pandas as pd
#
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
#
# from sklearn.cluster import KMeans
#
# # instantiate the model
# kmeans = KMeans(n_clusters=3)
# # fit the model
# kmeans.fit(X_scaled)
# # make predictions
# y_pred = kmeans.predict(X_scaled)
# print(y_pred)
# TODO: В кластере 0 60 вин, 65 в кластере 1 и 53 в кластере 2.
#  Чтобы проверить координаты трех центроидов:
# import pandas as pd
#
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
#
# from sklearn.cluster import KMeans
#
# # instantiate the model
# kmeans = KMeans(n_clusters=3)
# # fit the model
# kmeans.fit(X_scaled)
# # make predictions
# kmeans.predict(X_scaled)
#
# print(kmeans.cluster_centers_)
# TODO: Лучший способ увидеть результаты — визуализировать их:
# import pandas as pd
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
#
# from sklearn.cluster import KMeans
#
# # instantiate the model
# kmeans = KMeans(n_clusters=3)
# # fit the model
# kmeans.fit(X_scaled)
# # make predictions
# y_pred = kmeans.predict(X_scaled)
#
# import matplotlib.pyplot as plt
#
# # plot the scaled data
# plt.scatter(X_scaled[:, 0],
#             X_scaled[:, 1],
#             c=y_pred)
# # identify the centroids
# plt.scatter(kmeans.cluster_centers_[:, 0],
#             kmeans.cluster_centers_[:, 1],
#             marker="*",
#             s=250,
#             c=[0, 1, 2],
#             edgecolors='k')
# plt.xlabel('alcohol');
# plt.ylabel('total phenols')
# plt.title('k-means (k=3)')
# plt.savefig("plot.png")
# plt.show()
# TODO: Звезды являются центроидами. K-means делит вина на три группы: слабоалкогольные,
#  но с высоким содержанием фенолов (вверху справа зеленым цветом), с высоким содержанием алкоголя
#  и высоким содержанием фенолов (вверху слева желтым цветом) и с низким содержанием фенолов (внизу фиолетовым цветом).
#  Для любого нового вина с химическим отчетом об алкоголе и общем количестве фенолов мы теперь можем классифицировать
#  его на основе его расстояния до каждого из центроидов. Предположим, что есть молодое вино с содержанием спирта 13
#  и общим содержанием фенолов 2.5, давайте предскажем, к какому кластеру модель отнесет новое вино.
#  Сначала нам нужно поместить новые данные в массив 2d:
# X_new = np.array([[13, 2.5]])
# TODO: Далее нам нужно стандартизировать новые данные:
# X_new_scaled = scale.transform(X_new)
# print(X_new_scaled)
# [[-0.00076337  0.32829793]]
# TODO: Теперь мы готовы предсказать кластер:
# import pandas as pd
# import numpy as np
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
# from sklearn.cluster import KMeans
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
#
# # instantiate the model
# kmeans = KMeans(n_clusters=3)
# # fit the model
# kmeans.fit(X_scaled)
# # make predictions
# kmeans.predict(X_scaled)
#
# X_new = np.array([[13, 2.5]])
# X_new_scaled = scale.transform(X_new)
#
# print(kmeans.predict(X_new_scaled))
# TODO: Действительно, среди трех центроидов новое вино ближе всего к кластеру 1,
#  центроид которого находится в точке (0,07498401, -1,14070754).
#  Ожидайте получать немного разные результаты каждый раз, когда вы запускаете код,
#  поскольку порядок кластеров может измениться.
#  Одним из основных недостатков k-средних является то, что случайное начальное предположение
#  для центроидов может привести к плохой кластеризации, и алгоритм k-средних++ устраняет это препятствие,
#  определяя процедуру для инициализации центроидов перед переходом к стандартному алгоритму k-средних.
#  В scikit-learn механизм инициализации по умолчанию установлен на k-means++.

# TODO: Optimal k: The Elbow Method (Оптимальное k: метод локтя)
#  Можем ли мы разделить вина на две подгруппы?
#  См. Рис: optimal_k_picture_1.png
#  Нет проблем, как насчет четырех?
#  См. Рис: optimal_k_picture_2.png
#  Конечно! Как показано, k-средние будут счастливы разделить набор данных на любое целое число кластеров,
#  начиная от 1, крайнего случая, когда все точки данных принадлежат одному большому кластеру, до n,
#  еще одного предельного случая, когда каждая точка данных является своей собственной. кластер.
#  Итак, какой из них мы должны выбрать, 2, или 3, или 4 для вин? Интуитивно задача k-средних
#  разбивает n точек данных на k тесных наборов, так что точки данных находятся ближе друг к другу,
#  чем к точкам данных в других кластерах. И герметичность может быть измерена как сумма квадратов расстояния
#  от точки данных до ее ближайшего центроида или инерции.
#  В scikit-learn он хранится как inertia_, например, когда k = 2, искажение равно 185:
# import pandas as pd
# import matplotlib.pyplot as plt
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
# from sklearn.cluster import KMeans
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
#
# kmeans = KMeans(n_clusters=2)
# kmeans.fit(X_scaled)
# print(kmeans.inertia_)
# TODO: Или, когда k равно 3, искажение уменьшается до 114.
# import pandas as pd
# import matplotlib.pyplot as plt
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
# from sklearn.cluster import KMeans
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
#
# kmeans = KMeans(n_clusters=3)
# kmeans.fit(X_scaled)
# print(kmeans.inertia_)
# TODO: Построим график инерции для разных значений k:
# import pandas as pd
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
# import numpy as np
# import matplotlib.pyplot as plt
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine[['alcohol', 'total_phenols']]
#
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
#
#
# from sklearn.cluster import KMeans
# # calculate distortion for a range of number of cluster
# inertia = []
# for i in np.arange(1, 11):
#     km = KMeans(
#         n_clusters=i
#     )
#     km.fit(X_scaled)
#     inertia.append(km.inertia_)
#
# # plot
# plt.plot(np.arange(1, 11), inertia, marker='o')
# plt.xlabel('Number of clusters')
# plt.ylabel('Inertia')
# plt.savefig("plot.png")
# plt.show()
# TODO: См. Рис: optimal_k_picture_3.png
# TODO: Как видно из графика, инерция уменьшается по мере увеличения количества кластеров.
#  Оптимальное значение k должно быть таким, при котором инерция больше не уменьшается так быстро.
# TODO: См. Рис: optimal_k_picture_4.png
# TODO: Например, k=3 кажется оптимальным, так как при увеличении количества кластеров с 3 до 4
#  уменьшение инерции значительно замедляется по сравнению с уменьшением от 2 до 3.
#  Такой подход называется методом локтя (понимаете, почему ?).
#  Это полезный графический инструмент для оценки оптимального k в k-средних.
#  Одна единственная инерция сама по себе не подходит для определения оптимального k,
#  потому что чем больше k, тем ниже будет инерция.

# TODO: Modeling With More Features (Моделирование с дополнительными характеристиками)
#  Ранее для построения моделей kmeans мы использовали два (из тринадцати) признака: спирт и общее количество фенолов.
#  Выбор случайный, и результаты легко визуализировать.
#  Однако можем ли мы использовать больше характеристик, например, все?
#  Почему бы и нет? Давай попробуем.
# X = wine
# TODO: Не забудьте стандартизировать каждую характеристику.
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
# TODO: Постройте инерцию для диапазона k, чтобы определить оптимальное значение k с помощью метода локтя:
# import pandas as pd
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
# import numpy as np
# import matplotlib.pyplot as plt
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine
#
# scale = StandardScaler()
# scale.fit(X)
# X_scaled = scale.transform(X)
#
# from sklearn.cluster import KMeans
#
# # calculate distortion for a range of number of cluster
# inertia = []
# for i in np.arange(1, 11):
#     km = KMeans(
#         n_clusters=i
#     )
#     km.fit(X_scaled)
#     inertia.append(km.inertia_)
# plt.plot(np.arange(1, 11), inertia, marker='o')
# plt.xlabel('Number of clusters')
# plt.ylabel('Inertia')
# plt.title("all features")
# plt.savefig("plot.png")
# plt.show()
# TODO: Точно так же мы замечаем, что инерция больше не уменьшается так быстро после k = 3.
#  Затем мы завершаем модель, устанавливая n_clusters = 3 и получаем предсказания.
# import numpy as np
# import pandas as pd
# from sklearn.datasets import load_wine
# from sklearn.preprocessing import StandardScaler
# from sklearn.cluster import KMeans
#
# data = load_wine()
# wine = pd.DataFrame(data.data, columns=data.feature_names)
#
# X = wine
#
# scale = StandardScaler()
# scale.fit(X)
#
# X_scaled = scale.transform(X)
#
# k_opt = 3
# kmeans = KMeans(k_opt)
# kmeans.fit(X_scaled)
# y_pred = kmeans.predict(X_scaled)
# print(y_pred)
# TODO: По сравнению с прогнозами, использующими только две характеристики, две модели дают очень похожие результаты.
#  Например, согласно прогнозам, первые 21 вино принадлежат к одному и тому же кластеру из обеих моделей,
#  как и последние 19 вин. На самом деле, только 13 из 178 вин были сгруппированы по-разному в двух моделях.
#  Естественно спросить, какая модель лучше? Напомним, что кластеризация — это метод обучения без присмотра,
#  который указывает на то, что мы не знаем истинности меток. Таким образом, трудно, если вообще возможно, определить,
#  что модель с двумя признаками более точна при группировании вин, чем модель со всеми 13 признаками, или наоборот.
#  Какую модель, другими словами, какие характеристики выбрать, часто определяется внешней информацией.
#  Например, отдел маркетинга хочет знать, нужна ли для продажи этих вин стратегия,
#  ориентированная на конкретный континент.
#  Теперь у нас есть доступ к демографической информации потребителей, и три кластера, выделенные из модели А,
#  лучше соответствуют клиентам в Европе, Азии и Северной Америке соответственно, чем модель В;
#  тогда модель А является победителем. Это слишком упрощенный пример, но суть вы поняли.
#  На практике характеристики часто выбираются в сотрудничестве между учеными по данным и экспертами в предметной области.

# TODO: ЗАДАЧА: Data Science - Pandas Pandas Pandas (Наука о данных — Панды Панды Панды)
#  Поиск следующего центроида Кластеризация алгоритма обучения без учителя включает
#  обновление центроида каждого кластера.
#  Здесь мы находим следующие центроиды для заданных точек данных и начальных центроидов.
#  Задача Предположим, что среди заданных двумерных точек данных есть два кластера,
#  и две случайные точки (0, 0) и (2, 2) являются начальными центроидами кластера.
#  Вычислите евклидово расстояние между каждой точкой данных и каждым из центроидов,
#  назначьте каждую точку данных ближайшему центроиду, а затем рассчитайте новый центроид.
#  Если есть связь, назначьте точку данных кластеру с центроидом (0, 0).
#  Если ни одна из точек данных не была назначена данному центроиду, верните None.
#  Формат ввода Первая строка: целое число, указывающее количество точек данных (n)
#  Следующие n строк: два числовых значения в каждой строке для представления точки данных в двухмерном пространстве.
#  Выходной формат Два списка для двух центроидов. Числа округляются до второго десятичного знака.
#  Sample Input:
#  3
#  1 0
#  0 .5
#  4 0
#  Sample Output:
#  [0.5 0.25]
#  [4. 0.]
#  Объяснение:
#  Есть 3 точки данных, и мы хотели бы выделить среди них два кластера.
#  Начальные центроиды даны (0, 0) и (2, 2).
#  Расстояния между первой точкой данных (1, 0) и каждым из центроидов составляют 1,0 и 2,24,
#  округленные до второго десятичного знака. Первая точка данных ближе к (0, 0), поэтому назначается 0-й кластер.
#  Точно так же точка данных (0, .5) ближе к (0, 0), чем к (2, 2), также отнесена к 0-му кластеру;
#  в то время как (4, 0) ближе к (2, 2), поэтому относится к 1-му кластеру.
#  Чтобы вычислить новые центроиды, возьмите среднее значение всех точек данных в 0-м и 1-м кластерах соответственно.
#  Отсюда результаты [0,5 0,25] и [4. 0.].
import math
import numpy as np

n = int(input())
ce1 = [0, 0]
ce2 = [2, 2]
cl1 = np.empty([0, 2], float)
cl2 = np.empty([0, 2], float)

for i in range(n):
    x = [float(j) for j in input().split()]
    d21 = (np.array(ce1) - np.array(x)) ** 2
    d1 = math.sqrt(d21.sum())
    d2 = math.sqrt(((np.array(ce2) - np.array(x)) ** 2).sum())
    if d1 <= d2:
        cl1 = np.append(cl1, np.array([x]), axis=0)
    else:
        cl2 = np.append(cl2, np.array([x]), axis=0)
if cl1.shape[0] != 0:
    print(np.mean(cl1, axis=0).round(2))
else:
    print(None)
if cl2.shape[0] != 0:
    print(np.mean(cl2, axis=0).round(2))
else:
    print(None)
